{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/71580375662.jpg","path":"images/71580375662.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/flowchart.png","path":"images/flowchart.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/DrawingHands.jpg","path":"images/DrawingHands.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/120625074322660dd39deed2d7.gif","path":"images/120625074322660dd39deed2d7.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/LICENSE","path":"lib/bookmark/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/README.md","path":"lib/bookmark/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/index.js","path":"lib/bookmark/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/package.json","path":"lib/bookmark/package.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/bookmark.min.js","path":"lib/bookmark/bookmark.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/bookmark/renovate.json","path":"lib/bookmark/renovate.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"5fb5dac0d98e6c7491a8ed039eca86a2451cdd83","modified":1663852760666},{"_id":"themes/next/.DS_Store","hash":"abe01f5593a8e5561a9323d831c9f9492218de51","modified":1582733519835},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1580486111282},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1580486111282},{"_id":"themes/next/.gitignore","hash":"7b68ca7a46104cf9aa84ec0541a4856ab1836eca","modified":1580486111287},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1580486111282},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1580486111287},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1580486111287},{"_id":"themes/next/README.md","hash":"dc026053a4d9fb97a58dbc3e9060e480f6852b23","modified":1580486111288},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1580486111287},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1580486111288},{"_id":"themes/next/gulpfile.js","hash":"72e6d5a6e32d5f95d82e4c4d0c963d39555bb760","modified":1580486111297},{"_id":"themes/next/_config.yml","hash":"bac0e0327796e708bf3e5b6bc1f3dfca64c29da2","modified":1638040438827},{"_id":"themes/next/package.json","hash":"19dda7fab09594faba989669e29de88c4289877f","modified":1580486111322},{"_id":"source/_posts/.DS_Store","hash":"4a095d406be8e8ae6ee8934652c672826d8d67d6","modified":1647771821476},{"_id":"source/_posts/2020-02-20-数理逻辑.md","hash":"2ddeb36cb9b6cf3ca0429bf851438e3569b85b8c","modified":1583057045221},{"_id":"source/_posts/2020-02-26-AI_NB_SVM_DT.md","hash":"2ac8415904969e2637a8b7c3c1fedecbbe67a2b9","modified":1603203658891},{"_id":"source/_posts/2020-02-12-Python-基础（二）.md","hash":"f5291487e6fa668a33e3f247654d982763579530","modified":1584029707290},{"_id":"source/_posts/2020-02-26-拉格朗日乘子法.md","hash":"cceb93d7ad8ce63176c5ef738b2fa7bc19dc94ac","modified":1605148815859},{"_id":"source/_posts/2020-10-20-AI_RL.md","hash":"d76a522b19070d48e4919dbac173ac5912117366","modified":1604389969539},{"_id":"source/_posts/20200528_ai_custom_tensorflow.md","hash":"e27aaddc5e4d60ea817052742b0be405c173124e","modified":1641788770539},{"_id":"source/_posts/AI_Neural_Network.md","hash":"e8e9b8f4cdea6b0e3f69a5ff90f9c1617a8c0ec1","modified":1603473599740},{"_id":"source/_posts/2020-11-21-AI-Numba_GPU.md","hash":"ebedff772c0ee16559b915edc6517329c8ab08a3","modified":1607867635202},{"_id":"source/_posts/Linear and nolinear.md","hash":"08177f02c4eeaff9d52fea4ba8ad26ebabd2bda2","modified":1638358854852},{"_id":"source/_posts/Anaconda_install_Tensorflow.md","hash":"073eebae992bd40ee4b599a6bc5ad4b1bc80f279","modified":1651908135953},{"_id":"source/_posts/Marcel Duchamp's Urinal.md","hash":"d14d4f14c3e04791acd16d44602c97058afe0c08","modified":1607962420287},{"_id":"source/_posts/Python 基础（一）.md","hash":"3ee459bd02dcf550bed6b0b58cdf87e90042cf60","modified":1586538956755},{"_id":"source/_posts/LuXun.md","hash":"86b07fccb31319b5c447d088e9c044c9ecc1440e","modified":1629983639898},{"_id":"source/_posts/PCA_CCA_PLS.md","hash":"363dcb8d9c8d3b229fb4f79be432eeb0191aa1b8","modified":1604563665180},{"_id":"source/_posts/Sampling Error.md","hash":"b6457bdfda036b9beb6927c4ddc02b890c982ebb","modified":1642793752714},{"_id":"source/_posts/Relation.md","hash":"0aa33d9baa13c3da2fd0096955a2d38b9454dfc4","modified":1615099164666},{"_id":"source/_posts/aslongas fast enough.md","hash":"0463f4c5ad469f0d5338e09a67f9c8951ef08aa7","modified":1580398121446},{"_id":"source/_posts/boundary&infinity.md","hash":"af3135bc3a482f6f9fb39856eba7c3044436e93e","modified":1629983711711},{"_id":"source/_posts/function.md","hash":"2d916a1d22897c5122c91af2833ac9974f3b072b","modified":1605755261558},{"_id":"source/_posts/how_to_predict_a_coin_flip.md","hash":"28a201c181766965c610120ad8d09a51165b7a06","modified":1642527137478},{"_id":"source/_posts/photopainting2.md","hash":"c026efbb949e0e42fc1279e022181add7914696a","modified":1642842114019},{"_id":"source/_posts/photopainting3.md","hash":"0ba9afed6ec7cbd556999f14eff3e71e7236e5ec","modified":1642791853207},{"_id":"source/_posts/photopainting.md","hash":"a2b8f0140f842276165bbaa52af60d7c7e119020","modified":1663852819656},{"_id":"source/_posts/python_environment.md","hash":"f28bfb5f8e4da231fbf588d02fde8b9699029462","modified":1600356983830},{"_id":"source/_posts/reading gaigelicheng.md","hash":"7a9f2aecead8e7a623540bf32531ad53fe3d615e","modified":1629983622724},{"_id":"source/_posts/reading Whatismath.md","hash":"e7fb7cbf4a4667e679c46a1016a5bde71911fa4c","modified":1584942103200},{"_id":"source/_posts/reading MaxWebber.md","hash":"891b9ea46dc577b6692e78519b007b42ea20af46","modified":1581493511748},{"_id":"source/_posts/命题逻辑.md","hash":"02afe67043c37636aaa3f560327ced9e5d23619b","modified":1603691791118},{"_id":"source/_posts/环境保护主义.md","hash":"b273b563c5955637b74c12608dd68b414c8eff55","modified":1584941902034},{"_id":"source/_posts/计算的极限.md","hash":"ce0c1214bfea9d0c54adb2a094307d9d01b47d8f","modified":1642768286530},{"_id":"source/_posts/集合论.md","hash":"82f1152c9f6ba2320af41d2c8263278d6c30f5b7","modified":1605153464322},{"_id":"source/categories/index.md","hash":"2ee52a5b200d45c4fd060ff844a79ca658ed0f12","modified":1580373927449},{"_id":"source/about/.DS_Store","hash":"45673a8ec47535550c1655fb6f129816c4bf94f9","modified":1590988782123},{"_id":"source/about/index.md","hash":"6bd08d8f4e1ff9f0afbbc571167fc7e4543f74f8","modified":1590939123349},{"_id":"source/tags/index.md","hash":"7bc3d999a08e63199116226c02aa169d8b23e1e2","modified":1580378042842},{"_id":"source/lib/.DS_Store","hash":"e9e540245d3abf3cc247cec7cb9226bffcd3e10e","modified":1580744036059},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"715dcf9b0429a4a4c7107d6d1d00a34e4cf9aa99","modified":1580486111285},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1580486111283},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ef63f34bd3b6bd4d7304d48ce5e0e3e2bead385","modified":1580486111283},{"_id":"themes/next/.github/config.yml","hash":"e4f4b9afe59bc508c4f7634895b33d7d460a7cb1","modified":1580486111285},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1580486111285},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1580486111286},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1580486111286},{"_id":"themes/next/.github/issue-close-app.yml","hash":"7cba457eec47dbfcfd4086acd1c69eaafca2f0cd","modified":1580486111285},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1580486111286},{"_id":"themes/next/.github/stale.yml","hash":"941209526c2f7d916c76163c9e1ac1af9d956679","modified":1580486111286},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1580486111287},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"5b4c013e0598b3211ebd899265936cfdaf7c139f","modified":1580486111291},{"_id":"themes/next/docs/DATA-FILES.md","hash":"40a8089076005e0d26ef7c0db58a2b5b464cda6c","modified":1580486111291},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1580486111291},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1580486111290},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"08cda41b4bcf687facfda19ab39718ec7a05ae54","modified":1580486111292},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1580486111292},{"_id":"themes/next/docs/MATH.md","hash":"f520b336f16665e164d6edf075bdcc6aa17b31bc","modified":1580486111292},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1580486111292},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"682937d48bf5d243842a76190921322e26c75247","modified":1580486111293},{"_id":"themes/next/languages/de.yml","hash":"3530753456db8a7a7cb72b47505bf6197ac9a9f2","modified":1580486111298},{"_id":"themes/next/languages/default.yml","hash":"2c415150d5ba44a05ec7ca5fa671d6129c7fe7db","modified":1580486111298},{"_id":"themes/next/languages/en.yml","hash":"2c415150d5ba44a05ec7ca5fa671d6129c7fe7db","modified":1580486111298},{"_id":"themes/next/languages/es.yml","hash":"9596836628169400beed4124eeb5867821075b00","modified":1580486111298},{"_id":"themes/next/languages/fa.yml","hash":"c77d0ab86695651f4fb09e0e9af484114fe80292","modified":1580486111298},{"_id":"themes/next/languages/hu.yml","hash":"a317ba3ab2d9de40fe8b5447d0e63b9d8ab97c8b","modified":1580486111299},{"_id":"themes/next/languages/id.yml","hash":"85441f625f1f93a329b7eab01f919e81a6f36172","modified":1580486111299},{"_id":"themes/next/languages/it.yml","hash":"8e4d494cd7f4e33d2c416da3a48f9bd8968243ea","modified":1580486111299},{"_id":"themes/next/languages/ja.yml","hash":"2a767bc258a5072f8d97ffdfce53b4e2332570f4","modified":1580486111300},{"_id":"themes/next/languages/ko.yml","hash":"04ba65574f1fa63ffb95fcfb7a4515ee4b01532f","modified":1580486111300},{"_id":"themes/next/languages/nl.yml","hash":"dea4d7d516071c9ec9e57935a84aaab15c3d2e97","modified":1580486111300},{"_id":"themes/next/languages/pt-BR.yml","hash":"66e3cc8efc2a26184fcbe4db8981b84099348276","modified":1580486111300},{"_id":"themes/next/languages/pt.yml","hash":"f184ffed48395675a7025cb1ccfdade546f3e987","modified":1580486111300},{"_id":"themes/next/languages/ru.yml","hash":"79f7f3b6e96184cf21ce2c9a2adb01b0f0228cf8","modified":1580486111301},{"_id":"themes/next/languages/uk.yml","hash":"a6af58b5351e0d0dde1fec22a3e80fc76bd8eecf","modified":1580486111301},{"_id":"themes/next/languages/tr.yml","hash":"5c5b60faa00401e822c92abcdcf9c5d410a8b753","modified":1580486111301},{"_id":"themes/next/languages/vi.yml","hash":"2b698896b08d79333a3414185fc2b445b3657361","modified":1580486111301},{"_id":"themes/next/languages/zh-CN.yml","hash":"ab2fa8bfdbf4afd77c336412f29930dc8256a5fa","modified":1580486111302},{"_id":"themes/next/languages/zh-TW.yml","hash":"d4517fa82652ccc75915469985efd7da1895963a","modified":1580486111302},{"_id":"themes/next/languages/fr.yml","hash":"30702ddcb121ee1ada258b8b900d826fec697e5e","modified":1580486111299},{"_id":"themes/next/layout/.DS_Store","hash":"6f6717d31f935220ff177c57a05fac5370cd5bfc","modified":1582733528776},{"_id":"themes/next/layout/_layout.swig","hash":"29ee038b0d5ffdb45327598733ea968588367769","modified":1580486111302},{"_id":"themes/next/layout/index.swig","hash":"3bc6fb1e9707d74b96e1346d3f03fe6584f764f4","modified":1580486111321},{"_id":"themes/next/layout/page.swig","hash":"e61d64c055b6497a04affc143f47fdd0a6dc495b","modified":1580486111321},{"_id":"themes/next/layout/category.swig","hash":"c55debb2588e4746b02d31ec249bf0a84fdea260","modified":1580486111320},{"_id":"themes/next/layout/archive.swig","hash":"26526c09a4334099e2141456697696fcd1f9783f","modified":1580486111320},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1580486111327},{"_id":"themes/next/layout/post.swig","hash":"382d9f9a9b35e1f369585f7f9f9b5dd6fa58d2f0","modified":1580486111321},{"_id":"themes/next/layout/tag.swig","hash":"7ff6e34d557a3da1c6a29ecd97842bf73ff213dc","modified":1580486111321},{"_id":"themes/next/source/.DS_Store","hash":"08bfefa242670458d4f0547c79bdde7dba4466ab","modified":1581776121512},{"_id":"themes/next/languages/zh-HK.yml","hash":"6e333aaca52dccee8e5f2d1dc28b0a6cda7b0c9a","modified":1580486111302},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"8ae029084b9ac482adf0fae2a0979dd388476513","modified":1580486111283},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"89667adbb85c25716dba607cd7a38191acf60736","modified":1580486111284},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"789a3cceb8f37a4b63b1fb2452a03332a3c365ed","modified":1580486111284},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"7a9526f749205c882d672a4f51e6a3033c80ca6e","modified":1580486111284},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1580486111293},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1580486111293},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"fe3f5cda1975114884d84bef384a562920d70335","modified":1580486111294},{"_id":"themes/next/docs/ru/README.md","hash":"41b1bef32fb991410ebf559b4c45022549f95215","modified":1580486111293},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"4245fe9472647226692fcbdd5a52d6e6dcd251bc","modified":1580486111294},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1580486111295},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1580486111294},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"17d7203e85a8ce9760c42a853dee0f26a8f7ee4e","modified":1580486111295},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"7f37327bbcae7ed7d04d187fd5e9bc6bbf14926a","modified":1580486111297},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1580486111296},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"f0ffb74de522749c9f2fda46970a61bdafbfbc24","modified":1580486111296},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"0b0b9ec6ec4a89e701a3b91f8d7d95752d3e241b","modified":1580486111296},{"_id":"themes/next/docs/zh-CN/README.md","hash":"b6a3611d40863c12804c5846994786119ce3b79f","modified":1580486111297},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"e2c9db54cc9e154e882008fde6588b065fadc9a7","modified":1580486111303},{"_id":"themes/next/layout/_macro/post.swig","hash":"87a40f829ae6bb786c6c51bd83d110140eb41b80","modified":1580486111303},{"_id":"themes/next/layout/_partials/.DS_Store","hash":"cf278c491e5ea5b7a64d8912864ef04743f5a2f8","modified":1582733504386},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"815676d904f92748ddf4f529bed2baf066997bc6","modified":1580486111303},{"_id":"themes/next/layout/_partials/comments.swig","hash":"0c4914a5fd08f15beec71940218c814ad9a89f3f","modified":1580486111304},{"_id":"themes/next/layout/_partials/footer.swig","hash":"1ee6335c12773dc43f8b92136770cb10d460c25c","modified":1580486111304},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1580486111307},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1580486111310},{"_id":"themes/next/layout/_scripts/.DS_Store","hash":"6072e39aa6a785b555ea959c7c6660d61556bb34","modified":1582733533798},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1580486111311},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1580486111311},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"a3462c37ab6d7642b1e95860ea5c4cfbac78efab","modified":1580486111311},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1580486111313},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1580486111313},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"8627c8c8b031ecee16c522433b66fa4d6979b8ea","modified":1580486111314},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1580486111316},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"dd6bd817cb69b5ad5e9746498146314b54054ff8","modified":1580486111318},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"da6a9d14ed10203e378c6e2c00a7b5e7afabca58","modified":1580486111318},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1580486111326},{"_id":"themes/next/scripts/filters/locals.js","hash":"5bbfdc1c373542159660b7a68ed0b57ca18ad10b","modified":1580486111326},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1580486111326},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1580486111326},{"_id":"themes/next/scripts/events/index.js","hash":"9047d8ae2670e43429b16a7919a08a0a0a81afe0","modified":1580486111322},{"_id":"themes/next/scripts/filters/post.js","hash":"f2f566f2577c554377fd704442399acdd14a8118","modified":1580486111327},{"_id":"themes/next/scripts/helpers/font.js","hash":"32268fb4c59c5b37c1eb1c9582ab630e09e5cc7d","modified":1580486111327},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cb211b6b50913454b1737782e9e2af96cfa40448","modified":1580486111327},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"58347687b02f7ab5e64bef07525c8efa97c9e8fb","modified":1580486111327},{"_id":"themes/next/scripts/tags/button.js","hash":"1d1d25f7e579d92fa563778dd0f163e8eda190da","modified":1580486111328},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"20e392b8583ba6ae5037449c2c7e191d3927641b","modified":1580486111328},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1580486111329},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1580486111328},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1580486111329},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1580486111329},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1580486111329},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1580486111330},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1580486111329},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1580486111330},{"_id":"themes/next/source/css/.DS_Store","hash":"2035805fa490a69e4d7ab2b6a76d6786eafd5fc7","modified":1581776121509},{"_id":"themes/next/source/css/main.styl","hash":"68c3377b643162aeaae2b60c196486fdb3b509c3","modified":1580486111359},{"_id":"themes/next/source/images/71580375662.jpg","hash":"9e1d6e5c8111ff3f5a9172d712096e223b57d24d","modified":1580482186428},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1580486111359},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1580486111359},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1580486111360},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1580486111359},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1580486111360},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1580486111360},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1580486111360},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1580486111361},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1580486111362},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1580486111362},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1580486111362},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1580486111361},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1580486111362},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1580486111362},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1580486111363},{"_id":"themes/next/source/images/flowchart.png","hash":"4d4dc3c5bf6c5a9e8025c3e73f5c3b9fd56b751c","modified":1580838637464},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1580486111366},{"_id":"themes/next/source/lib/.DS_Store","hash":"e9e540245d3abf3cc247cec7cb9226bffcd3e10e","modified":1580491806502},{"_id":"themes/next/source/js/algolia-search.js","hash":"813afcc30feee11d59f297f2d5a96f98fbbd4743","modified":1580486111363},{"_id":"themes/next/source/js/local-search.js","hash":"f2e568298c71d2417a1a1c7e56025ce5842b1220","modified":1580486111364},{"_id":"themes/next/source/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1580486111363},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1580486111364},{"_id":"themes/next/source/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1580486111364},{"_id":"themes/next/source/js/utils.js","hash":"a1e70ac9d32697907d5d78f2533d627f4df8ebcb","modified":1580486111366},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1580486111351},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1580486111351},{"_id":"themes/next/source/images/DrawingHands.jpg","hash":"d3339915e14234095b614bd8103e04badf39b2b8","modified":1580839078903},{"_id":"themes/next/source/images/120625074322660dd39deed2d7.gif","hash":"3e37402766bee0f297048a9a825e584f287b1ee8","modified":1580839099040},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"1638483d2d2dad1da4c841a6fb9f6ee96b850187","modified":1580486111304},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"e6076865dba066c5f0008e22217efb850d5af69c","modified":1580486111305},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"30528a8da30994b1ef9355a72b09b2cd85a7c0e9","modified":1580486111305},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"7487ca8f0e4b16351ea0d6b35dc52b0d32176d57","modified":1580486111305},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"54ba9508a901c295a02c8e34e9cece7c7dcad518","modified":1580486111306},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1580486111306},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"bbf0c8e42491fac70f4f8165224f1d7d92a040d7","modified":1580486111305},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"0172055d118d1d7f4c8379c8495c1ee1aa50c7d9","modified":1580486111304},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"94d54b0c65d504f772af1e62424952e092b6c21d","modified":1580486111308},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1580486111309},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1580486111309},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"a56e4f6ad95c106f361d354f828d1ef4810b1d76","modified":1580486111306},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"1b7faa20a458b46369779b57bcc695bbc5d1c13a","modified":1580486111309},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"7fa01334a0ba84500e920bb9202baa08067d2ee1","modified":1580486111309},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"733d6874aa4f50d1071e670a554508a5a0094eb3","modified":1580486111310},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"716b78cd90addc4216413719554721cb362b0c18","modified":1580486111310},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"a2bb0bec243685e670b60a3d54142950adc03af0","modified":1580486111311},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"14c33bd544903e74388739599fffe3ecb66ed4b0","modified":1580486111308},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"d6fa9e4432b87004c5678dfe2d4b2c1f4a702b93","modified":1580486111310},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1580486111312},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1580486111312},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1580486111312},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1580486111312},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1580486111314},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1580486111314},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1580486111313},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1580486111314},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1580486111315},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"fdcf006e1ba2e53eab65e901b6c63159538307ef","modified":1580486111315},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1580486111315},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1580486111315},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"d3dc3e051e6816cdd576d00cc70b18b8a4c6a495","modified":1580486111316},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4d6f9e09ca4056ff6a5d4923e202126a75242183","modified":1580486111316},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"ed236103bccbcf608f7d5d5b33b9f995d2f1a7de","modified":1580486111316},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1580486111316},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1580486111317},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"fb27a38f9a4b8fcba4f637b03904f7a83cc73416","modified":1580486111317},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"e456d7a2aaabe55447f78cd952b30d70a6c1e742","modified":1580486111318},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1580486111317},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1580486111318},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1580486111318},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d7258d02bcf0dac6c0fd8377c0909ddecb09d1d4","modified":1580486111319},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1580486111319},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"0ea0bac09b0747bc16fde852164c0eaab2efe02c","modified":1580486111319},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1580486111319},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"801e1d2f59f7d2db4096c4788b8469b4165f4965","modified":1580486111319},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"4958fa997ff6df2b2ce05341f40cc3a81b0f91bb","modified":1580486111320},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"1f20213af8da3127701e6bb9da995e5c91be2051","modified":1580486111323},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"7fa72dc60c078842979861622839b109683e05a3","modified":1580486111320},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"0803d4f4d3d02c24417c163ad0b27b60fda79250","modified":1580486111323},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1580486111324},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"19cbd24880d0fbbd4d5698cd54da598f03b942da","modified":1580486111324},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1580486111325},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1580486111325},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1580486111325},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6a72b5928cdab9526a288177991e4b2aedd028cf","modified":1580486111325},{"_id":"themes/next/scripts/events/lib/config.js","hash":"b205d72a56b1827681f0a260c266e0c02065fd08","modified":1580486111322},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1580486111322},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1580486111323},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2315dd8a7a2c7aabd29efa6193df08e805cb15fc","modified":1580486111351},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0a25f3df1b5c39794365efde387647da81da884a","modified":1580486111352},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"644c1f0b02be9bf59955ebdf496136b3fa4b660b","modified":1580486111352},{"_id":"themes/next/source/css/_schemes/.DS_Store","hash":"5aa2f9c1c5d45f7fdf5b2910005ba0358a8ad7e4","modified":1580766633744},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"50bc57e66331c0f15a4527010b4ca3316ac92403","modified":1580486111358},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"52550138127ae9ebbe049bcdacd94d767c003855","modified":1580486111357},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c261d685c5ed0df34718d94bb2ba977c0ed443e6","modified":1580486111357},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"b9d7058db61df7bbd2b58779efe45621a06ffc18","modified":1580486111357},{"_id":"themes/next/source/css/_variables/base.styl","hash":"cb86d5bc11538501eb0da2f3cf63c530831f92a6","modified":1581777260828},{"_id":"themes/next/source/lib/bookmark/.eslintrc.js","hash":"76bec2314ec3477b18e357f9992cf42a9524a3f7","modified":1580491473003},{"_id":"themes/next/source/lib/bookmark/LICENSE","hash":"1440355648a72be5b7625fe6fa055b462fcc78b0","modified":1580491473009},{"_id":"themes/next/source/lib/bookmark/.eslintignore","hash":"3d3910611724b7584a29297a3f7132334a3ad091","modified":1580491473002},{"_id":"themes/next/source/lib/bookmark/.gitignore","hash":"5410a1bef9807f666cd92a0d2020f700e67e4096","modified":1580491473008},{"_id":"themes/next/source/lib/bookmark/README.md","hash":"ecfb09270cf202fb2228e9a79fa970b60be69222","modified":1580491473012},{"_id":"themes/next/source/lib/bookmark/index.js","hash":"5e5cba645a1a4531ccbb4782df2f7a075626393f","modified":1580491473015},{"_id":"themes/next/source/lib/bookmark/package.json","hash":"013ff96fc03cd4ed7b17059af207e53f94953202","modified":1580491473017},{"_id":"themes/next/source/lib/bookmark/bookmark.min.js","hash":"9e525329553335c2484f6faf9e933a6bbee9ab6d","modified":1580491473014},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1580486111367},{"_id":"themes/next/source/lib/bookmark/renovate.json","hash":"cb29cc16e61b0b8a6dac34657d76822ae29ad5aa","modified":1580491473018},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1580486111367},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1580486111367},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1580486111367},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1580486111368},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1580486111380},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1580486111380},{"_id":"themes/next/source/js/schemes/muse.js","hash":"ae2a2502b77203e83f75a040c43e86a7ecb4873c","modified":1580486111364},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"f068b46f8c305c7436c2767492a6bed42dcd764c","modified":1580486111365},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"170c4598cbbe49cd1527f94158d97d2320a6b906","modified":1580486111338},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"112f5e8f1fe5cec4419e87acfbdef0e615ed23f3","modified":1580486111331},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"bc87cea0b534f2d75db60f300b069456f6516d1b","modified":1580486111338},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"80d359661d08b80ad561b97f8508766b3e1f6d01","modified":1580486111331},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"44fe82eadbdbb2f66adda37ac83ebd0f85876bfc","modified":1580486111343},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"990bd301ce2de0a6b936781c58318f3945d81bc2","modified":1580486111343},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"47ee915d7b0a97e74140a25fbfc01c04d6781534","modified":1580486111331},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"4f7879a50d4608c46cc2061c725a2564597a45bd","modified":1580486111346},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"d8ba44b8e1a0332c5c1079ff65fc83d2918a5865","modified":1580486111347},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"3faa8a7cdb05ef3f3b02920d381773dfd54270a5","modified":1580486111347},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1580486111349},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1580486111348},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"8e1cc5e3b20d804a7265f945b877388bffee39eb","modified":1580486111349},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1580486111349},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1580486111351},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"b8ddb39a17d3ffefde8ad3012eb9eeb78ef7d87c","modified":1580487869985},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"1693ec6b53758ac15d2c7798c789d6ae8af913ea","modified":1580486111352},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"48743ac61af37a4de2026667e15a65de5e8cf542","modified":1580486111353},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"48dcffe0012ccfbbf3269a86cdc575c7ee1acf84","modified":1580765988676},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"3646e915b0a55f3b66e41d802b082aba88a76e06","modified":1580486111354},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1580486111353},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"a54662bd4cbbe316048a811d3b5c83fb6df63739","modified":1580486111354},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1580486111354},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1580486111355},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"5adf2fdea25630893283e1ad5ba1721b698d6e95","modified":1580486111355},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1580486111355},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"b797c693378d94160121a2b8d9df0622a76cdbda","modified":1580486111356},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"f1f81bca31e82ccbd375f0cb9fb8dbb3beac810d","modified":1580486111355},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"71a3d7f2242706f7a6b79933a67ef3664cca3a24","modified":1580486111356},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"84a3b8fe07fc790082ee988dd8f6c2536fde72de","modified":1580486111356},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"24a086a6904bbf5355a354403c9b0e6069f7eb01","modified":1580486111353},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1580486111357},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0424a1fcffa1ae82fe70935972a894aca885bf9a","modified":1580486111356},{"_id":"themes/next/source/lib/bookmark/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1580491472974},{"_id":"themes/next/source/lib/bookmark/.git/config","hash":"7c5ec419dd0549da89bdd8376a1327f19049f461","modified":1580491472982},{"_id":"themes/next/source/lib/bookmark/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1580491467937},{"_id":"themes/next/source/lib/bookmark/.git/index","hash":"9585e5a45afc9a782a5ae66047bc65e7123a3484","modified":1580491473019},{"_id":"themes/next/source/lib/bookmark/.git/packed-refs","hash":"5133cf40bb12de7e6665640ebaad6c6f6913c693","modified":1580491472967},{"_id":"themes/next/source/lib/bookmark/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1580491473005},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1580486111368},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1580486111370},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1580486111370},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1580486111377},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1580486111331},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1580486111332},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"6336c2b129db802221b1fd75e5fbe8aab85c0a1f","modified":1580486111332},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1580486111332},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"7ddb7453bf9b85b01bff136e9d10a7f06baac9e8","modified":1580486111333},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"e3ade812b5541eca5b863ad3ff234ea95925bf31","modified":1580486111334},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"4b84f35e7040f9adb5cc540c366d7f9eb4c48bcc","modified":1580486111334},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"af182c0d1e52f94280f6108936914f04ed541eee","modified":1580486111333},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"5d5c022aa3b2f89c2f2a178212338bb64804dd75","modified":1580486111334},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"60ed14e9ddcb138837ca22efb8886f9bff2a3dd2","modified":1580486111335},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1580486111335},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"9df5ba77e6cf36129bddc270407215c23c60ff38","modified":1580486111336},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1580486111379},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"bcba503e956e4b737b062faa66341bd880f16c10","modified":1580486111336},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"eca4d80dd0df1c3b1bc06bd39e6a4bd6c56198df","modified":1580486111336},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1580486111336},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"c7939407797acbd1ae0d8bae8e13b2bf045f870e","modified":1580486111337},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1580486111337},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1580486111337},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1580486111338},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1580486111338},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"a48ed1dc9978b1b621f0e12664738c0d855d8013","modified":1580486111339},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1580486111339},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1580486111339},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"0672ea2acf28dcc2cfc5244da36d3387d71a17cb","modified":1580486111340},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e02b1097a72a7d2ddc45ea8d53aa6d77c25ac407","modified":1580486111340},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"f5821481440a0624c8aec5fc85f093de1527095f","modified":1580486111340},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"4b237e2344f35e9d1f6dbc3842d5e432d478ebfd","modified":1580486111341},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1580486111341},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"d5d85d3646d184e0340924addcfd2523fb289d00","modified":1580486111341},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"a7ed54e2f52185a7b6bb9a8201f6c3aa74b0cb00","modified":1580486111342},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"c27b3056d5e22d3c66d8a152a23634314d5c4a60","modified":1580486111342},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1580486111344},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"9b3ff4aa24069eab0e9771437013f45e450d4217","modified":1580486111344},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"95339b71ac90553fb1634c536c9749055e0c788a","modified":1580486111344},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"a237c290e8934d1a8cbbf22b3f30503d9663021d","modified":1580486111345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1580486111345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1580486111345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"521534f483440434e808f92377bc3fc73667c89a","modified":1580486111345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1580486111346},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"6d8680894e24a516e2b5263af86b485767c3be63","modified":1580486111346},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"9a18b186b08ec220d1b17cf83812bcdd06077814","modified":1580486111346},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"b492a45422773ab2af06ee345d527ba4c6bbc608","modified":1580486111347},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1580486111348},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"b4923515ca8e44aa62e839ce948f759cfd1f896f","modified":1580486111348},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1580486111348},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"7213e3d0ad7c95717ecd4e701d6ee9248ef2bf9f","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"5bf28a03ef021c27cfd713971caca5a0b9466fd1","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"1a14c1b92d8a4dd8aabb5949333ac0ac79094c6c","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"2dc2a5b7becb11de1d4bdab6b5195588ae878cfc","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"d2f0f2171722533bba308f944a2ec727b083582c","modified":1580486111350},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1580486111351},{"_id":"themes/next/source/lib/bookmark/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1580491467962},{"_id":"themes/next/source/lib/bookmark/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1580491467952},{"_id":"themes/next/source/lib/bookmark/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1580491467941},{"_id":"themes/next/source/lib/bookmark/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1580491467954},{"_id":"themes/next/source/lib/bookmark/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1580491467964},{"_id":"themes/next/source/lib/bookmark/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1580491467945},{"_id":"themes/next/source/lib/bookmark/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1580491467943},{"_id":"themes/next/source/lib/bookmark/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1580491467966},{"_id":"themes/next/source/lib/bookmark/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1580491467959},{"_id":"themes/next/source/lib/bookmark/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1580491467960},{"_id":"themes/next/source/lib/bookmark/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1580491467934},{"_id":"themes/next/source/lib/bookmark/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1580491467968},{"_id":"themes/next/source/lib/bookmark/.git/logs/HEAD","hash":"4dab6f07c3323cf225fe9cae35fb6769396b8266","modified":1580491472977},{"_id":"themes/next/source/lib/bookmark/.git/objects/0d/61683ea74054da2645effdd51d0a3282d6a9ff","hash":"879563c866dbe7efa0074a1ebcd206723e552f27","modified":1580491472559},{"_id":"themes/next/source/lib/bookmark/.git/objects/0c/dada082d621dbfdd00f7020c33dc751129167f","hash":"b490c11cdefde6b331a7d4ddb055e34ad08459d8","modified":1580491472605},{"_id":"themes/next/source/lib/bookmark/.git/objects/0a/9e6b66e9d710a68e08fcc295fca440eabcb4bc","hash":"aa0b269640b41735ba54a9279f10ddd19ecd5887","modified":1580491472572},{"_id":"themes/next/source/lib/bookmark/.git/objects/02/b27223487ca819cf7167dc23626af595112e25","hash":"e5bbb1906aa9a8e2696817d19d85aeedf82539ed","modified":1580491472849},{"_id":"themes/next/source/lib/bookmark/.git/objects/07/4d4d38a37d506f5db482b5c27ad1143bba5aab","hash":"ee76158694b6490056328594b5d15a54faba253e","modified":1580491472778},{"_id":"themes/next/source/lib/bookmark/.git/objects/11/8348fe672067e7d9c32c73d586023804eeb565","hash":"2b73c34eb535ff7e7d91cefb5215feddd9f2c36f","modified":1580491472743},{"_id":"themes/next/source/lib/bookmark/.git/objects/17/ad1e81c2805cc9c5c00846c8bf9bf16524b55a","hash":"8f779f28267873af1e5c71386603042dbaff6211","modified":1580491472769},{"_id":"themes/next/source/lib/bookmark/.git/objects/16/75d42fcbd2aded2e9b42efe38c347ced893495","hash":"979ec745cfdc8dd979785b78c9be7645bb73d922","modified":1580491472709},{"_id":"themes/next/source/lib/bookmark/.git/objects/1b/db4ff581f77318e6c4783d8e1ce811c37f13bf","hash":"06b3da6e152ffd68ed4617db1c4e3912f41d8bec","modified":1580491472562},{"_id":"themes/next/source/lib/bookmark/.git/objects/25/42ea91169dbe9da086918ad02160e42310e185","hash":"3c942c82f3ef7fcd9a15276c82fcf59fa141521c","modified":1580491472875},{"_id":"themes/next/source/lib/bookmark/.git/objects/29/de98ce86c01618ebbe3352e213db74feda0dc0","hash":"046bf7ab442c095f0150c96fe06d83855c92092f","modified":1580491472716},{"_id":"themes/next/source/lib/bookmark/.git/objects/18/77388dd9427b36d7c8703038a215b79f1218c2","hash":"8d48c7c0d7427116dbeb32552c576cc4edbe2d38","modified":1580491472856},{"_id":"themes/next/source/lib/bookmark/.git/objects/2b/d59834388b6377fd14413f2614f550e35e35f1","hash":"654763c707c65aa3f353f8a9edb0cc0ae4bcccfa","modified":1580491472595},{"_id":"themes/next/source/lib/bookmark/.git/objects/2b/358da01f778126b38fc4ddf5ecafa625cad38b","hash":"d4613d86e15d9679b8f1f18950b71a182af4b5c8","modified":1580491472557},{"_id":"themes/next/source/lib/bookmark/.git/objects/2f/9eba51ec174b1e0c719d12cafa7c3c07140471","hash":"fc994d9d8b3b21ec7c941eea7e3862970e297e9b","modified":1580491472609},{"_id":"themes/next/source/lib/bookmark/.git/objects/2f/425e03cfa918b2d638bbd37279a8b1e7757508","hash":"283713eda90efb64849519db267b652facfdbd63","modified":1580491472727},{"_id":"themes/next/source/lib/bookmark/.git/objects/36/6ee3e25fa36be8ea88ed1c7913a8de8fd3b820","hash":"81c45f061381dd947da576758992b64b6101fbc8","modified":1580491472704},{"_id":"themes/next/source/lib/bookmark/.git/objects/3f/30e2e442e4fdc1e91d5d1b642662e3900e5d6f","hash":"358397c32831b80fa4828334de7080bdaafe1c52","modified":1580491472853},{"_id":"themes/next/source/lib/bookmark/.git/objects/40/b878db5b1c97fc77049537a71bb2e249abe5dc","hash":"929471c4b27858bbdae034381982f996144f6b3d","modified":1580491472613},{"_id":"themes/next/source/lib/bookmark/.git/objects/41/bde8ccd130442ef0b3f8b24764a6d72635a485","hash":"4a833bb669ac126a00dec7153f8c263eab47b024","modified":1580491472888},{"_id":"themes/next/source/lib/bookmark/.git/objects/42/c32aba927940c9e92b9928dceee88eacbf4874","hash":"2c7b6bb078f2eb52eead906df3ed4fa136a1e651","modified":1580491472498},{"_id":"themes/next/source/lib/bookmark/.git/objects/41/fcb40408cfe2e9e0d1796ce1384024f7ba2c1c","hash":"600f3ee28c5dc2f1b21f8d9043d271b9601750b1","modified":1580491472449},{"_id":"themes/next/source/lib/bookmark/.git/objects/48/7e5a296eebd45809199d2f557e340603931bad","hash":"6cf9f9b5ae990bbb0083b37b0c29b269cd8cc735","modified":1580491472814},{"_id":"themes/next/source/lib/bookmark/.git/objects/47/1dd1f3ddf970ee3418f801ae787bf98191d47e","hash":"8634518aeeffc4b8b0dbe6ca6159480264cf1916","modified":1580491472462},{"_id":"themes/next/source/lib/bookmark/.git/objects/4a/6e0d94cae4e01312054983b4e2e4007e3fd3b8","hash":"9eff6f1c82336ebdab0503e0ab0f753cd18f01ff","modified":1580491472582},{"_id":"themes/next/source/lib/bookmark/.git/objects/4a/16717bb5a449fab2291e6397c30c6cb34814f1","hash":"5e244da7b9b6dfc9ad924ee5259805d4ae272c2e","modified":1580491472747},{"_id":"themes/next/source/lib/bookmark/.git/objects/25/b729e9ad0151b51f50297ea35280111b953236","hash":"af4bf7f3feb175c6980ee81a84f679149f713987","modified":1580491472757},{"_id":"themes/next/source/lib/bookmark/.git/objects/4a/7e8d64b8956ffd9339000e62d490f18dcd3ecd","hash":"3cc8fd65b3663a750887093f67bb28696492d5e5","modified":1580491472884},{"_id":"themes/next/source/lib/bookmark/.git/objects/4e/7ec652ef197de221d6576a717edf0b583a688a","hash":"e45f3b02a04d04eb3bcd6373ae79d86cbbd62798","modified":1580491472802},{"_id":"themes/next/source/lib/bookmark/.git/objects/55/31419f7d03a523182be5373e246e43acdf60d7","hash":"274085edc584079218a79c199bd0c7360a700290","modified":1580491472547},{"_id":"themes/next/source/lib/bookmark/.git/objects/58/b257bc636adc71871da51e615e6734c9479f3a","hash":"4e63de59c359c9bf480b47aa5e342ed78663fb58","modified":1580491472872},{"_id":"themes/next/source/lib/bookmark/.git/objects/5c/ffa97fb277e628a550bc1d8f7fa7c5c776129f","hash":"8c4e4a79d14522f34280ef85c794df6f4886e040","modified":1580491472712},{"_id":"themes/next/source/lib/bookmark/.git/objects/5e/038b5098480c14a29cefd944e3eb23109473b2","hash":"9c3aba7ff1afc70708a90e2f4f610499ee268d4a","modified":1580491472886},{"_id":"themes/next/source/lib/bookmark/.git/objects/5c/02525c8b55f89f0ec0304cd0d266453bdf0541","hash":"51cab03e6cbebdb421eea45818b1ff7d9ec7783f","modified":1580491472776},{"_id":"themes/next/source/lib/bookmark/.git/objects/5f/1541db4b58002e45b9b5fe19426551a8239a49","hash":"9a50dcb663e39892adbfccab943fb8dc37add2d0","modified":1580491472822},{"_id":"themes/next/source/lib/bookmark/.git/objects/61/4795cc61f429929887fde3962027254db90a62","hash":"c4e415dda2e421df728dda51f953cc2f01296378","modified":1580491472869},{"_id":"themes/next/source/lib/bookmark/.git/objects/62/150c9d8f54ca62e1af3c9d666bb5e35eb3bdbd","hash":"2998eaea45a0135e879da15851a535ae2940d85e","modified":1580491472553},{"_id":"themes/next/source/lib/bookmark/.git/objects/60/438c0c99eece781f9c7b2779ab7673ab34ba62","hash":"76032e20bf2916b03d9f795982831868f42a03af","modified":1580491472707},{"_id":"themes/next/source/lib/bookmark/.git/objects/6d/1188177a8798893f9efe275d4d75d0f9301be9","hash":"b8652b3249ef6662a8dee7c01e5667f7e4f33cc4","modified":1580491472565},{"_id":"themes/next/source/lib/bookmark/.git/objects/69/d5670af93cd1d3f0acc082583b1d150c5d61b5","hash":"e6c6fdb25f5fc4949594e217ae14d309b2fba4a1","modified":1580491472466},{"_id":"themes/next/source/lib/bookmark/.git/objects/74/dba5056028cc4bac4f79ffec7da1e076260ffd","hash":"cd7bbf512f3c0291badb58f0679485c18a98c034","modified":1580491472736},{"_id":"themes/next/source/lib/bookmark/.git/objects/75/4a45e75b8a6dc256755287312c729adc2fc6c9","hash":"e700708d93a902401e54d96d4d632ae4225cdf2c","modified":1580491472783},{"_id":"themes/next/source/lib/bookmark/.git/objects/77/d7bbc93f0a411d2e50b7c67a2827dece42727f","hash":"6865e0eb54a9882e55e70669a44cdddb2da1665e","modified":1580491472570},{"_id":"themes/next/source/lib/bookmark/.git/objects/79/c8a354ccacb4edc27acbcc503f26acf6119624","hash":"7a44189d1189ad7aedea595fc8a4ef387de7041d","modified":1580491472701},{"_id":"themes/next/source/lib/bookmark/.git/objects/84/08abfa6c03a2809cb160da5badc6d1050bfbb3","hash":"9d3f29a89223403d1a0292681a330cbf7616c190","modified":1580491472459},{"_id":"themes/next/source/lib/bookmark/.git/objects/7e/58471cd13382a6bafd05015cd0a5b5a8e899eb","hash":"21ae04d7ac48092d7b07b7278bf099cfc2691e57","modified":1580491472722},{"_id":"themes/next/source/lib/bookmark/.git/objects/8d/ae3702857a081653c16320999ed63f3c6fb719","hash":"cc5ed8d3d17451c722a3ebfa3251fff27d348218","modified":1580491472837},{"_id":"themes/next/source/lib/bookmark/.git/objects/85/839f2b88dfaef92ae7c84c10f446e98d07b852","hash":"9662dba024cf43e039d41f4253e7aa282e2141ba","modified":1580491472454},{"_id":"themes/next/source/lib/bookmark/.git/objects/8a/7f4f27cc671188881ca5cd80d941720a78be04","hash":"de24124a9660dad2ee4e1d25a3580ac87d42b3e0","modified":1580491472749},{"_id":"themes/next/source/lib/bookmark/.git/objects/8e/b07c89ff54bda39a40d6448a6a8ef4df57c8f2","hash":"fadb84b2a2c5dee0b8f67dd53b4e2b8a5b270e92","modified":1580491472694},{"_id":"themes/next/source/lib/bookmark/.git/objects/97/b44840ca6d92e3e650428f433b75a501b1427c","hash":"a899d561a89c4ab798063c1df81d78015992ad15","modified":1580491472491},{"_id":"themes/next/source/lib/bookmark/.git/objects/9d/a80d05852c1c5aec3e8d31b63d429bcfecc0c5","hash":"392385eddea633b0bea1194c2c66db593c6a9c0a","modified":1580491472600},{"_id":"themes/next/source/lib/bookmark/.git/objects/96/353334c808a19403f7d2f0e880668bf8f0e320","hash":"dbcb9efdc52a50d5f0431394c24b35ff47f1833d","modified":1580491472483},{"_id":"themes/next/source/lib/bookmark/.git/objects/92/f5900e81d1cf9e90787b7138f4285434e45c10","hash":"05c5b8557d69133fd01ebd1b85a5e009a822ca20","modified":1580491472773},{"_id":"themes/next/source/lib/bookmark/.git/objects/ad/b3297e18bb4d11be9f7305db144e5015f40738","hash":"b44f4417307f3dbb33d30c22700dbfb61c87f198","modified":1580491472798},{"_id":"themes/next/source/lib/bookmark/.git/objects/aa/d78f9de11c827340e4695cbc1f5a73f53ee577","hash":"0f5086a5818caf093dcf60126a7d0f72bc73928b","modified":1580491472787},{"_id":"themes/next/source/lib/bookmark/.git/objects/b5/c8b8aa1a6dba7c8152b49ef103a0952f1bb9ca","hash":"d070cad9094ddcf44be0e16b3872f1fb4f05ae0d","modified":1580491472494},{"_id":"themes/next/source/lib/bookmark/.git/objects/a9/21fc59cd52b57fc3093fbdc04b446bae0a2f8a","hash":"a2e879b7fdb0cf153f8526064a6b8810b2cd8988","modified":1580491472780},{"_id":"themes/next/source/lib/bookmark/.git/objects/b8/e5e058b339881ced4b4b5c4d89f337ccfb67de","hash":"0d229cce5fbe7e41e4961070dd2e4d76c7b3efb6","modified":1580491472587},{"_id":"themes/next/source/lib/bookmark/.git/objects/bf/c91bf59adb7231937b98d35cb952b4bab01fff","hash":"016dc36835bb17ba3f54eebaef4d57f5498015da","modified":1580491472544},{"_id":"themes/next/source/lib/bookmark/.git/objects/bc/0570da6c07cf361d794008e58455455a2580be","hash":"822af280dbbe78499a932faf9229f4c467d48cbe","modified":1580491472785},{"_id":"themes/next/source/lib/bookmark/.git/objects/bf/05dfc0c17af4b48e94676e1e2feaba911d8c71","hash":"4d8adbebcb8ffb4bc3c61b9c97bd225f128b8e74","modified":1580491472568},{"_id":"themes/next/source/lib/bookmark/.git/objects/c7/631242bf4ea5781b2d173d999b41c69fdd8ace","hash":"f2d3b9fb648bed7f61dc917d9ad45550c4c70e77","modified":1580491472894},{"_id":"themes/next/source/lib/bookmark/.git/objects/c6/26688a33395276a262158e171027a61c56150c","hash":"3727c122ab41e9dd1abc6490a40eb28f212c5c8e","modified":1580491472471},{"_id":"themes/next/source/lib/bookmark/.git/objects/c1/7940ffb480f454472c9083efabce723ff53cd4","hash":"29bbded01d4399f509abd809c00957f182226432","modified":1580491472729},{"_id":"themes/next/source/lib/bookmark/.git/objects/c3/75bd4420a3baa49b9a5d52f8040400491ac933","hash":"beb87437d1591c6bec51a6e85176f576092a83f2","modified":1580491472852},{"_id":"themes/next/source/lib/bookmark/.git/objects/ca/799c34c37f63bd64ed9b49413e7fc6db8ab2ef","hash":"4e6ee2dffc199b5954fb34b129fe2f012099463d","modified":1580491472577},{"_id":"themes/next/source/lib/bookmark/.git/objects/cd/ea049768c3f2d2b89792c19a523584590292c5","hash":"5e69fbae98becc6ed3be947a2450511294252891","modified":1580491472790},{"_id":"themes/next/source/lib/bookmark/.git/objects/d2/45f9acfffc2e168c17bcc4e115527f21a77521","hash":"76d31ac0c9ca563b5854fe5b6c5ab46ebb202680","modified":1580491472818},{"_id":"themes/next/source/lib/bookmark/.git/objects/ce/70a57b7d6bcddae01e3434d416c27c84b24727","hash":"d8ccbf99469b51ae629347665f96354c264457b4","modified":1580491472753},{"_id":"themes/next/source/lib/bookmark/.git/objects/d4/d4c40411de7936e90c6ebbc19e182ad225cb4c","hash":"f8d44f87905d5dfbeb1fdf5519a5256d27d00120","modified":1580491472866},{"_id":"themes/next/source/lib/bookmark/.git/objects/d3/a06b992cfe7d26e022b14abf5ba563e6e63c4a","hash":"6820962ea4a4da9e9de834c97c1630477c30ef20","modified":1580491472683},{"_id":"themes/next/source/lib/bookmark/.git/objects/db/3b3566df6e7d89575c4150067f61087be1bedb","hash":"118afda770f93908ad97d6ad0bb6a42b21aab9cf","modified":1580491472479},{"_id":"themes/next/source/lib/bookmark/.git/objects/d6/31561b3edf93b81d38f81278d6574c77006e39","hash":"afaad8a2c6944b11695122cf45cedadbc3bd7605","modified":1580491472792},{"_id":"themes/next/source/lib/bookmark/.git/objects/de/52a356f671fa73a9d9910c26968d47100c655f","hash":"5bd5f521df149c8e24640151b6d83b3c6be1fd30","modified":1580491472475},{"_id":"themes/next/source/lib/bookmark/.git/objects/df/af7173f5d152681fb2bc7adfdebb54ad195281","hash":"7fc27717b2f7da8f6a6e40ac8d88c75485a505fa","modified":1580491472688},{"_id":"themes/next/source/lib/bookmark/.git/objects/e7/aba3f8ec35017b4dd2bc8f5ca94253a8674ce7","hash":"8fe85003f71bf38e6ae367366ae6e95a819d046d","modified":1580491472457},{"_id":"themes/next/source/lib/bookmark/.git/objects/ef/090e162cd7cc22d2da95d77acac1d7a1eb9c43","hash":"a6997644fb850bedaeeb01adfc90fc404cf80074","modified":1580491472697},{"_id":"themes/next/source/lib/bookmark/.git/objects/f4/5d8f110c3034162a1091dafe4b03d2e56b323e","hash":"963dad8248030a8d7b185f4726e40a65a0583c0e","modified":1580491472732},{"_id":"themes/next/source/lib/bookmark/.git/objects/f6/c892849ea799eb017a3d5bf05edb602780cd38","hash":"3b568af69c7a3dc7439155fe36b64a65e8b0bb79","modified":1580491472487},{"_id":"themes/next/source/lib/bookmark/.git/objects/f8/efc68c4efb88e2ff0660752cfe028e78a2f7b8","hash":"4c3753e97e390be8668cb07fececcfd02be084f9","modified":1580491472918},{"_id":"themes/next/source/lib/bookmark/.git/objects/f7/bc8429b09417e69629d9fa0c2874dd5adc30d2","hash":"d8b3844ae942e165b59b6a57c5decdeb35582539","modified":1580491472806},{"_id":"themes/next/source/lib/bookmark/.git/objects/fb/0a1d42d6507805651dec61ecc4df11f37dc1a6","hash":"fcef8ee1a40833b2cf5ecae90bfa22002f46b29a","modified":1580491472843},{"_id":"themes/next/source/lib/bookmark/.git/refs/heads/master","hash":"0ff01610d23a5f9b86dfe1138ddf331d9cd008d1","modified":1580491472976},{"_id":"themes/next/source/lib/bookmark/.git/objects/fc/63af0044284a5b08f20cf1444a062176d835fe","hash":"9994dd502452c757856b88aa0d1be86540dd7a82","modified":1580491472840},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1580486111375},{"_id":"themes/next/source/lib/bookmark/.git/logs/refs/heads/master","hash":"4dab6f07c3323cf225fe9cae35fb6769396b8266","modified":1580491472977},{"_id":"themes/next/source/lib/bookmark/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1580491472972},{"_id":"themes/next/source/lib/bookmark/.git/logs/refs/remotes/origin/HEAD","hash":"4dab6f07c3323cf225fe9cae35fb6769396b8266","modified":1580491472972},{"_id":"public/atom.xml","hash":"ae757b6522f11efb14836f3aa4b47a466fb0c285","modified":1663852835195},{"_id":"public/search.xml","hash":"aec7c6b9ffc83252cfb2c81df29d421dec66269f","modified":1663852835195},{"_id":"public/categories/index.html","hash":"89cca4d26a59fa4ae2d9dd6dbe96e9a22ef427d6","modified":1658991492494},{"_id":"public/about/index.html","hash":"207cc4c88c7fa7ec165f1e3020d43de4cae6249a","modified":1658991492494},{"_id":"public/tags/index.html","hash":"91b3a4a2810bcd0f63412fe2f2cfd276e78c2b8d","modified":1658991492494},{"_id":"public/archives/page/4/index.html","hash":"04d643ece5c5eeb1b0fcc857c1e5266da4f7ef86","modified":1658991492494},{"_id":"public/archives/2018/index.html","hash":"f3310087e0923e3542597a1f2e4ab3958ce03e29","modified":1658991492494},{"_id":"public/archives/2018/06/index.html","hash":"894233e145a695deacb3067ca7b1660b35a1ca18","modified":1658991492494},{"_id":"public/archives/2018/07/index.html","hash":"bc2af18a7c29adae2e4c18e7ee3e04f185179990","modified":1658991492494},{"_id":"public/archives/2019/index.html","hash":"79343aba7e1f885d9fd449c3d9ce36e5ac3490b8","modified":1658991492494},{"_id":"public/archives/2019/01/index.html","hash":"e651671b089ce48c8621374d5ec65e157c850f80","modified":1658991492494},{"_id":"public/archives/2019/06/index.html","hash":"1384d16d51d15ece6e0365f247d4c3e361aed87e","modified":1658991492494},{"_id":"public/archives/2019/11/index.html","hash":"7fb6e13da323fff28b19ad8c54575836dd379d60","modified":1658991492494},{"_id":"public/archives/2020/page/2/index.html","hash":"e894b475fb6d2cf4e600d2e4c97b726320fcdaf3","modified":1658991492494},{"_id":"public/archives/2020/01/index.html","hash":"fb53e0718948fdee8bbda100838cab7c3f1d7abc","modified":1658991492494},{"_id":"public/archives/2020/02/index.html","hash":"4f62607b0b0c244f82ab7b1ead3662b6f325ae6c","modified":1658991492494},{"_id":"public/archives/2020/03/index.html","hash":"ac9c0fd6527864a66dee12bcb4b3ed1eee61059b","modified":1658991492494},{"_id":"public/archives/2020/05/index.html","hash":"a357a804075aec9e23f06b7498764a1beb69ec1d","modified":1658991492494},{"_id":"public/archives/2020/06/index.html","hash":"3fe069872d89f1d540a4c5d522ccdcff4783a3d6","modified":1658991492494},{"_id":"public/archives/2020/08/index.html","hash":"41b0619f152e592a10e601bdb726c3f47fe8c2ca","modified":1658991492494},{"_id":"public/archives/2020/09/index.html","hash":"a34a00a95958e6e9322512adcd289002709e94b2","modified":1658991492494},{"_id":"public/archives/2020/10/index.html","hash":"769f9d4a45d3c2aaaed79cd5f4e7c1b72dd31fe4","modified":1658991492494},{"_id":"public/archives/2020/11/index.html","hash":"c2ee44f0ca4b9bc6843fcb7a5c0c72d823b23bf6","modified":1658991492494},{"_id":"public/archives/2021/index.html","hash":"2e589c04408cac5d97f8e930ea881d2f6805fb5b","modified":1658991492494},{"_id":"public/archives/2021/06/index.html","hash":"1e6dfb7cb981fe2343b6673a7f0c944614b8f048","modified":1658991492494},{"_id":"public/archives/2021/08/index.html","hash":"d42f6827344cba70321fdeb9b1f44281d647b28c","modified":1658991492494},{"_id":"public/archives/2021/11/index.html","hash":"582062a05d0f86762824bc1b4fc768785cd24cb3","modified":1658991492494},{"_id":"public/archives/2022/index.html","hash":"19f20ee5e7ae69c9056fe66da143e6d078798c52","modified":1658991492494},{"_id":"public/archives/2022/01/index.html","hash":"eb526a31fef5eaa1a8ff0b0ddb96a42d4c0c55eb","modified":1658991492494},{"_id":"public/categories/数理逻辑/index.html","hash":"55a994173796a08e167a0decb0c849ee53aaec79","modified":1658991492494},{"_id":"public/categories/最优化/index.html","hash":"3d49daca8f07c9b691277adde626d1693b3660c1","modified":1658991492494},{"_id":"public/categories/杂想/index.html","hash":"21bbe5adc9fe86a277d8d0bff6e057accdb0188b","modified":1658991492494},{"_id":"public/categories/统计/index.html","hash":"6aaae521ae48c81a9a9afa78b1f0cfd3f8ae2a2f","modified":1658991492494},{"_id":"public/categories/相片与画/index.html","hash":"4a4fb80fac7d72671ba94eedaeeee43b3e68d4a2","modified":1658991492494},{"_id":"public/categories/社会科学/index.html","hash":"77d34e5309d459988992a9aea090c2a233538b9c","modified":1658991492494},{"_id":"public/page/4/index.html","hash":"ca3933fc08f9537111bfe8a802ff9147a9461019","modified":1658991492494},{"_id":"public/tags/逻辑学/index.html","hash":"6513b321b71ab0ae37b5078801c406f85e595004","modified":1658991492494},{"_id":"public/tags/数学/page/2/index.html","hash":"3ef89e68463d6403a3361b42587b6d81e040c0b5","modified":1658991492494},{"_id":"public/tags/哲学/index.html","hash":"8f2cb9602866ff0509feeee0bc88059968ccac77","modified":1658991492494},{"_id":"public/tags/人工智能/index.html","hash":"764393effefbc21cadda0171af36fa0180147506","modified":1658991492494},{"_id":"public/tags/算法/index.html","hash":"986b3eee69299e8eef869ed8c3678edc196747aa","modified":1658991492494},{"_id":"public/tags/最优化/index.html","hash":"a40ce06d22f52afd6fcb4327a832118c42061345","modified":1658991492494},{"_id":"public/tags/tensorflow/index.html","hash":"10ca222a037231a07ad215528696091b9c65e44e","modified":1658991492494},{"_id":"public/tags/机器学习/index.html","hash":"c850c36c78756a5aa9ad37dc6bd62864309d70ae","modified":1658991492494},{"_id":"public/tags/GPU/index.html","hash":"7df3e42f270c41061f0b457e4db98103b6e74891","modified":1658991492494},{"_id":"public/tags/深度学习/index.html","hash":"902bce142c85f93da4f478c853c608c8894525b2","modified":1658991492494},{"_id":"public/tags/社会科学/index.html","hash":"0ff0b858ebe73a9b7611d4875d8846a8cd69acce","modified":1658991492494},{"_id":"public/tags/艺术/index.html","hash":"7a406e0739032ee86cbaf2aef9c3e55332879c5f","modified":1658991492494},{"_id":"public/tags/近代史/index.html","hash":"9867ce2fcb402915d688a269fdd5789b7c98511b","modified":1658991492494},{"_id":"public/tags/统计/index.html","hash":"d3d6b6480447eeae2be223cdc286c63990ebce86","modified":1658991492494},{"_id":"public/tags/回归分析/index.html","hash":"e5f18f314e279adf1e2557761c14543690c8bb1b","modified":1658991492494},{"_id":"public/tags/集合论/index.html","hash":"80bc0b94c9a410c2f4b23185f7dacbadddc40810","modified":1658991492494},{"_id":"public/tags/相对论/index.html","hash":"12ca85182080fc802e6c0d7258c35dc68999abd4","modified":1658991492494},{"_id":"public/tags/物理/index.html","hash":"4c3ff4947e4610403583932679f4d945a26d4e68","modified":1658991492494},{"_id":"public/tags/宗教/index.html","hash":"a936fa5567d2625ded4b466f2c74165ede1a9d6c","modified":1658991492494},{"_id":"public/tags/像/index.html","hash":"b44275b659b6d07b885e7ebac4fe2b7e4729a317","modified":1658991492494},{"_id":"public/tags/画/index.html","hash":"8b9638e5965ca63f55c9bc50473f810968ae17e2","modified":1658991492494},{"_id":"public/tags/资本主义/index.html","hash":"0edc6ed9e97a7d6b8775ed93a78b894d63256890","modified":1658991492494},{"_id":"public/tags/科学/index.html","hash":"af2dc6adb40c639a6d94a31b0c7d05f699be3bb0","modified":1658991492494},{"_id":"public/tags/环保主义/index.html","hash":"0bfa46d8005add63918fa93b634acc60d4bb084c","modified":1658991492494},{"_id":"public/2022/01/21/photopainting3/index.html","hash":"598cd77d71b6f96ae7d7e9c2c1070b664d68ab06","modified":1658991492494},{"_id":"public/2022/01/21/photopainting2/index.html","hash":"3f14887fef2ccbf4109cae67e6afe628a1e7c8bf","modified":1658991492494},{"_id":"public/2022/01/21/photopainting/index.html","hash":"b6c0793ae8a8e6ae39e3b711b3d934f6e73be24c","modified":1663852835195},{"_id":"public/2022/01/19/Sampling Error/index.html","hash":"e541b8255b265507abd0b1a67b883132a3be3830","modified":1658991492494},{"_id":"public/2021/11/28/Linear and nolinear/index.html","hash":"616860dfa6ff441614689958abfea683f3b7549c","modified":1658991492494},{"_id":"public/2021/08/12/how_to_predict_a_coin_flip/index.html","hash":"1a5008f0b6c91b67351d5f4098389e98686f44c6","modified":1658991492494},{"_id":"public/2021/06/28/20200528_ai_custom_tensorflow/index.html","hash":"18ca1af3d345b267b57426d413997b09fd0562cc","modified":1658991492494},{"_id":"public/2020/11/21/2020-11-21-AI-Numba_GPU/index.html","hash":"67218b35c5b9d0c8acd325db9483af97fddb3985","modified":1658991492494},{"_id":"public/2020/11/05/function/index.html","hash":"8e8596d13937ad922234503fba8cef8e4d20d432","modified":1658991492494},{"_id":"public/2020/10/26/Relation/index.html","hash":"052a012031d726093c55ffe5d5e908a62db29c3b","modified":1658991492494},{"_id":"public/2020/10/20/2020-10-20-AI_RL/index.html","hash":"0430723f449e19d69fb6f8b236f5df97607f9b9c","modified":1658991492494},{"_id":"public/2020/09/10/Anaconda_install_Tensorflow/index.html","hash":"f4bbb9ce8d50eb4a413755ec2955fc7693d068a6","modified":1658991492494},{"_id":"public/2020/08/11/2020-02-26-AI_NB_SVM_DT/index.html","hash":"a07ccec3ba33617173f991807f605e6e7875948a","modified":1658991492494},{"_id":"public/2020/08/11/2020-02-26-拉格朗日乘子法/index.html","hash":"21c4f2969573595c88777a7cf8264b95f2c41567","modified":1658991492494},{"_id":"public/2020/06/05/PCA_CCA_PLS/index.html","hash":"3358577decd34092971f501e46c7816fab5668bc","modified":1658991492494},{"_id":"public/2020/05/28/python_environment/index.html","hash":"610db286ec363da9a779411d5b6b9123850110e7","modified":1658991492494},{"_id":"public/2020/03/23/环境保护主义/index.html","hash":"8b3ee780c20b36836ea0c1b1f6829fff08bfa96b","modified":1658991492494},{"_id":"public/2020/02/26/AI_Neural_Network/index.html","hash":"f215534a48a649bc28b12fc9aa017bce10dfe551","modified":1658991492494},{"_id":"public/2020/02/25/命题逻辑/index.html","hash":"e79bfbdad900ede846da3ea1b57c797137e7dd6d","modified":1658991492494},{"_id":"public/2020/02/20/2020-02-20-数理逻辑/index.html","hash":"d0e851685238ba6584eec7267ca97f12d3f6b858","modified":1658991492494},{"_id":"public/2020/02/12/2020-02-12-Python-基础（二）/index.html","hash":"9c0ead197f347bea4d12a6b4c17936492eb73402","modified":1658991492494},{"_id":"public/2020/02/12/Python 基础（一）/index.html","hash":"98527f232261eba5f1eac79763d152d05806f6d3","modified":1658991492494},{"_id":"public/2020/02/04/计算的极限/index.html","hash":"90409f89299e94475fa262aa296da976e20c58a2","modified":1658991492494},{"_id":"public/2020/02/02/集合论/index.html","hash":"b31e43c96095ae93b8c212b2bb18937c58c7c265","modified":1658991492494},{"_id":"public/2020/01/28/boundary&infinity/index.html","hash":"4825bbeb1e18df81b2822c10761f0a0e130c1bf5","modified":1658991492494},{"_id":"public/2019/11/02/aslongas fast enough/index.html","hash":"bc506f6a3489ecc5623f8fed01aee11799a116b8","modified":1658991492494},{"_id":"public/2019/06/19/reading gaigelicheng/index.html","hash":"2ba4ce31a21c875df3ca0f7dd9d520d1a720982e","modified":1658991492494},{"_id":"public/2019/06/11/Marcel Duchamp's Urinal/index.html","hash":"b0543e0f7363d4b825406316283fd135db776c3f","modified":1658991492494},{"_id":"public/2019/01/21/reading MaxWebber/index.html","hash":"477a050d3c23b2af993cf60b0e11d6d574947067","modified":1658991492494},{"_id":"public/2018/07/08/reading Whatismath/index.html","hash":"73e23aa5b5210f8081bd1e21a27e3905135c1c48","modified":1658991492494},{"_id":"public/2018/06/27/LuXun/index.html","hash":"ad50bf22a2911527ee89924558b0d0d0801467f5","modified":1658991492494},{"_id":"public/archives/index.html","hash":"3f413c375bab8bb5d80193af6dcce4c02aa04046","modified":1658991492494},{"_id":"public/archives/page/2/index.html","hash":"2823ab513b2650738c44fd504461268b209ba852","modified":1658991492494},{"_id":"public/archives/page/3/index.html","hash":"bbf0405d0f89d88b2ffdc25618b03ff605dd0bb7","modified":1658991492494},{"_id":"public/archives/2020/index.html","hash":"4403221364eea5388c95896b925a64bd906a2eef","modified":1658991492494},{"_id":"public/categories/计算机科学/index.html","hash":"50ff242e30bd631821cceaf4e4243a5f259f003f","modified":1658991492494},{"_id":"public/index.html","hash":"8e6ee34c320e093349a517b8a83a6ac10b95e067","modified":1663852835195},{"_id":"public/page/2/index.html","hash":"c3c0d8703c69264ac49ab4ebd78a6abec86c90a1","modified":1658991492494},{"_id":"public/page/3/index.html","hash":"9e49f760923d55348e1a4c3fc2c03891b6f8b34e","modified":1658991492494},{"_id":"public/tags/数学/index.html","hash":"4d98aee26966fd3226aa805db651d7c1e259e8ea","modified":1658991492494},{"_id":"public/tags/python/index.html","hash":"11be6e6944df051313c2316ef3dd09cd495bb890","modified":1658991492494},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1642786142807},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1642786142807},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1642786142807},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1642786142807},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1642786142807},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1642786142807},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1642786142807},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1642786142807},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1642786142807},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1642786142807},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1642786142807},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1642786142807},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1642786142807},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1642786142807},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1642786142807},{"_id":"public/images/flowchart.png","hash":"4d4dc3c5bf6c5a9e8025c3e73f5c3b9fd56b751c","modified":1642786142807},{"_id":"public/images/DrawingHands.jpg","hash":"d3339915e14234095b614bd8103e04badf39b2b8","modified":1642786142807},{"_id":"public/lib/bookmark/LICENSE","hash":"1440355648a72be5b7625fe6fa055b462fcc78b0","modified":1642786142807},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1642786142807},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1642786142807},{"_id":"public/images/71580375662.jpg","hash":"9e1d6e5c8111ff3f5a9172d712096e223b57d24d","modified":1642786142807},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1642786142807},{"_id":"public/js/algolia-search.js","hash":"813afcc30feee11d59f297f2d5a96f98fbbd4743","modified":1642786142807},{"_id":"public/js/local-search.js","hash":"f2e568298c71d2417a1a1c7e56025ce5842b1220","modified":1642786142807},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1642786142807},{"_id":"public/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1642786142807},{"_id":"public/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1642786142807},{"_id":"public/js/utils.js","hash":"a1e70ac9d32697907d5d78f2533d627f4df8ebcb","modified":1642786142807},{"_id":"public/lib/bookmark/index.js","hash":"5e5cba645a1a4531ccbb4782df2f7a075626393f","modified":1642786142807},{"_id":"public/lib/bookmark/package.json","hash":"9f06f3432c12b68a2c2fe3f318455b35a965a1da","modified":1642786142807},{"_id":"public/lib/bookmark/bookmark.min.js","hash":"9e525329553335c2484f6faf9e933a6bbee9ab6d","modified":1642786142807},{"_id":"public/lib/bookmark/renovate.json","hash":"94990e0ad04ce4a7c6f0ac3543318d9e02db1264","modified":1642786142807},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1642786142807},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1642786142807},{"_id":"public/js/schemes/muse.js","hash":"ae2a2502b77203e83f75a040c43e86a7ecb4873c","modified":1642786142807},{"_id":"public/js/schemes/pisces.js","hash":"f068b46f8c305c7436c2767492a6bed42dcd764c","modified":1642786142807},{"_id":"public/lib/bookmark/README.html","hash":"6d5b0a9061413e3f98b3a954f02d322cd57e97f0","modified":1642786142807},{"_id":"public/css/main.css","hash":"378b68001410d001e172b3e7ae575b5d12f98022","modified":1642786142807},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1642786142807},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1642786142807},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1642786142807},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1642786142807},{"_id":"public/images/120625074322660dd39deed2d7.gif","hash":"3e37402766bee0f297048a9a825e584f287b1ee8","modified":1642786142807},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1642786142807},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1642786142807},{"_id":"source/_posts/life_dream_x.md","hash":"4bc2139633c438d0f025fe5df7f6f4f5a5c8dc35","modified":1650595726404},{"_id":"public/2022/02/08/life_dream_x/index.html","hash":"f1be08d29e6b3f000607c6caad0055bf555696dc","modified":1658991492494},{"_id":"public/archives/2022/02/index.html","hash":"7776d2020a95588c67b3e86d3159bb67680dcedb","modified":1658991492494},{"_id":"source/_posts/multi_table_n_calclator.md","hash":"1d0a2fea9a4a4546f65402a780506636b790bcac","modified":1647544293948},{"_id":"public/2022/03/02/multi_table_n_calclator/index.html","hash":"1e422e84b0b265aab649c426363633cee8c7c848","modified":1658991492494},{"_id":"public/archives/2022/03/index.html","hash":"4a90b37365985b95b8dc29e1bc4841387d6dc8b6","modified":1658991492494},{"_id":"source/_posts/life_dream_x copy.md","hash":"822c066943d0ddd129bb22e9b628e335cdc2e6d7","modified":1650595378936},{"_id":"public/2022/03/29/life_dream_x copy/index.html","hash":"324462b9c31a3ac63b4e6b6cd0efb5e571ad7913","modified":1650595422857},{"_id":"source/_posts/touch.md","hash":"18ed2c8ffd6a9db4a8b566ff55ea3228af06984f","modified":1657183873057},{"_id":"public/2022/03/29/touch/index.html","hash":"fd638a329d3feb7e759bf563e427c922010df494","modified":1658991492494},{"_id":"source/_posts/LearningBible.md","hash":"094f0bebe9096e14589d359c87462df621f22113","modified":1658991446844},{"_id":"public/2022/07/28/LearningBible/index.html","hash":"4437ef8fb7fffc9c98487d0b91b3632a712f9b61","modified":1658991492494},{"_id":"public/archives/2022/07/index.html","hash":"ec1839ba7abdfac9977644a98f3145e840f475ca","modified":1658991492494},{"_id":"public/categories/杂想/page/2/index.html","hash":"7e9bb46d4f2045b15219a0f095172a47d00de888","modified":1658991492494}],"Category":[{"name":"数理逻辑","_id":"ckyoogd5800042sfyckmy6ka0"},{"name":"计算机科学","_id":"ckyoogd5g000a2sfy8l48hd7k"},{"name":"最优化","_id":"ckyoogd5y000r2sfyhlva7uf8"},{"name":"杂想","_id":"ckyoogd6400102sfydlvubysr"},{"name":"统计","_id":"ckyoogd6h001l2sfyfx121cfc"},{"name":"相片与画","_id":"ckyoogd6t00262sfy85pm8aqg"},{"name":"社会科学","_id":"ckyoogd75002v2sfy8r9kd4fn"}],"Data":[],"Page":[{"title":"分类","date":"2020-01-30T08:39:29.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2020-01-30 16:39:29\ntype: \"categories\"\ncomments: false\n---\n","updated":"2020-01-30T08:45:27.449Z","path":"categories/index.html","layout":"page","_id":"ckyoogd5300012sfyb4sx7khp","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"关于","date":"2020-01-30T15:36:50.000Z","_content":"\n情感和渴望是人类一切努力和创造的动力\n\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2020-01-30 23:36:50\n---\n\n情感和渴望是人类一切努力和创造的动力\n\n","updated":"2020-05-31T15:32:03.349Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckyoogd5700032sfy22n9cvnj","content":"<p>情感和渴望是人类一切努力和创造的动力</p>\n","site":{"data":{}},"excerpt":"","more":"<p>情感和渴望是人类一切努力和创造的动力</p>\n"},{"title":"标签","date":"2020-01-30T08:48:28.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2020-01-30 16:48:28\ntype: \"tags\"\ncomments: false\n---\n","updated":"2020-01-30T09:54:02.842Z","path":"tags/index.html","layout":"page","_id":"ckyoogd5d00072sfy73sgaoay","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"数理逻辑--初窥门径","date":"2020-02-19T16:00:00.000Z","mathjax":true,"_content":"\n公元前400多年的古希腊，德尔斐的阿波罗神庙上刻着一句著名的神谕：“认识你自己”。人降生在这个世界上时一无所知，苏格拉底说：追求知识和真理就是最高的品德。<!--more-->\n\n### 数理逻辑简史\n\n亚里士多德（394BC-322BC）继承苏格拉底和柏拉图的意志，并且更彻底地追求真理，他说：“吾爱吾师，吾更爱真理。”为了他的目标，亚里士多德创立了一套方法论，成为逻辑的起源。其中包括著名的“三段论”——全称、特称、结论，以及逻辑推理的三大规律——同一律、无矛盾律和排中律。\n\n欧几里得（325BC-265BC）的《几何原本》将公理和逻辑引入到数学中。以几条不证自明的简单公理为基础，用逻辑演绎的方法构建起欧氏几何的大厦。\n\n由于自然语言存在歧义，使用自然语言进行逻辑推理容易走入诡辩的迷雾（拓展：公孙龙的“白马非马”论，哥德尔用本体论证明上帝存在）。为了摆脱含混不清，令逻辑达到绝对的严谨和明晰，将**语义**与**语法**分离，推动逻辑推理形式化、符号化成了逻辑发展走向现代的风向标。\n\n千年之后，德国的莱布尼茨（1646-1716）第一个试图把逻辑推理符号化，以求逻辑推理变得直观简洁，为更复杂的逻辑推理开辟道路。\n\n1832年，伽罗华开创了研究抽象公理代数系统的抽象代数。\n\n布尔（1815-1864）把代数引入逻辑，用0，1分别代表False，True，结合算子和逻辑规则，使逻辑运算成为可能。\n\n弗雷格（1848-1925）试图把数学转变成纯粹的符号逻辑，他留下了一本著作《概念文字》，引入的符号语言后来被称为一阶语言。\n\n1872-1874年，康托尔建立的集合论似乎告诉人们：数学系统的形式化工作已经完成，数学的绝对严密已经实现。所以有了1900年希尔伯特在巴黎数学大会上的雄心勃勃的宣言。\n\n但仅仅过了1年，罗素便发现了朴素集合论中著名的“罗素悖论”：假设S是所有不属于自身的集合的集合，即$S=\\{x|x\\not \\in x\\} $，那么S是否属于自身就会导出矛盾。数学大厦遭遇了危机。后来数学家们用新的公理替代Cantor原则，形成了公理化集合论。\n\n……\n\n<br/>\n\n---\n\n### 什么是命题，什么是论证\n\n首先看一组自然语言：\n\n>1. “明天会下雨”\n>\n>2. “Tomorrow will rain”\n>\n>3. “$2+3=5$”\n>\n>4. “$2^{=3}+5$”\n>\n>5. “别吃冰淇淋”\n>\n>6. “吃什么口味的？”\n\n1和2是不同的语句，但对于一个同时掌握中文和英文的人来说，会认为1和2想要表达某种相同的东西，我们暂时称这种语句背后的“含义”为“命题”。那么，1的含义是什么呢？假如有人在2020年1月1日说了“明天会下雨”，这句话的含义是2020年1月2日会下雨；但假如此人是在2020年1月2日说了这句话，那么这句话的含义又变成了2020年1月3日会下雨。所以，同一个语句，也有可能表达多个含义。自然语言中充满了各种各样有歧义的语句。对于逻辑学家，想要追求确定性和唯一性，就不能容许含糊的歧义存在，把语义从语句中抽离出来，就是必要的工作。\n\n语句被分为**语法**和**语义**。简单来说，语法中包含一些符号，这些符号按特定的规则排列组合，使用这种语言的人遵循这些规则，就可以表达某种语义。以3和4为例，学过算术的人一眼就可以看懂3表达的含义，而4仅仅是把同样的一些符号换了换顺序，就让人看不懂了。\n\n现在知道了是什么构成一个**语句**。开头说过，暂时把语句的“含义”叫做“命题”，那是不是所有有含义的语句都算作一个命题呢？来看看5和6，它们显然也都能传递一些信息，但对于一个具有古希腊演绎精神的哲学家用处不大。一个皇帝确实可以依仗手中的权力号令手下为他杀掉一个叛臣，但就算他破口大骂，也不能让一个没电的灯泡发出光来。同样，一个道士向上天询问日月星辰运行之理，答案也很难凭着打坐降临这颗虔诚的头脑。只有**真伪**才有意义，**命题**就是这样一些语句，它们非真即伪，必然可以判定（*二值原则*）。\n\n判定命题真伪的过程就叫做**论证**。一般来说，一个论证由**前提**和**结论**组成，并且前提和结论都必须是命题。看下面两个例子：\n\n>1. 苏格拉底是人，人都会死，所以苏格拉底也会死。\n>2. 苏格拉底会死，人都会死，所以苏格拉底是人。\n\n人们会接受1的说法，但却不大会接受2的说法，即使2的前提是真的。当人们接受前提的情况下，一定也会接受结论，才可以称一个论证为正确的论证。如果我们换掉1中的“苏格拉底”，“人”，“会死”，在对应的位置换成其它项：\n\n> 黑天鹅是天鹅，天鹅有两只翅膀，所以黑天鹅有两只翅膀。\n\n只要我们承认黑天鹅是天鹅，天鹅有两只翅膀，那么就会认同结论是正确的。事实上，“苏格拉底”，“人”，“会死”可以换成任何东西，完全可以用符号代替，这样就可以抽象出论证的形式：\n\n> P 是 Q，所有的 Q 都是 R，所以 P 是 R。\n\n按照一个**有效论证**的形式，只要前提为真，其结论就一定为真。这称为逻辑论证的**可靠性（soundness）**。\n\n<br/>\n\n---\n\n### 逻辑系统\n\n苏格拉底的例子是亚里士多德提出的经典“三段论”结构，其基本单位是**项（terms）**，被称为**语词逻辑（term logic）**。\n\n还有一种常见的逻辑结构，以**命题（proposition）**为基本单位：\n\n> 如果苹果熟了，那么苹果就会落地。\n>\n> 苹果熟了。\n>\n> 苹果会落地。\n\n这种论证形式可以表示为：\n\n>若 P，则 Q\n>\n>P\n>\n>所以 Q\n\n这种逻辑系统被称为**命题逻辑（propositional logic）**，命题逻辑是一种很简单也很基本的逻辑系统，处理能力有限。\n\n更复杂的逻辑系统有**述词逻辑（predicate logic）**，把命题拆分为主词（subject）和述词（predicate）；**一阶逻辑（first order logic）**（项、量词、加入关系述词等）等等。\n\n……\n\n以上的逻辑理论都基于一个基本的假设：命题必然非真即伪，这在语义学中被称为“二值原则”，而在亚里士多德提出的逻辑三大规律中被称为“排中律”。如果不满足这个基本假设，事实上，自然语言中这样的语句并不少见，比如：\n\n> 明天气温会超过40摄氏度。\n>\n> 小光工作勤勤恳恳。\n\n第一句话如果把“明天”看作永远未到来的“明天”，那这句话永远也无法判定真伪，只对可能性做了一个声明；\n\n第二句话加入小光一辈子都没有找到一份工作，这句话也无法判定真伪。\n\n处理这样的语句有另外的逻辑系统，比如**模态逻辑**，**直觉逻辑**。\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[台大开放式课程 傅皓政《逻辑》](http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2)*</small>","source":"_posts/2020-02-20-数理逻辑.md","raw":"---\ntitle: 数理逻辑--初窥门径\ndate: 2020-02-20 00:00:00\ncategories:\n- 数理逻辑\ntags: \n- 逻辑学\n- 数学\n- 哲学\nmathjax: true\n\n---\n\n公元前400多年的古希腊，德尔斐的阿波罗神庙上刻着一句著名的神谕：“认识你自己”。人降生在这个世界上时一无所知，苏格拉底说：追求知识和真理就是最高的品德。<!--more-->\n\n### 数理逻辑简史\n\n亚里士多德（394BC-322BC）继承苏格拉底和柏拉图的意志，并且更彻底地追求真理，他说：“吾爱吾师，吾更爱真理。”为了他的目标，亚里士多德创立了一套方法论，成为逻辑的起源。其中包括著名的“三段论”——全称、特称、结论，以及逻辑推理的三大规律——同一律、无矛盾律和排中律。\n\n欧几里得（325BC-265BC）的《几何原本》将公理和逻辑引入到数学中。以几条不证自明的简单公理为基础，用逻辑演绎的方法构建起欧氏几何的大厦。\n\n由于自然语言存在歧义，使用自然语言进行逻辑推理容易走入诡辩的迷雾（拓展：公孙龙的“白马非马”论，哥德尔用本体论证明上帝存在）。为了摆脱含混不清，令逻辑达到绝对的严谨和明晰，将**语义**与**语法**分离，推动逻辑推理形式化、符号化成了逻辑发展走向现代的风向标。\n\n千年之后，德国的莱布尼茨（1646-1716）第一个试图把逻辑推理符号化，以求逻辑推理变得直观简洁，为更复杂的逻辑推理开辟道路。\n\n1832年，伽罗华开创了研究抽象公理代数系统的抽象代数。\n\n布尔（1815-1864）把代数引入逻辑，用0，1分别代表False，True，结合算子和逻辑规则，使逻辑运算成为可能。\n\n弗雷格（1848-1925）试图把数学转变成纯粹的符号逻辑，他留下了一本著作《概念文字》，引入的符号语言后来被称为一阶语言。\n\n1872-1874年，康托尔建立的集合论似乎告诉人们：数学系统的形式化工作已经完成，数学的绝对严密已经实现。所以有了1900年希尔伯特在巴黎数学大会上的雄心勃勃的宣言。\n\n但仅仅过了1年，罗素便发现了朴素集合论中著名的“罗素悖论”：假设S是所有不属于自身的集合的集合，即$S=\\{x|x\\not \\in x\\} $，那么S是否属于自身就会导出矛盾。数学大厦遭遇了危机。后来数学家们用新的公理替代Cantor原则，形成了公理化集合论。\n\n……\n\n<br/>\n\n---\n\n### 什么是命题，什么是论证\n\n首先看一组自然语言：\n\n>1. “明天会下雨”\n>\n>2. “Tomorrow will rain”\n>\n>3. “$2+3=5$”\n>\n>4. “$2^{=3}+5$”\n>\n>5. “别吃冰淇淋”\n>\n>6. “吃什么口味的？”\n\n1和2是不同的语句，但对于一个同时掌握中文和英文的人来说，会认为1和2想要表达某种相同的东西，我们暂时称这种语句背后的“含义”为“命题”。那么，1的含义是什么呢？假如有人在2020年1月1日说了“明天会下雨”，这句话的含义是2020年1月2日会下雨；但假如此人是在2020年1月2日说了这句话，那么这句话的含义又变成了2020年1月3日会下雨。所以，同一个语句，也有可能表达多个含义。自然语言中充满了各种各样有歧义的语句。对于逻辑学家，想要追求确定性和唯一性，就不能容许含糊的歧义存在，把语义从语句中抽离出来，就是必要的工作。\n\n语句被分为**语法**和**语义**。简单来说，语法中包含一些符号，这些符号按特定的规则排列组合，使用这种语言的人遵循这些规则，就可以表达某种语义。以3和4为例，学过算术的人一眼就可以看懂3表达的含义，而4仅仅是把同样的一些符号换了换顺序，就让人看不懂了。\n\n现在知道了是什么构成一个**语句**。开头说过，暂时把语句的“含义”叫做“命题”，那是不是所有有含义的语句都算作一个命题呢？来看看5和6，它们显然也都能传递一些信息，但对于一个具有古希腊演绎精神的哲学家用处不大。一个皇帝确实可以依仗手中的权力号令手下为他杀掉一个叛臣，但就算他破口大骂，也不能让一个没电的灯泡发出光来。同样，一个道士向上天询问日月星辰运行之理，答案也很难凭着打坐降临这颗虔诚的头脑。只有**真伪**才有意义，**命题**就是这样一些语句，它们非真即伪，必然可以判定（*二值原则*）。\n\n判定命题真伪的过程就叫做**论证**。一般来说，一个论证由**前提**和**结论**组成，并且前提和结论都必须是命题。看下面两个例子：\n\n>1. 苏格拉底是人，人都会死，所以苏格拉底也会死。\n>2. 苏格拉底会死，人都会死，所以苏格拉底是人。\n\n人们会接受1的说法，但却不大会接受2的说法，即使2的前提是真的。当人们接受前提的情况下，一定也会接受结论，才可以称一个论证为正确的论证。如果我们换掉1中的“苏格拉底”，“人”，“会死”，在对应的位置换成其它项：\n\n> 黑天鹅是天鹅，天鹅有两只翅膀，所以黑天鹅有两只翅膀。\n\n只要我们承认黑天鹅是天鹅，天鹅有两只翅膀，那么就会认同结论是正确的。事实上，“苏格拉底”，“人”，“会死”可以换成任何东西，完全可以用符号代替，这样就可以抽象出论证的形式：\n\n> P 是 Q，所有的 Q 都是 R，所以 P 是 R。\n\n按照一个**有效论证**的形式，只要前提为真，其结论就一定为真。这称为逻辑论证的**可靠性（soundness）**。\n\n<br/>\n\n---\n\n### 逻辑系统\n\n苏格拉底的例子是亚里士多德提出的经典“三段论”结构，其基本单位是**项（terms）**，被称为**语词逻辑（term logic）**。\n\n还有一种常见的逻辑结构，以**命题（proposition）**为基本单位：\n\n> 如果苹果熟了，那么苹果就会落地。\n>\n> 苹果熟了。\n>\n> 苹果会落地。\n\n这种论证形式可以表示为：\n\n>若 P，则 Q\n>\n>P\n>\n>所以 Q\n\n这种逻辑系统被称为**命题逻辑（propositional logic）**，命题逻辑是一种很简单也很基本的逻辑系统，处理能力有限。\n\n更复杂的逻辑系统有**述词逻辑（predicate logic）**，把命题拆分为主词（subject）和述词（predicate）；**一阶逻辑（first order logic）**（项、量词、加入关系述词等）等等。\n\n……\n\n以上的逻辑理论都基于一个基本的假设：命题必然非真即伪，这在语义学中被称为“二值原则”，而在亚里士多德提出的逻辑三大规律中被称为“排中律”。如果不满足这个基本假设，事实上，自然语言中这样的语句并不少见，比如：\n\n> 明天气温会超过40摄氏度。\n>\n> 小光工作勤勤恳恳。\n\n第一句话如果把“明天”看作永远未到来的“明天”，那这句话永远也无法判定真伪，只对可能性做了一个声明；\n\n第二句话加入小光一辈子都没有找到一份工作，这句话也无法判定真伪。\n\n处理这样的语句有另外的逻辑系统，比如**模态逻辑**，**直觉逻辑**。\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[台大开放式课程 傅皓政《逻辑》](http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2)*</small>","slug":"2020-02-20-数理逻辑","published":1,"updated":"2020-03-01T10:04:05.221Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd4k00002sfy8szt1ked","content":"<p>公元前400多年的古希腊，德尔斐的阿波罗神庙上刻着一句著名的神谕：“认识你自己”。人降生在这个世界上时一无所知，苏格拉底说：追求知识和真理就是最高的品德。<a id=\"more\"></a></p>\n<h3 id=\"数理逻辑简史\"><a href=\"#数理逻辑简史\" class=\"headerlink\" title=\"数理逻辑简史\"></a>数理逻辑简史</h3><p>亚里士多德（394BC-322BC）继承苏格拉底和柏拉图的意志，并且更彻底地追求真理，他说：“吾爱吾师，吾更爱真理。”为了他的目标，亚里士多德创立了一套方法论，成为逻辑的起源。其中包括著名的“三段论”——全称、特称、结论，以及逻辑推理的三大规律——同一律、无矛盾律和排中律。</p>\n<p>欧几里得（325BC-265BC）的《几何原本》将公理和逻辑引入到数学中。以几条不证自明的简单公理为基础，用逻辑演绎的方法构建起欧氏几何的大厦。</p>\n<p>由于自然语言存在歧义，使用自然语言进行逻辑推理容易走入诡辩的迷雾（拓展：公孙龙的“白马非马”论，哥德尔用本体论证明上帝存在）。为了摆脱含混不清，令逻辑达到绝对的严谨和明晰，将<strong>语义</strong>与<strong>语法</strong>分离，推动逻辑推理形式化、符号化成了逻辑发展走向现代的风向标。</p>\n<p>千年之后，德国的莱布尼茨（1646-1716）第一个试图把逻辑推理符号化，以求逻辑推理变得直观简洁，为更复杂的逻辑推理开辟道路。</p>\n<p>1832年，伽罗华开创了研究抽象公理代数系统的抽象代数。</p>\n<p>布尔（1815-1864）把代数引入逻辑，用0，1分别代表False，True，结合算子和逻辑规则，使逻辑运算成为可能。</p>\n<p>弗雷格（1848-1925）试图把数学转变成纯粹的符号逻辑，他留下了一本著作《概念文字》，引入的符号语言后来被称为一阶语言。</p>\n<p>1872-1874年，康托尔建立的集合论似乎告诉人们：数学系统的形式化工作已经完成，数学的绝对严密已经实现。所以有了1900年希尔伯特在巴黎数学大会上的雄心勃勃的宣言。</p>\n<p>但仅仅过了1年，罗素便发现了朴素集合论中著名的“罗素悖论”：假设S是所有不属于自身的集合的集合，即$S=\\{x|x\\not \\in x\\} $，那么S是否属于自身就会导出矛盾。数学大厦遭遇了危机。后来数学家们用新的公理替代Cantor原则，形成了公理化集合论。</p>\n<p>……</p>\n<p><br/></p>\n<hr>\n<h3 id=\"什么是命题，什么是论证\"><a href=\"#什么是命题，什么是论证\" class=\"headerlink\" title=\"什么是命题，什么是论证\"></a>什么是命题，什么是论证</h3><p>首先看一组自然语言：</p>\n<blockquote>\n<ol>\n<li><p>“明天会下雨”</p>\n</li>\n<li><p>“Tomorrow will rain”</p>\n</li>\n<li><p>“$2+3=5$”</p>\n</li>\n<li><p>“$2^{=3}+5$”</p>\n</li>\n<li><p>“别吃冰淇淋”</p>\n</li>\n<li><p>“吃什么口味的？”</p>\n</li>\n</ol>\n</blockquote>\n<p>1和2是不同的语句，但对于一个同时掌握中文和英文的人来说，会认为1和2想要表达某种相同的东西，我们暂时称这种语句背后的“含义”为“命题”。那么，1的含义是什么呢？假如有人在2020年1月1日说了“明天会下雨”，这句话的含义是2020年1月2日会下雨；但假如此人是在2020年1月2日说了这句话，那么这句话的含义又变成了2020年1月3日会下雨。所以，同一个语句，也有可能表达多个含义。自然语言中充满了各种各样有歧义的语句。对于逻辑学家，想要追求确定性和唯一性，就不能容许含糊的歧义存在，把语义从语句中抽离出来，就是必要的工作。</p>\n<p>语句被分为<strong>语法</strong>和<strong>语义</strong>。简单来说，语法中包含一些符号，这些符号按特定的规则排列组合，使用这种语言的人遵循这些规则，就可以表达某种语义。以3和4为例，学过算术的人一眼就可以看懂3表达的含义，而4仅仅是把同样的一些符号换了换顺序，就让人看不懂了。</p>\n<p>现在知道了是什么构成一个<strong>语句</strong>。开头说过，暂时把语句的“含义”叫做“命题”，那是不是所有有含义的语句都算作一个命题呢？来看看5和6，它们显然也都能传递一些信息，但对于一个具有古希腊演绎精神的哲学家用处不大。一个皇帝确实可以依仗手中的权力号令手下为他杀掉一个叛臣，但就算他破口大骂，也不能让一个没电的灯泡发出光来。同样，一个道士向上天询问日月星辰运行之理，答案也很难凭着打坐降临这颗虔诚的头脑。只有<strong>真伪</strong>才有意义，<strong>命题</strong>就是这样一些语句，它们非真即伪，必然可以判定（<em>二值原则</em>）。</p>\n<p>判定命题真伪的过程就叫做<strong>论证</strong>。一般来说，一个论证由<strong>前提</strong>和<strong>结论</strong>组成，并且前提和结论都必须是命题。看下面两个例子：</p>\n<blockquote>\n<ol>\n<li>苏格拉底是人，人都会死，所以苏格拉底也会死。</li>\n<li>苏格拉底会死，人都会死，所以苏格拉底是人。</li>\n</ol>\n</blockquote>\n<p>人们会接受1的说法，但却不大会接受2的说法，即使2的前提是真的。当人们接受前提的情况下，一定也会接受结论，才可以称一个论证为正确的论证。如果我们换掉1中的“苏格拉底”，“人”，“会死”，在对应的位置换成其它项：</p>\n<blockquote>\n<p>黑天鹅是天鹅，天鹅有两只翅膀，所以黑天鹅有两只翅膀。</p>\n</blockquote>\n<p>只要我们承认黑天鹅是天鹅，天鹅有两只翅膀，那么就会认同结论是正确的。事实上，“苏格拉底”，“人”，“会死”可以换成任何东西，完全可以用符号代替，这样就可以抽象出论证的形式：</p>\n<blockquote>\n<p>P 是 Q，所有的 Q 都是 R，所以 P 是 R。</p>\n</blockquote>\n<p>按照一个<strong>有效论证</strong>的形式，只要前提为真，其结论就一定为真。这称为逻辑论证的<strong>可靠性（soundness）</strong>。</p>\n<p><br/></p>\n<hr>\n<h3 id=\"逻辑系统\"><a href=\"#逻辑系统\" class=\"headerlink\" title=\"逻辑系统\"></a>逻辑系统</h3><p>苏格拉底的例子是亚里士多德提出的经典“三段论”结构，其基本单位是<strong>项（terms）</strong>，被称为<strong>语词逻辑（term logic）</strong>。</p>\n<p>还有一种常见的逻辑结构，以<strong>命题（proposition）</strong>为基本单位：</p>\n<blockquote>\n<p>如果苹果熟了，那么苹果就会落地。</p>\n<p>苹果熟了。</p>\n<p>苹果会落地。</p>\n</blockquote>\n<p>这种论证形式可以表示为：</p>\n<blockquote>\n<p>若 P，则 Q</p>\n<p>P</p>\n<p>所以 Q</p>\n</blockquote>\n<p>这种逻辑系统被称为<strong>命题逻辑（propositional logic）</strong>，命题逻辑是一种很简单也很基本的逻辑系统，处理能力有限。</p>\n<p>更复杂的逻辑系统有<strong>述词逻辑（predicate logic）</strong>，把命题拆分为主词（subject）和述词（predicate）；<strong>一阶逻辑（first order logic）</strong>（项、量词、加入关系述词等）等等。</p>\n<p>……</p>\n<p>以上的逻辑理论都基于一个基本的假设：命题必然非真即伪，这在语义学中被称为“二值原则”，而在亚里士多德提出的逻辑三大规律中被称为“排中律”。如果不满足这个基本假设，事实上，自然语言中这样的语句并不少见，比如：</p>\n<blockquote>\n<p>明天气温会超过40摄氏度。</p>\n<p>小光工作勤勤恳恳。</p>\n</blockquote>\n<p>第一句话如果把“明天”看作永远未到来的“明天”，那这句话永远也无法判定真伪，只对可能性做了一个声明；</p>\n<p>第二句话加入小光一辈子都没有找到一份工作，这句话也无法判定真伪。</p>\n<p>处理这样的语句有另外的逻辑系统，比如<strong>模态逻辑</strong>，<strong>直觉逻辑</strong>。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2\" target=\"_blank\" rel=\"noopener\">台大开放式课程 傅皓政《逻辑》</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>公元前400多年的古希腊，德尔斐的阿波罗神庙上刻着一句著名的神谕：“认识你自己”。人降生在这个世界上时一无所知，苏格拉底说：追求知识和真理就是最高的品德。","more":"</p>\n<h3 id=\"数理逻辑简史\"><a href=\"#数理逻辑简史\" class=\"headerlink\" title=\"数理逻辑简史\"></a>数理逻辑简史</h3><p>亚里士多德（394BC-322BC）继承苏格拉底和柏拉图的意志，并且更彻底地追求真理，他说：“吾爱吾师，吾更爱真理。”为了他的目标，亚里士多德创立了一套方法论，成为逻辑的起源。其中包括著名的“三段论”——全称、特称、结论，以及逻辑推理的三大规律——同一律、无矛盾律和排中律。</p>\n<p>欧几里得（325BC-265BC）的《几何原本》将公理和逻辑引入到数学中。以几条不证自明的简单公理为基础，用逻辑演绎的方法构建起欧氏几何的大厦。</p>\n<p>由于自然语言存在歧义，使用自然语言进行逻辑推理容易走入诡辩的迷雾（拓展：公孙龙的“白马非马”论，哥德尔用本体论证明上帝存在）。为了摆脱含混不清，令逻辑达到绝对的严谨和明晰，将<strong>语义</strong>与<strong>语法</strong>分离，推动逻辑推理形式化、符号化成了逻辑发展走向现代的风向标。</p>\n<p>千年之后，德国的莱布尼茨（1646-1716）第一个试图把逻辑推理符号化，以求逻辑推理变得直观简洁，为更复杂的逻辑推理开辟道路。</p>\n<p>1832年，伽罗华开创了研究抽象公理代数系统的抽象代数。</p>\n<p>布尔（1815-1864）把代数引入逻辑，用0，1分别代表False，True，结合算子和逻辑规则，使逻辑运算成为可能。</p>\n<p>弗雷格（1848-1925）试图把数学转变成纯粹的符号逻辑，他留下了一本著作《概念文字》，引入的符号语言后来被称为一阶语言。</p>\n<p>1872-1874年，康托尔建立的集合论似乎告诉人们：数学系统的形式化工作已经完成，数学的绝对严密已经实现。所以有了1900年希尔伯特在巴黎数学大会上的雄心勃勃的宣言。</p>\n<p>但仅仅过了1年，罗素便发现了朴素集合论中著名的“罗素悖论”：假设S是所有不属于自身的集合的集合，即$S=\\{x|x\\not \\in x\\} $，那么S是否属于自身就会导出矛盾。数学大厦遭遇了危机。后来数学家们用新的公理替代Cantor原则，形成了公理化集合论。</p>\n<p>……</p>\n<p><br/></p>\n<hr>\n<h3 id=\"什么是命题，什么是论证\"><a href=\"#什么是命题，什么是论证\" class=\"headerlink\" title=\"什么是命题，什么是论证\"></a>什么是命题，什么是论证</h3><p>首先看一组自然语言：</p>\n<blockquote>\n<ol>\n<li><p>“明天会下雨”</p>\n</li>\n<li><p>“Tomorrow will rain”</p>\n</li>\n<li><p>“$2+3=5$”</p>\n</li>\n<li><p>“$2^{=3}+5$”</p>\n</li>\n<li><p>“别吃冰淇淋”</p>\n</li>\n<li><p>“吃什么口味的？”</p>\n</li>\n</ol>\n</blockquote>\n<p>1和2是不同的语句，但对于一个同时掌握中文和英文的人来说，会认为1和2想要表达某种相同的东西，我们暂时称这种语句背后的“含义”为“命题”。那么，1的含义是什么呢？假如有人在2020年1月1日说了“明天会下雨”，这句话的含义是2020年1月2日会下雨；但假如此人是在2020年1月2日说了这句话，那么这句话的含义又变成了2020年1月3日会下雨。所以，同一个语句，也有可能表达多个含义。自然语言中充满了各种各样有歧义的语句。对于逻辑学家，想要追求确定性和唯一性，就不能容许含糊的歧义存在，把语义从语句中抽离出来，就是必要的工作。</p>\n<p>语句被分为<strong>语法</strong>和<strong>语义</strong>。简单来说，语法中包含一些符号，这些符号按特定的规则排列组合，使用这种语言的人遵循这些规则，就可以表达某种语义。以3和4为例，学过算术的人一眼就可以看懂3表达的含义，而4仅仅是把同样的一些符号换了换顺序，就让人看不懂了。</p>\n<p>现在知道了是什么构成一个<strong>语句</strong>。开头说过，暂时把语句的“含义”叫做“命题”，那是不是所有有含义的语句都算作一个命题呢？来看看5和6，它们显然也都能传递一些信息，但对于一个具有古希腊演绎精神的哲学家用处不大。一个皇帝确实可以依仗手中的权力号令手下为他杀掉一个叛臣，但就算他破口大骂，也不能让一个没电的灯泡发出光来。同样，一个道士向上天询问日月星辰运行之理，答案也很难凭着打坐降临这颗虔诚的头脑。只有<strong>真伪</strong>才有意义，<strong>命题</strong>就是这样一些语句，它们非真即伪，必然可以判定（<em>二值原则</em>）。</p>\n<p>判定命题真伪的过程就叫做<strong>论证</strong>。一般来说，一个论证由<strong>前提</strong>和<strong>结论</strong>组成，并且前提和结论都必须是命题。看下面两个例子：</p>\n<blockquote>\n<ol>\n<li>苏格拉底是人，人都会死，所以苏格拉底也会死。</li>\n<li>苏格拉底会死，人都会死，所以苏格拉底是人。</li>\n</ol>\n</blockquote>\n<p>人们会接受1的说法，但却不大会接受2的说法，即使2的前提是真的。当人们接受前提的情况下，一定也会接受结论，才可以称一个论证为正确的论证。如果我们换掉1中的“苏格拉底”，“人”，“会死”，在对应的位置换成其它项：</p>\n<blockquote>\n<p>黑天鹅是天鹅，天鹅有两只翅膀，所以黑天鹅有两只翅膀。</p>\n</blockquote>\n<p>只要我们承认黑天鹅是天鹅，天鹅有两只翅膀，那么就会认同结论是正确的。事实上，“苏格拉底”，“人”，“会死”可以换成任何东西，完全可以用符号代替，这样就可以抽象出论证的形式：</p>\n<blockquote>\n<p>P 是 Q，所有的 Q 都是 R，所以 P 是 R。</p>\n</blockquote>\n<p>按照一个<strong>有效论证</strong>的形式，只要前提为真，其结论就一定为真。这称为逻辑论证的<strong>可靠性（soundness）</strong>。</p>\n<p><br/></p>\n<hr>\n<h3 id=\"逻辑系统\"><a href=\"#逻辑系统\" class=\"headerlink\" title=\"逻辑系统\"></a>逻辑系统</h3><p>苏格拉底的例子是亚里士多德提出的经典“三段论”结构，其基本单位是<strong>项（terms）</strong>，被称为<strong>语词逻辑（term logic）</strong>。</p>\n<p>还有一种常见的逻辑结构，以<strong>命题（proposition）</strong>为基本单位：</p>\n<blockquote>\n<p>如果苹果熟了，那么苹果就会落地。</p>\n<p>苹果熟了。</p>\n<p>苹果会落地。</p>\n</blockquote>\n<p>这种论证形式可以表示为：</p>\n<blockquote>\n<p>若 P，则 Q</p>\n<p>P</p>\n<p>所以 Q</p>\n</blockquote>\n<p>这种逻辑系统被称为<strong>命题逻辑（propositional logic）</strong>，命题逻辑是一种很简单也很基本的逻辑系统，处理能力有限。</p>\n<p>更复杂的逻辑系统有<strong>述词逻辑（predicate logic）</strong>，把命题拆分为主词（subject）和述词（predicate）；<strong>一阶逻辑（first order logic）</strong>（项、量词、加入关系述词等）等等。</p>\n<p>……</p>\n<p>以上的逻辑理论都基于一个基本的假设：命题必然非真即伪，这在语义学中被称为“二值原则”，而在亚里士多德提出的逻辑三大规律中被称为“排中律”。如果不满足这个基本假设，事实上，自然语言中这样的语句并不少见，比如：</p>\n<blockquote>\n<p>明天气温会超过40摄氏度。</p>\n<p>小光工作勤勤恳恳。</p>\n</blockquote>\n<p>第一句话如果把“明天”看作永远未到来的“明天”，那这句话永远也无法判定真伪，只对可能性做了一个声明；</p>\n<p>第二句话加入小光一辈子都没有找到一份工作，这句话也无法判定真伪。</p>\n<p>处理这样的语句有另外的逻辑系统，比如<strong>模态逻辑</strong>，<strong>直觉逻辑</strong>。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2\" target=\"_blank\" rel=\"noopener\">台大开放式课程 傅皓政《逻辑》</a></em></small></p>"},{"title":"AI学习笔记--监督学习","date":"2020-08-11T15:45:00.000Z","mathjax":true,"_content":"\nLogistic Regression，Naive Bayes，SVM，Decision Trees，KNN，Random Forest，Ada Boost<!--more-->\n\n不涉及一些方法背后繁复的数学推导，只求列出要领。\n\n### Logistic Regression（逻辑回归/对数几率回归）\n\n一般的线性回归模型形式为\n$$\nf(\\textbf{x})=\\omega^T\\textbf{x}+b\n$$\n其输出$f(\\textbf{x})$的值域是一个连续的实数域。但如果我们想要预测的是一个事件发生与否，比如基于体检指标预测是否患癌症，根据大气温度、湿度、风力和压强等预测是否会下雨，这时的因变量就是一个离散的布尔变量，$f(\\textbf{x})\\in \\{0,1\\}$。\n\n* #### 用于分类的逻辑回归\n\n  这种类型的问题可以归纳为二分类问题。从一般的线性回归模型到二分类模型，我们需要一个函数，能够把因变量从连续的实数值域映射到{0,1}值域。阶跃函数正是这样一种函数，比如，我们可以规定\n  $$\n  y=\\left\\{\n               \\begin{array}{lr}\n               0, \\ f(\\textbf{x})<0 &  \\\\\n               1, \\ f(\\textbf{x})\\geq0 & \n               \\end{array}\n  \\right.\n  $$\n  对于原函数$f(\\textbf{x})$，我们还可以稍作变换，找到一个跟阶跃函数很像的”连续版本“——Sigmoid函数\n  $$\n  y=\\frac{1}{1+e^{-z}}\n  $$\n  这里$z=f(\\textbf{x})$。Sigmoid函数的图像为\n\n  <img src=\"/images/截屏2020-08-17 下午11.04.31.png\" alt=\"截屏2020-08-17 下午11.04.31\" style=\"zoom:50%;\" />\n\n  可以看出，其值域为(0,1)，而且在y=0.5附近很陡峭。正是得益于此特性，某种程度上，Sigmoid函数的y**可以看作事件发生的概率**。当$y> 0.5$时，认为事件发生概率大于不发生的概率，分类为1；$y<0.$5时，反之，分类为0（这也对应一个阶跃函数）。\n\n  Sigmoid的反函数为：\n  $$\n  z = ln\\frac{y}{1-y}\n  $$\n  通常把$\\frac{y}{1-y}$称为“**胜算**”或“**几率**”。\n\n* #### 求逻辑回归的回归系数\n\n  逻辑回归的回归系数用极大似然法求解，似然的概念在朴素贝叶斯方法里有简要介绍。\n\n  假设我们的训练集为$\\{(\\textbf{x}_1,d_1),(\\textbf{x}_2,d_2)...(\\textbf{x}_m,d_m)\\}$，为了区分二值的分类结果和Sigmoid函数的事件“概率”，这里分类结果记作$d_i$。回归模型为\n  $$\n  z=\\omega^T\\textbf{x}+b\n  $$\n  则回归模型的对数似然为\n  $$\n  L(\\omega,b;(X,D))=\\sum_{i=1}^mP(d_i|\\omega,b;\\textbf{x}_i)\n  $$\n  其中$P(d_i|\\omega,b;\\textbf{x}_i)$可以根据Sigmoid函数求得。\n  $$\n  P(d_i|\\omega,b;\\textbf{x}_i)=d_iy_i+(1-d_i)(1-y_i)\n  $$\n  最大化对数似然，求得回归系数$\\omega,b$。\n\n### Naive Bayes (朴素贝叶斯)\n\n* #### Bayes Rule（贝叶斯规则）\n\n  $$\n  P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n  $$\n\n  分子部分P(B|A)，P(A)为先验概率，要求的P(A|B)是已知B发生的情况下A的后验概率。\n\n* #### 贝叶斯规则用于分类\n\n  从训练样本中估计先验概率，利用贝叶斯规则对测试样本分类，这种分类方法被称为贝叶斯分类器（Bayse Classifier）。\n  \n  贝叶斯公式运用到分类时具体为\n  $$\n  P(label|features)=\\frac{P(features|label)P(label)}{P(features)}\n  $$\n  这里label是我们想要的分类结果，features是样本所具有的一些特征。$P(label)$是先验概率，$P(features|label)$是label条件下的类条件概率（class-conditional probability），$P(features)$用于归一化，与分类标签无关。\n  \n  贝叶斯分类的关键是如何从训练数据中估计$P(label)$和$P(features|label)$。\n  \n  对于$P(label)$，可以简单地根据大数定律估计：\n  $$\n  P(label)=\\frac{D_{label}}{D}\n  $$\n  其中$D$是总的样本数，$D_{label}$是分类为label的样本数。\n  \n  然而要估计$P(features|label)$却没那么简单，这是所有特征的**联合概率**。而特征往往有很多个，假设每个特征都是二值的，总共有d个特征，则对于一个样本来说它的特征就有$2^d$个可能取值。不幸的的是，样本数往往比$2^d$要小，也就意味着有很多的可能取值会因为样本数不足而没有机会出现。所以，以大数定律粗暴估计就不可取。\n  \n  解决办法有两个，一个是参数估计，另外一个是假设特征条件独立。\n  \n  * ##### 参数估计（极大似然估计）\n  \n  先简单了解一下什么是似然（Likelihood)。似然和概率可以说是一对互为逆反的双生子。一般情况下，我们知道一个概率分布，去估计某一具体事件发生的可能性，这时用到的是概率；但还有些时候，我们知道的是一些事件的结果，反过来要去推断概率分布的参数，这时用到的就是似然。\n  \n  n个独立同分布的样本$X(x_1,x_2...x_n)$，其似然是\n  $$\n  L(\\theta;X)=\\prod_{i=1}^nP(x_i|\\theta)\n  $$\n  在对$P(features|label)$的估计中，我们先假定样本集$D_{label}$具有某种概率分布，然后令该分布的参数之似然函数（或对数似然）最大，得到的就是该种分布其参数的最优估计。这种方法称为极大似然估计。\n  \n  * ##### 朴素贝叶斯分类\n  \n  另外一种克服直接估计$P(features|label)$困难的方法是，假设每种特征对分类的影响是独立的，即“属性条件独立性假设”（attribute conditional independence assumption）。有了这个假设，$P(features|label)$就可写开成为：\n  $$\n  P(features|label)=\\prod_{j=1}^dP(f_j|label)\n  $$\n  其中$features(f_1,f_2...f_d)$有d个。这时，每个$P(f_j|label)$就可以用频率来估计了：\n  $$\n  P(f_j|label)=\\frac{D^{f_j}_{label}}{D_{label}}\n  $$\n  最早朴素贝叶斯算法被用于语词分类，它无法顾及语词（每个语词是一个feature）的顺序和连用，就是因为假设每个特征之间是独立的。\n\n### Surpport Vector Machine（支持向量机）\n\n* #### SVM原理\n\n  找到一个超平面（hyperplane）或决策边界（decision boundary），能将样本最优分类。距离超平面最近的样本点被称为“支持向量（Support Vector）”，两个异类支持向量到超平面的距离之和称为**间隔（margin）**。SVM希望令间隔最大，这可以转化为一个有不等式约束条件的极值问题，用拉格朗日乘子法解决（KKT条件）。\n\n  <img src=\"/images/截屏2020-08-27 下午1.51.54.png\" alt=\"截屏2020-08-27 下午1.51.54\" style=\"zoom:80%;\" />\n\n* #### SVM用于分类\n\n  * ##### 硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）\n\n  在认识什么是SVM之前，先了解一下两个概念：MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier），它们是SVM的基础。\n\n  在SVM思想刚刚提出的时候，把所有样本分类**正确**是必须的要求，其次才是使得**间隔最大**。这种分类器称为Maximum Margin Classifier（MMC）。这样严格的要求限制了其应用的场景，只有非常理想的线性可分的数据集才能使用。当找不到一个超平面划分数据集时，MMC认为出现了异常值（outlier)。\n\n  <img src=\"/images/截屏2020-08-27 下午2.55.10.png\" alt=\"截屏2020-08-27 下午2.55.10\" style=\"zoom:80%;\" />\n\n  SVC则可以容忍个别异常值，允许有个别样本点在分类边界的错误一侧。区别于严格将所有样本分类正确，这时的间隔称为**软间隔（soft margin）**，前者当然就是**硬间隔（hard margin）**。简单来说是在最小化目标函数时，额外考虑边界将样本分类错误的惩罚，在原来的严格的不等式约束前加一个惩罚系数C，C越大，对样本分类正确性的要求就越严格。这样就引入了一些“弹性”，从“硬间隔”变成了“软间隔”。\n\n  * ##### 核技巧\n\n  当在原始输入的特征空间内不能找到一个线性的超平面来分割时，用到**核技巧（kernel trick）**，把原始低维的特征映射到高维，找到一个线性超平面，再在原始特征空间表示出来。用一个最简单的例子来直观感受一下：\n\n  <img src=\"/images/IMG_0726.JPG\" alt=\"IMG_0726\" style=\"zoom:40%;\" />\n\n  在高维的特征空间用拉格朗日乘子法求解问题时会遇到一个困难。在原始特征空间内，不等式约束的极值问题转化为拉格朗日函数的对偶问题后，约化后求解需要计算$x_i^Tx_j^T$，而转化到高维特征空间，这个式子就变策成了$\\phi(x_i)^T\\phi(x_j)$，其中$\\phi(x)$是$x$映射到高维特征空间的向量。当$\\phi(x)$所在空间的维数很高时，不可能对$\\phi(x_i)^T\\phi(x_j)$直接计算。但是假如我们知道一个函数$\\kappa(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$，就可以直接在原始特征空间计算高维特征空间的内积。这个$\\kappa(x_i,x_j)$函数就称为**核函数（kernel function）**。\n\n  实际中，我们不是先知，核函数的具体形式我们是不知道的，只能靠猜。所以核函数的选择是SVM一个非常大的不确定性因素。\n\n  SVM可以用于多分类吗？\n\n* #### SVM用于回归\n\n  SVM也可以用来处理回归问题。传统的回归问题是最小化模型输出$f(x)$样本真实值$y$之间的差距。SVM回归的区别在于它可以容忍$f(x)$和$y$之间有$\\epsilon$的偏差，小于$\\epsilon$的偏差忽略不计入损失。相当于以$f(x)$为中心，构建了一个宽度为$2\\epsilon$的间隔带，落入此间隔带的样本都被认为是预测正确的。\n\n### Decision Trees（决策树）\n\n面临一个多元线性问题时，决策树可以找到一个“锯齿状”的边界。这是决策树边界的一个最明显的特征。决策树算法就是用计算机找到最合适的这种边界。它从训练集中得到一个树状模型，包含一个根节点，若干内部节点和叶节点。每个分支节点都是一次决策，这样一系列的决策决定了我们的最终分类判断。\n\n<img src=\"/images/IMG_0727.JPG\" alt=\"IMG_0727\" style=\"zoom:40%;\" />\n\n* #### Entropy（熵）\n\n  **熵（Entropy）**用来衡量一类中样本的杂质含量（impurity）。公式为：\n  $$\n  Entropy(D) = \\sum\\limits_{i}^{|\\textbf{y}|}{-P_i}log_2P_i\n  $$\n  其中$P_i$是当前样本集D中第i类的样本所占的比例，$|\\textbf{y}|$是类别数。当所有样本均为同一类时，纯度最高，Entropy=0；当各类样本在样本集中均匀分布时，杂质含量最高，$Entropy = log_2|\\textbf{y}|$。\n\n  一种更本质的理解是，熵是不确定性的度量，写成$Entropy = \\sum\\limits_{i}{P_i}log_2\\frac{1}{P_i}$，$log_2\\frac{1}{P_i}$是第i类的不确定程度/混乱程度。机器学习算法想要从海量**数据**（经验）中获得确定性的**信息**（模型），就如同沙里淘金。当沙子和金子混在一起时，纯度很低，熵很高；当沙子和金子清楚地分成两堆，这时纯度就很高，熵就很低。纯度很高的金子就是我们想要的确定性的信息，沙子则是**噪音**。\n\n* #### Information gain（信息增益）\n\n  如何把金子从沙堆里淘出来呢？现在我们有了评价金子纯度的方法——熵，要想办法让熵越小越好，这样金子的纯度就越高。决策树就是依次选择最优的特征进行划分，能让熵减小得越多的划分方法，就是越好的划分。**信息增益（information gain）**就是评价“熵减小”量的一个指标。\n\n  决策树划定决策边界的依据，也就是决策树算法的核心，是要**最大化信息增益**。用特征a对样本集D划分的信息增益为：\n  $$\n  information\\ gain(D,a)=Entropy_{parent}(D)-\\sum\\limits_{v}^{V}\\frac{|D^v|}{|D|}Entropy_{children}(D^v)\n  $$\n  其中特征a共有V个可能取值，取值为$a^v$的样本子集包含的样本数为$|D^v|$。在上一层划分的基础上，如何决定下一个分支节点用哪个特征进行划分，需要用到信息增益。举个例子：\n\n  <img src=\"/images/IMG_0728.PNG\" alt=\"IMG_0728\" style=\"zoom:40%;\" />\n\n  假如我们想把样本集分为两类：蓝圈和红叉。根节点是样本全集。计算出根节点的信息熵为1。下面要决定下一个分支节点选用哪个特征进行划分，这里拿特征$x>1$和$x>2$举例。比较信息增益得知，前者更适合作为下一个分支节点。\n\n  以信息增益作为判别标准的算法称为ID3（Iterative Dichotomiser）。信息增益有一个缺点，它对可能取值数多的特征有偏好。同样是在各特征取值均匀分布情况，特征可能取值数多的，信息增益会更大。为了克服这个缺点，改进的C4.5算法是以**增益率**作为标准。后来的CART决策树是以**基尼指数（Gini index）**作为标准。\n\n### K-Nearest Neighbors（K最邻近）\n\n### Random Forest（随机森林）\n\n### Ada Boost\n\n\n\n**选择算法**\n\n1）理解算法原理，适合哪种数据\n\n2）用测试集检验算法的表现\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[机器学习入门]-udacity](https://www.udacity.com/course/intro-to-machine-learning--ud120)*</small>\n\n<small>*[[机器学习西瓜书]-周志华]()*</small>\n\n<small>*[[机器学习算法（AI入门体验）开源学习资料]-Datawhale](https://github.com/datawhalechina/team-learning-data-mining/tree/master/IntroductionExperienceAI)*</small>\n\n<small>*[[决策树算法原理]-刘建平](https://www.cnblogs.com/pinard/p/6050306.html)*</small>\n\n<small>*[[Road to SVM: Maximal Margin Classifier and Support Vector Classifier]-Valentina ALto](https://medium.com/analytics-vidhya/road-to-svm-maximal-margin-classifier-and-support-vector-classifier-85cb1e3dcc0a)*</small>\n\n","source":"_posts/2020-02-26-AI_NB_SVM_DT.md","raw":"---\ntitle: AI学习笔记--监督学习\ndate: 2020-08-11 23:45:00\ncategories:\n- 计算机科学\ntags: \n- 人工智能\n- python\n- 算法\nmathjax: true\n---\n\nLogistic Regression，Naive Bayes，SVM，Decision Trees，KNN，Random Forest，Ada Boost<!--more-->\n\n不涉及一些方法背后繁复的数学推导，只求列出要领。\n\n### Logistic Regression（逻辑回归/对数几率回归）\n\n一般的线性回归模型形式为\n$$\nf(\\textbf{x})=\\omega^T\\textbf{x}+b\n$$\n其输出$f(\\textbf{x})$的值域是一个连续的实数域。但如果我们想要预测的是一个事件发生与否，比如基于体检指标预测是否患癌症，根据大气温度、湿度、风力和压强等预测是否会下雨，这时的因变量就是一个离散的布尔变量，$f(\\textbf{x})\\in \\{0,1\\}$。\n\n* #### 用于分类的逻辑回归\n\n  这种类型的问题可以归纳为二分类问题。从一般的线性回归模型到二分类模型，我们需要一个函数，能够把因变量从连续的实数值域映射到{0,1}值域。阶跃函数正是这样一种函数，比如，我们可以规定\n  $$\n  y=\\left\\{\n               \\begin{array}{lr}\n               0, \\ f(\\textbf{x})<0 &  \\\\\n               1, \\ f(\\textbf{x})\\geq0 & \n               \\end{array}\n  \\right.\n  $$\n  对于原函数$f(\\textbf{x})$，我们还可以稍作变换，找到一个跟阶跃函数很像的”连续版本“——Sigmoid函数\n  $$\n  y=\\frac{1}{1+e^{-z}}\n  $$\n  这里$z=f(\\textbf{x})$。Sigmoid函数的图像为\n\n  <img src=\"/images/截屏2020-08-17 下午11.04.31.png\" alt=\"截屏2020-08-17 下午11.04.31\" style=\"zoom:50%;\" />\n\n  可以看出，其值域为(0,1)，而且在y=0.5附近很陡峭。正是得益于此特性，某种程度上，Sigmoid函数的y**可以看作事件发生的概率**。当$y> 0.5$时，认为事件发生概率大于不发生的概率，分类为1；$y<0.$5时，反之，分类为0（这也对应一个阶跃函数）。\n\n  Sigmoid的反函数为：\n  $$\n  z = ln\\frac{y}{1-y}\n  $$\n  通常把$\\frac{y}{1-y}$称为“**胜算**”或“**几率**”。\n\n* #### 求逻辑回归的回归系数\n\n  逻辑回归的回归系数用极大似然法求解，似然的概念在朴素贝叶斯方法里有简要介绍。\n\n  假设我们的训练集为$\\{(\\textbf{x}_1,d_1),(\\textbf{x}_2,d_2)...(\\textbf{x}_m,d_m)\\}$，为了区分二值的分类结果和Sigmoid函数的事件“概率”，这里分类结果记作$d_i$。回归模型为\n  $$\n  z=\\omega^T\\textbf{x}+b\n  $$\n  则回归模型的对数似然为\n  $$\n  L(\\omega,b;(X,D))=\\sum_{i=1}^mP(d_i|\\omega,b;\\textbf{x}_i)\n  $$\n  其中$P(d_i|\\omega,b;\\textbf{x}_i)$可以根据Sigmoid函数求得。\n  $$\n  P(d_i|\\omega,b;\\textbf{x}_i)=d_iy_i+(1-d_i)(1-y_i)\n  $$\n  最大化对数似然，求得回归系数$\\omega,b$。\n\n### Naive Bayes (朴素贝叶斯)\n\n* #### Bayes Rule（贝叶斯规则）\n\n  $$\n  P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n  $$\n\n  分子部分P(B|A)，P(A)为先验概率，要求的P(A|B)是已知B发生的情况下A的后验概率。\n\n* #### 贝叶斯规则用于分类\n\n  从训练样本中估计先验概率，利用贝叶斯规则对测试样本分类，这种分类方法被称为贝叶斯分类器（Bayse Classifier）。\n  \n  贝叶斯公式运用到分类时具体为\n  $$\n  P(label|features)=\\frac{P(features|label)P(label)}{P(features)}\n  $$\n  这里label是我们想要的分类结果，features是样本所具有的一些特征。$P(label)$是先验概率，$P(features|label)$是label条件下的类条件概率（class-conditional probability），$P(features)$用于归一化，与分类标签无关。\n  \n  贝叶斯分类的关键是如何从训练数据中估计$P(label)$和$P(features|label)$。\n  \n  对于$P(label)$，可以简单地根据大数定律估计：\n  $$\n  P(label)=\\frac{D_{label}}{D}\n  $$\n  其中$D$是总的样本数，$D_{label}$是分类为label的样本数。\n  \n  然而要估计$P(features|label)$却没那么简单，这是所有特征的**联合概率**。而特征往往有很多个，假设每个特征都是二值的，总共有d个特征，则对于一个样本来说它的特征就有$2^d$个可能取值。不幸的的是，样本数往往比$2^d$要小，也就意味着有很多的可能取值会因为样本数不足而没有机会出现。所以，以大数定律粗暴估计就不可取。\n  \n  解决办法有两个，一个是参数估计，另外一个是假设特征条件独立。\n  \n  * ##### 参数估计（极大似然估计）\n  \n  先简单了解一下什么是似然（Likelihood)。似然和概率可以说是一对互为逆反的双生子。一般情况下，我们知道一个概率分布，去估计某一具体事件发生的可能性，这时用到的是概率；但还有些时候，我们知道的是一些事件的结果，反过来要去推断概率分布的参数，这时用到的就是似然。\n  \n  n个独立同分布的样本$X(x_1,x_2...x_n)$，其似然是\n  $$\n  L(\\theta;X)=\\prod_{i=1}^nP(x_i|\\theta)\n  $$\n  在对$P(features|label)$的估计中，我们先假定样本集$D_{label}$具有某种概率分布，然后令该分布的参数之似然函数（或对数似然）最大，得到的就是该种分布其参数的最优估计。这种方法称为极大似然估计。\n  \n  * ##### 朴素贝叶斯分类\n  \n  另外一种克服直接估计$P(features|label)$困难的方法是，假设每种特征对分类的影响是独立的，即“属性条件独立性假设”（attribute conditional independence assumption）。有了这个假设，$P(features|label)$就可写开成为：\n  $$\n  P(features|label)=\\prod_{j=1}^dP(f_j|label)\n  $$\n  其中$features(f_1,f_2...f_d)$有d个。这时，每个$P(f_j|label)$就可以用频率来估计了：\n  $$\n  P(f_j|label)=\\frac{D^{f_j}_{label}}{D_{label}}\n  $$\n  最早朴素贝叶斯算法被用于语词分类，它无法顾及语词（每个语词是一个feature）的顺序和连用，就是因为假设每个特征之间是独立的。\n\n### Surpport Vector Machine（支持向量机）\n\n* #### SVM原理\n\n  找到一个超平面（hyperplane）或决策边界（decision boundary），能将样本最优分类。距离超平面最近的样本点被称为“支持向量（Support Vector）”，两个异类支持向量到超平面的距离之和称为**间隔（margin）**。SVM希望令间隔最大，这可以转化为一个有不等式约束条件的极值问题，用拉格朗日乘子法解决（KKT条件）。\n\n  <img src=\"/images/截屏2020-08-27 下午1.51.54.png\" alt=\"截屏2020-08-27 下午1.51.54\" style=\"zoom:80%;\" />\n\n* #### SVM用于分类\n\n  * ##### 硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）\n\n  在认识什么是SVM之前，先了解一下两个概念：MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier），它们是SVM的基础。\n\n  在SVM思想刚刚提出的时候，把所有样本分类**正确**是必须的要求，其次才是使得**间隔最大**。这种分类器称为Maximum Margin Classifier（MMC）。这样严格的要求限制了其应用的场景，只有非常理想的线性可分的数据集才能使用。当找不到一个超平面划分数据集时，MMC认为出现了异常值（outlier)。\n\n  <img src=\"/images/截屏2020-08-27 下午2.55.10.png\" alt=\"截屏2020-08-27 下午2.55.10\" style=\"zoom:80%;\" />\n\n  SVC则可以容忍个别异常值，允许有个别样本点在分类边界的错误一侧。区别于严格将所有样本分类正确，这时的间隔称为**软间隔（soft margin）**，前者当然就是**硬间隔（hard margin）**。简单来说是在最小化目标函数时，额外考虑边界将样本分类错误的惩罚，在原来的严格的不等式约束前加一个惩罚系数C，C越大，对样本分类正确性的要求就越严格。这样就引入了一些“弹性”，从“硬间隔”变成了“软间隔”。\n\n  * ##### 核技巧\n\n  当在原始输入的特征空间内不能找到一个线性的超平面来分割时，用到**核技巧（kernel trick）**，把原始低维的特征映射到高维，找到一个线性超平面，再在原始特征空间表示出来。用一个最简单的例子来直观感受一下：\n\n  <img src=\"/images/IMG_0726.JPG\" alt=\"IMG_0726\" style=\"zoom:40%;\" />\n\n  在高维的特征空间用拉格朗日乘子法求解问题时会遇到一个困难。在原始特征空间内，不等式约束的极值问题转化为拉格朗日函数的对偶问题后，约化后求解需要计算$x_i^Tx_j^T$，而转化到高维特征空间，这个式子就变策成了$\\phi(x_i)^T\\phi(x_j)$，其中$\\phi(x)$是$x$映射到高维特征空间的向量。当$\\phi(x)$所在空间的维数很高时，不可能对$\\phi(x_i)^T\\phi(x_j)$直接计算。但是假如我们知道一个函数$\\kappa(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$，就可以直接在原始特征空间计算高维特征空间的内积。这个$\\kappa(x_i,x_j)$函数就称为**核函数（kernel function）**。\n\n  实际中，我们不是先知，核函数的具体形式我们是不知道的，只能靠猜。所以核函数的选择是SVM一个非常大的不确定性因素。\n\n  SVM可以用于多分类吗？\n\n* #### SVM用于回归\n\n  SVM也可以用来处理回归问题。传统的回归问题是最小化模型输出$f(x)$样本真实值$y$之间的差距。SVM回归的区别在于它可以容忍$f(x)$和$y$之间有$\\epsilon$的偏差，小于$\\epsilon$的偏差忽略不计入损失。相当于以$f(x)$为中心，构建了一个宽度为$2\\epsilon$的间隔带，落入此间隔带的样本都被认为是预测正确的。\n\n### Decision Trees（决策树）\n\n面临一个多元线性问题时，决策树可以找到一个“锯齿状”的边界。这是决策树边界的一个最明显的特征。决策树算法就是用计算机找到最合适的这种边界。它从训练集中得到一个树状模型，包含一个根节点，若干内部节点和叶节点。每个分支节点都是一次决策，这样一系列的决策决定了我们的最终分类判断。\n\n<img src=\"/images/IMG_0727.JPG\" alt=\"IMG_0727\" style=\"zoom:40%;\" />\n\n* #### Entropy（熵）\n\n  **熵（Entropy）**用来衡量一类中样本的杂质含量（impurity）。公式为：\n  $$\n  Entropy(D) = \\sum\\limits_{i}^{|\\textbf{y}|}{-P_i}log_2P_i\n  $$\n  其中$P_i$是当前样本集D中第i类的样本所占的比例，$|\\textbf{y}|$是类别数。当所有样本均为同一类时，纯度最高，Entropy=0；当各类样本在样本集中均匀分布时，杂质含量最高，$Entropy = log_2|\\textbf{y}|$。\n\n  一种更本质的理解是，熵是不确定性的度量，写成$Entropy = \\sum\\limits_{i}{P_i}log_2\\frac{1}{P_i}$，$log_2\\frac{1}{P_i}$是第i类的不确定程度/混乱程度。机器学习算法想要从海量**数据**（经验）中获得确定性的**信息**（模型），就如同沙里淘金。当沙子和金子混在一起时，纯度很低，熵很高；当沙子和金子清楚地分成两堆，这时纯度就很高，熵就很低。纯度很高的金子就是我们想要的确定性的信息，沙子则是**噪音**。\n\n* #### Information gain（信息增益）\n\n  如何把金子从沙堆里淘出来呢？现在我们有了评价金子纯度的方法——熵，要想办法让熵越小越好，这样金子的纯度就越高。决策树就是依次选择最优的特征进行划分，能让熵减小得越多的划分方法，就是越好的划分。**信息增益（information gain）**就是评价“熵减小”量的一个指标。\n\n  决策树划定决策边界的依据，也就是决策树算法的核心，是要**最大化信息增益**。用特征a对样本集D划分的信息增益为：\n  $$\n  information\\ gain(D,a)=Entropy_{parent}(D)-\\sum\\limits_{v}^{V}\\frac{|D^v|}{|D|}Entropy_{children}(D^v)\n  $$\n  其中特征a共有V个可能取值，取值为$a^v$的样本子集包含的样本数为$|D^v|$。在上一层划分的基础上，如何决定下一个分支节点用哪个特征进行划分，需要用到信息增益。举个例子：\n\n  <img src=\"/images/IMG_0728.PNG\" alt=\"IMG_0728\" style=\"zoom:40%;\" />\n\n  假如我们想把样本集分为两类：蓝圈和红叉。根节点是样本全集。计算出根节点的信息熵为1。下面要决定下一个分支节点选用哪个特征进行划分，这里拿特征$x>1$和$x>2$举例。比较信息增益得知，前者更适合作为下一个分支节点。\n\n  以信息增益作为判别标准的算法称为ID3（Iterative Dichotomiser）。信息增益有一个缺点，它对可能取值数多的特征有偏好。同样是在各特征取值均匀分布情况，特征可能取值数多的，信息增益会更大。为了克服这个缺点，改进的C4.5算法是以**增益率**作为标准。后来的CART决策树是以**基尼指数（Gini index）**作为标准。\n\n### K-Nearest Neighbors（K最邻近）\n\n### Random Forest（随机森林）\n\n### Ada Boost\n\n\n\n**选择算法**\n\n1）理解算法原理，适合哪种数据\n\n2）用测试集检验算法的表现\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[机器学习入门]-udacity](https://www.udacity.com/course/intro-to-machine-learning--ud120)*</small>\n\n<small>*[[机器学习西瓜书]-周志华]()*</small>\n\n<small>*[[机器学习算法（AI入门体验）开源学习资料]-Datawhale](https://github.com/datawhalechina/team-learning-data-mining/tree/master/IntroductionExperienceAI)*</small>\n\n<small>*[[决策树算法原理]-刘建平](https://www.cnblogs.com/pinard/p/6050306.html)*</small>\n\n<small>*[[Road to SVM: Maximal Margin Classifier and Support Vector Classifier]-Valentina ALto](https://medium.com/analytics-vidhya/road-to-svm-maximal-margin-classifier-and-support-vector-classifier-85cb1e3dcc0a)*</small>\n\n","slug":"2020-02-26-AI_NB_SVM_DT","published":1,"updated":"2020-10-20T14:20:58.891Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5500022sfycxz4cmiw","content":"<p>Logistic Regression，Naive Bayes，SVM，Decision Trees，KNN，Random Forest，Ada Boost<a id=\"more\"></a></p>\n<p>不涉及一些方法背后繁复的数学推导，只求列出要领。</p>\n<h3 id=\"Logistic-Regression（逻辑回归-对数几率回归）\"><a href=\"#Logistic-Regression（逻辑回归-对数几率回归）\" class=\"headerlink\" title=\"Logistic Regression（逻辑回归/对数几率回归）\"></a>Logistic Regression（逻辑回归/对数几率回归）</h3><p>一般的线性回归模型形式为</p>\n<script type=\"math/tex; mode=display\">\nf(\\textbf{x})=\\omega^T\\textbf{x}+b</script><p>其输出$f(\\textbf{x})$的值域是一个连续的实数域。但如果我们想要预测的是一个事件发生与否，比如基于体检指标预测是否患癌症，根据大气温度、湿度、风力和压强等预测是否会下雨，这时的因变量就是一个离散的布尔变量，$f(\\textbf{x})\\in \\{0,1\\}$。</p>\n<ul>\n<li><h4 id=\"用于分类的逻辑回归\"><a href=\"#用于分类的逻辑回归\" class=\"headerlink\" title=\"用于分类的逻辑回归\"></a>用于分类的逻辑回归</h4><p>这种类型的问题可以归纳为二分类问题。从一般的线性回归模型到二分类模型，我们需要一个函数，能够把因变量从连续的实数值域映射到{0,1}值域。阶跃函数正是这样一种函数，比如，我们可以规定</p>\n<script type=\"math/tex; mode=display\">\ny=\\left\\{\n             \\begin{array}{lr}\n             0, \\ f(\\textbf{x})<0 &  \\\\\n             1, \\ f(\\textbf{x})\\geq0 & \n             \\end{array}\n\\right.</script><p>对于原函数$f(\\textbf{x})$，我们还可以稍作变换，找到一个跟阶跃函数很像的”连续版本“——Sigmoid函数</p>\n<script type=\"math/tex; mode=display\">\ny=\\frac{1}{1+e^{-z}}</script><p>这里$z=f(\\textbf{x})$。Sigmoid函数的图像为</p>\n<p><img src=\"/images/截屏2020-08-17 下午11.04.31.png\" alt=\"截屏2020-08-17 下午11.04.31\" style=\"zoom:50%;\" /></p>\n<p>可以看出，其值域为(0,1)，而且在y=0.5附近很陡峭。正是得益于此特性，某种程度上，Sigmoid函数的y<strong>可以看作事件发生的概率</strong>。当$y&gt; 0.5$时，认为事件发生概率大于不发生的概率，分类为1；$y&lt;0.$5时，反之，分类为0（这也对应一个阶跃函数）。</p>\n<p>Sigmoid的反函数为：</p>\n<script type=\"math/tex; mode=display\">\nz = ln\\frac{y}{1-y}</script><p>通常把$\\frac{y}{1-y}$称为“<strong>胜算</strong>”或“<strong>几率</strong>”。</p>\n</li>\n<li><h4 id=\"求逻辑回归的回归系数\"><a href=\"#求逻辑回归的回归系数\" class=\"headerlink\" title=\"求逻辑回归的回归系数\"></a>求逻辑回归的回归系数</h4><p>逻辑回归的回归系数用极大似然法求解，似然的概念在朴素贝叶斯方法里有简要介绍。</p>\n<p>假设我们的训练集为$\\{(\\textbf{x}_1,d_1),(\\textbf{x}_2,d_2)…(\\textbf{x}_m,d_m)\\}$，为了区分二值的分类结果和Sigmoid函数的事件“概率”，这里分类结果记作$d_i$。回归模型为</p>\n<script type=\"math/tex; mode=display\">\nz=\\omega^T\\textbf{x}+b</script><p>则回归模型的对数似然为</p>\n<script type=\"math/tex; mode=display\">\nL(\\omega,b;(X,D))=\\sum_{i=1}^mP(d_i|\\omega,b;\\textbf{x}_i)</script><p>其中$P(d_i|\\omega,b;\\textbf{x}_i)$可以根据Sigmoid函数求得。</p>\n<script type=\"math/tex; mode=display\">\nP(d_i|\\omega,b;\\textbf{x}_i)=d_iy_i+(1-d_i)(1-y_i)</script><p>最大化对数似然，求得回归系数$\\omega,b$。</p>\n</li>\n</ul>\n<h3 id=\"Naive-Bayes-朴素贝叶斯\"><a href=\"#Naive-Bayes-朴素贝叶斯\" class=\"headerlink\" title=\"Naive Bayes (朴素贝叶斯)\"></a>Naive Bayes (朴素贝叶斯)</h3><ul>\n<li><h4 id=\"Bayes-Rule（贝叶斯规则）\"><a href=\"#Bayes-Rule（贝叶斯规则）\" class=\"headerlink\" title=\"Bayes Rule（贝叶斯规则）\"></a>Bayes Rule（贝叶斯规则）</h4><script type=\"math/tex; mode=display\">\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}</script><p>分子部分P(B|A)，P(A)为先验概率，要求的P(A|B)是已知B发生的情况下A的后验概率。</p>\n</li>\n<li><h4 id=\"贝叶斯规则用于分类\"><a href=\"#贝叶斯规则用于分类\" class=\"headerlink\" title=\"贝叶斯规则用于分类\"></a>贝叶斯规则用于分类</h4><p>从训练样本中估计先验概率，利用贝叶斯规则对测试样本分类，这种分类方法被称为贝叶斯分类器（Bayse Classifier）。</p>\n<p>贝叶斯公式运用到分类时具体为</p>\n<script type=\"math/tex; mode=display\">\nP(label|features)=\\frac{P(features|label)P(label)}{P(features)}</script><p>这里label是我们想要的分类结果，features是样本所具有的一些特征。$P(label)$是先验概率，$P(features|label)$是label条件下的类条件概率（class-conditional probability），$P(features)$用于归一化，与分类标签无关。</p>\n<p>贝叶斯分类的关键是如何从训练数据中估计$P(label)$和$P(features|label)$。</p>\n<p>对于$P(label)$，可以简单地根据大数定律估计：</p>\n<script type=\"math/tex; mode=display\">\nP(label)=\\frac{D_{label}}{D}</script><p>其中$D$是总的样本数，$D_{label}$是分类为label的样本数。</p>\n<p>然而要估计$P(features|label)$却没那么简单，这是所有特征的<strong>联合概率</strong>。而特征往往有很多个，假设每个特征都是二值的，总共有d个特征，则对于一个样本来说它的特征就有$2^d$个可能取值。不幸的的是，样本数往往比$2^d$要小，也就意味着有很多的可能取值会因为样本数不足而没有机会出现。所以，以大数定律粗暴估计就不可取。</p>\n<p>解决办法有两个，一个是参数估计，另外一个是假设特征条件独立。</p>\n<ul>\n<li><h5 id=\"参数估计（极大似然估计）\"><a href=\"#参数估计（极大似然估计）\" class=\"headerlink\" title=\"参数估计（极大似然估计）\"></a>参数估计（极大似然估计）</h5></li>\n</ul>\n<p>先简单了解一下什么是似然（Likelihood)。似然和概率可以说是一对互为逆反的双生子。一般情况下，我们知道一个概率分布，去估计某一具体事件发生的可能性，这时用到的是概率；但还有些时候，我们知道的是一些事件的结果，反过来要去推断概率分布的参数，这时用到的就是似然。</p>\n<p>n个独立同分布的样本$X(x_1,x_2…x_n)$，其似然是</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta;X)=\\prod_{i=1}^nP(x_i|\\theta)</script><p>在对$P(features|label)$的估计中，我们先假定样本集$D_{label}$具有某种概率分布，然后令该分布的参数之似然函数（或对数似然）最大，得到的就是该种分布其参数的最优估计。这种方法称为极大似然估计。</p>\n<ul>\n<li><h5 id=\"朴素贝叶斯分类\"><a href=\"#朴素贝叶斯分类\" class=\"headerlink\" title=\"朴素贝叶斯分类\"></a>朴素贝叶斯分类</h5></li>\n</ul>\n<p>另外一种克服直接估计$P(features|label)$困难的方法是，假设每种特征对分类的影响是独立的，即“属性条件独立性假设”（attribute conditional independence assumption）。有了这个假设，$P(features|label)$就可写开成为：</p>\n<script type=\"math/tex; mode=display\">\nP(features|label)=\\prod_{j=1}^dP(f_j|label)</script><p>其中$features(f_1,f_2…f_d)$有d个。这时，每个$P(f_j|label)$就可以用频率来估计了：</p>\n<script type=\"math/tex; mode=display\">\nP(f_j|label)=\\frac{D^{f_j}_{label}}{D_{label}}</script><p>最早朴素贝叶斯算法被用于语词分类，它无法顾及语词（每个语词是一个feature）的顺序和连用，就是因为假设每个特征之间是独立的。</p>\n</li>\n</ul>\n<h3 id=\"Surpport-Vector-Machine（支持向量机）\"><a href=\"#Surpport-Vector-Machine（支持向量机）\" class=\"headerlink\" title=\"Surpport Vector Machine（支持向量机）\"></a>Surpport Vector Machine（支持向量机）</h3><ul>\n<li><h4 id=\"SVM原理\"><a href=\"#SVM原理\" class=\"headerlink\" title=\"SVM原理\"></a>SVM原理</h4><p>找到一个超平面（hyperplane）或决策边界（decision boundary），能将样本最优分类。距离超平面最近的样本点被称为“支持向量（Support Vector）”，两个异类支持向量到超平面的距离之和称为<strong>间隔（margin）</strong>。SVM希望令间隔最大，这可以转化为一个有不等式约束条件的极值问题，用拉格朗日乘子法解决（KKT条件）。</p>\n<p><img src=\"/images/截屏2020-08-27 下午1.51.54.png\" alt=\"截屏2020-08-27 下午1.51.54\" style=\"zoom:80%;\" /></p>\n</li>\n<li><h4 id=\"SVM用于分类\"><a href=\"#SVM用于分类\" class=\"headerlink\" title=\"SVM用于分类\"></a>SVM用于分类</h4><ul>\n<li><h5 id=\"硬间隔与软间隔，MMC（Maximum-Margin-Classifier）与SVC（Support-Vector-Classifier）\"><a href=\"#硬间隔与软间隔，MMC（Maximum-Margin-Classifier）与SVC（Support-Vector-Classifier）\" class=\"headerlink\" title=\"硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）\"></a>硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）</h5></li>\n</ul>\n<p>在认识什么是SVM之前，先了解一下两个概念：MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier），它们是SVM的基础。</p>\n<p>在SVM思想刚刚提出的时候，把所有样本分类<strong>正确</strong>是必须的要求，其次才是使得<strong>间隔最大</strong>。这种分类器称为Maximum Margin Classifier（MMC）。这样严格的要求限制了其应用的场景，只有非常理想的线性可分的数据集才能使用。当找不到一个超平面划分数据集时，MMC认为出现了异常值（outlier)。</p>\n<p><img src=\"/images/截屏2020-08-27 下午2.55.10.png\" alt=\"截屏2020-08-27 下午2.55.10\" style=\"zoom:80%;\" /></p>\n<p>SVC则可以容忍个别异常值，允许有个别样本点在分类边界的错误一侧。区别于严格将所有样本分类正确，这时的间隔称为<strong>软间隔（soft margin）</strong>，前者当然就是<strong>硬间隔（hard margin）</strong>。简单来说是在最小化目标函数时，额外考虑边界将样本分类错误的惩罚，在原来的严格的不等式约束前加一个惩罚系数C，C越大，对样本分类正确性的要求就越严格。这样就引入了一些“弹性”，从“硬间隔”变成了“软间隔”。</p>\n<ul>\n<li><h5 id=\"核技巧\"><a href=\"#核技巧\" class=\"headerlink\" title=\"核技巧\"></a>核技巧</h5></li>\n</ul>\n<p>当在原始输入的特征空间内不能找到一个线性的超平面来分割时，用到<strong>核技巧（kernel trick）</strong>，把原始低维的特征映射到高维，找到一个线性超平面，再在原始特征空间表示出来。用一个最简单的例子来直观感受一下：</p>\n<p><img src=\"/images/IMG_0726.JPG\" alt=\"IMG_0726\" style=\"zoom:40%;\" /></p>\n<p>在高维的特征空间用拉格朗日乘子法求解问题时会遇到一个困难。在原始特征空间内，不等式约束的极值问题转化为拉格朗日函数的对偶问题后，约化后求解需要计算$x_i^Tx_j^T$，而转化到高维特征空间，这个式子就变策成了$\\phi(x_i)^T\\phi(x_j)$，其中$\\phi(x)$是$x$映射到高维特征空间的向量。当$\\phi(x)$所在空间的维数很高时，不可能对$\\phi(x_i)^T\\phi(x_j)$直接计算。但是假如我们知道一个函数$\\kappa(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$，就可以直接在原始特征空间计算高维特征空间的内积。这个$\\kappa(x_i,x_j)$函数就称为<strong>核函数（kernel function）</strong>。</p>\n<p>实际中，我们不是先知，核函数的具体形式我们是不知道的，只能靠猜。所以核函数的选择是SVM一个非常大的不确定性因素。</p>\n<p>SVM可以用于多分类吗？</p>\n</li>\n<li><h4 id=\"SVM用于回归\"><a href=\"#SVM用于回归\" class=\"headerlink\" title=\"SVM用于回归\"></a>SVM用于回归</h4><p>SVM也可以用来处理回归问题。传统的回归问题是最小化模型输出$f(x)$样本真实值$y$之间的差距。SVM回归的区别在于它可以容忍$f(x)$和$y$之间有$\\epsilon$的偏差，小于$\\epsilon$的偏差忽略不计入损失。相当于以$f(x)$为中心，构建了一个宽度为$2\\epsilon$的间隔带，落入此间隔带的样本都被认为是预测正确的。</p>\n</li>\n</ul>\n<h3 id=\"Decision-Trees（决策树）\"><a href=\"#Decision-Trees（决策树）\" class=\"headerlink\" title=\"Decision Trees（决策树）\"></a>Decision Trees（决策树）</h3><p>面临一个多元线性问题时，决策树可以找到一个“锯齿状”的边界。这是决策树边界的一个最明显的特征。决策树算法就是用计算机找到最合适的这种边界。它从训练集中得到一个树状模型，包含一个根节点，若干内部节点和叶节点。每个分支节点都是一次决策，这样一系列的决策决定了我们的最终分类判断。</p>\n<p><img src=\"/images/IMG_0727.JPG\" alt=\"IMG_0727\" style=\"zoom:40%;\" /></p>\n<ul>\n<li><h4 id=\"Entropy（熵）\"><a href=\"#Entropy（熵）\" class=\"headerlink\" title=\"Entropy（熵）\"></a>Entropy（熵）</h4><p><strong>熵（Entropy）</strong>用来衡量一类中样本的杂质含量（impurity）。公式为：</p>\n<script type=\"math/tex; mode=display\">\nEntropy(D) = \\sum\\limits_{i}^{|\\textbf{y}|}{-P_i}log_2P_i</script><p>其中$P_i$是当前样本集D中第i类的样本所占的比例，$|\\textbf{y}|$是类别数。当所有样本均为同一类时，纯度最高，Entropy=0；当各类样本在样本集中均匀分布时，杂质含量最高，$Entropy = log_2|\\textbf{y}|$。</p>\n<p>一种更本质的理解是，熵是不确定性的度量，写成$Entropy = \\sum\\limits_{i}{P_i}log_2\\frac{1}{P_i}$，$log_2\\frac{1}{P_i}$是第i类的不确定程度/混乱程度。机器学习算法想要从海量<strong>数据</strong>（经验）中获得确定性的<strong>信息</strong>（模型），就如同沙里淘金。当沙子和金子混在一起时，纯度很低，熵很高；当沙子和金子清楚地分成两堆，这时纯度就很高，熵就很低。纯度很高的金子就是我们想要的确定性的信息，沙子则是<strong>噪音</strong>。</p>\n</li>\n<li><h4 id=\"Information-gain（信息增益）\"><a href=\"#Information-gain（信息增益）\" class=\"headerlink\" title=\"Information gain（信息增益）\"></a>Information gain（信息增益）</h4><p>如何把金子从沙堆里淘出来呢？现在我们有了评价金子纯度的方法——熵，要想办法让熵越小越好，这样金子的纯度就越高。决策树就是依次选择最优的特征进行划分，能让熵减小得越多的划分方法，就是越好的划分。<strong>信息增益（information gain）</strong>就是评价“熵减小”量的一个指标。</p>\n<p>决策树划定决策边界的依据，也就是决策树算法的核心，是要<strong>最大化信息增益</strong>。用特征a对样本集D划分的信息增益为：</p>\n<script type=\"math/tex; mode=display\">\ninformation\\ gain(D,a)=Entropy_{parent}(D)-\\sum\\limits_{v}^{V}\\frac{|D^v|}{|D|}Entropy_{children}(D^v)</script><p>其中特征a共有V个可能取值，取值为$a^v$的样本子集包含的样本数为$|D^v|$。在上一层划分的基础上，如何决定下一个分支节点用哪个特征进行划分，需要用到信息增益。举个例子：</p>\n<p><img src=\"/images/IMG_0728.PNG\" alt=\"IMG_0728\" style=\"zoom:40%;\" /></p>\n<p>假如我们想把样本集分为两类：蓝圈和红叉。根节点是样本全集。计算出根节点的信息熵为1。下面要决定下一个分支节点选用哪个特征进行划分，这里拿特征$x&gt;1$和$x&gt;2$举例。比较信息增益得知，前者更适合作为下一个分支节点。</p>\n<p>以信息增益作为判别标准的算法称为ID3（Iterative Dichotomiser）。信息增益有一个缺点，它对可能取值数多的特征有偏好。同样是在各特征取值均匀分布情况，特征可能取值数多的，信息增益会更大。为了克服这个缺点，改进的C4.5算法是以<strong>增益率</strong>作为标准。后来的CART决策树是以<strong>基尼指数（Gini index）</strong>作为标准。</p>\n</li>\n</ul>\n<h3 id=\"K-Nearest-Neighbors（K最邻近）\"><a href=\"#K-Nearest-Neighbors（K最邻近）\" class=\"headerlink\" title=\"K-Nearest Neighbors（K最邻近）\"></a>K-Nearest Neighbors（K最邻近）</h3><h3 id=\"Random-Forest（随机森林）\"><a href=\"#Random-Forest（随机森林）\" class=\"headerlink\" title=\"Random Forest（随机森林）\"></a>Random Forest（随机森林）</h3><h3 id=\"Ada-Boost\"><a href=\"#Ada-Boost\" class=\"headerlink\" title=\"Ada Boost\"></a>Ada Boost</h3><p><strong>选择算法</strong></p>\n<p>1）理解算法原理，适合哪种数据</p>\n<p>2）用测试集检验算法的表现</p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\" target=\"_blank\" rel=\"noopener\">[机器学习入门]-udacity</a></em></small></p>\n<p><small><em><a href=\"\">[机器学习西瓜书]-周志华</a></em></small></p>\n<p><small><em><a href=\"https://github.com/datawhalechina/team-learning-data-mining/tree/master/IntroductionExperienceAI\" target=\"_blank\" rel=\"noopener\">[机器学习算法（AI入门体验）开源学习资料]-Datawhale</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/pinard/p/6050306.html\" target=\"_blank\" rel=\"noopener\">[决策树算法原理]-刘建平</a></em></small></p>\n<p><small><em><a href=\"https://medium.com/analytics-vidhya/road-to-svm-maximal-margin-classifier-and-support-vector-classifier-85cb1e3dcc0a\" target=\"_blank\" rel=\"noopener\">[Road to SVM: Maximal Margin Classifier and Support Vector Classifier]-Valentina ALto</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>Logistic Regression，Naive Bayes，SVM，Decision Trees，KNN，Random Forest，Ada Boost","more":"</p>\n<p>不涉及一些方法背后繁复的数学推导，只求列出要领。</p>\n<h3 id=\"Logistic-Regression（逻辑回归-对数几率回归）\"><a href=\"#Logistic-Regression（逻辑回归-对数几率回归）\" class=\"headerlink\" title=\"Logistic Regression（逻辑回归/对数几率回归）\"></a>Logistic Regression（逻辑回归/对数几率回归）</h3><p>一般的线性回归模型形式为</p>\n<script type=\"math/tex; mode=display\">\nf(\\textbf{x})=\\omega^T\\textbf{x}+b</script><p>其输出$f(\\textbf{x})$的值域是一个连续的实数域。但如果我们想要预测的是一个事件发生与否，比如基于体检指标预测是否患癌症，根据大气温度、湿度、风力和压强等预测是否会下雨，这时的因变量就是一个离散的布尔变量，$f(\\textbf{x})\\in \\{0,1\\}$。</p>\n<ul>\n<li><h4 id=\"用于分类的逻辑回归\"><a href=\"#用于分类的逻辑回归\" class=\"headerlink\" title=\"用于分类的逻辑回归\"></a>用于分类的逻辑回归</h4><p>这种类型的问题可以归纳为二分类问题。从一般的线性回归模型到二分类模型，我们需要一个函数，能够把因变量从连续的实数值域映射到{0,1}值域。阶跃函数正是这样一种函数，比如，我们可以规定</p>\n<script type=\"math/tex; mode=display\">\ny=\\left\\{\n             \\begin{array}{lr}\n             0, \\ f(\\textbf{x})<0 &  \\\\\n             1, \\ f(\\textbf{x})\\geq0 & \n             \\end{array}\n\\right.</script><p>对于原函数$f(\\textbf{x})$，我们还可以稍作变换，找到一个跟阶跃函数很像的”连续版本“——Sigmoid函数</p>\n<script type=\"math/tex; mode=display\">\ny=\\frac{1}{1+e^{-z}}</script><p>这里$z=f(\\textbf{x})$。Sigmoid函数的图像为</p>\n<p><img src=\"/images/截屏2020-08-17 下午11.04.31.png\" alt=\"截屏2020-08-17 下午11.04.31\" style=\"zoom:50%;\" /></p>\n<p>可以看出，其值域为(0,1)，而且在y=0.5附近很陡峭。正是得益于此特性，某种程度上，Sigmoid函数的y<strong>可以看作事件发生的概率</strong>。当$y&gt; 0.5$时，认为事件发生概率大于不发生的概率，分类为1；$y&lt;0.$5时，反之，分类为0（这也对应一个阶跃函数）。</p>\n<p>Sigmoid的反函数为：</p>\n<script type=\"math/tex; mode=display\">\nz = ln\\frac{y}{1-y}</script><p>通常把$\\frac{y}{1-y}$称为“<strong>胜算</strong>”或“<strong>几率</strong>”。</p>\n</li>\n<li><h4 id=\"求逻辑回归的回归系数\"><a href=\"#求逻辑回归的回归系数\" class=\"headerlink\" title=\"求逻辑回归的回归系数\"></a>求逻辑回归的回归系数</h4><p>逻辑回归的回归系数用极大似然法求解，似然的概念在朴素贝叶斯方法里有简要介绍。</p>\n<p>假设我们的训练集为$\\{(\\textbf{x}_1,d_1),(\\textbf{x}_2,d_2)…(\\textbf{x}_m,d_m)\\}$，为了区分二值的分类结果和Sigmoid函数的事件“概率”，这里分类结果记作$d_i$。回归模型为</p>\n<script type=\"math/tex; mode=display\">\nz=\\omega^T\\textbf{x}+b</script><p>则回归模型的对数似然为</p>\n<script type=\"math/tex; mode=display\">\nL(\\omega,b;(X,D))=\\sum_{i=1}^mP(d_i|\\omega,b;\\textbf{x}_i)</script><p>其中$P(d_i|\\omega,b;\\textbf{x}_i)$可以根据Sigmoid函数求得。</p>\n<script type=\"math/tex; mode=display\">\nP(d_i|\\omega,b;\\textbf{x}_i)=d_iy_i+(1-d_i)(1-y_i)</script><p>最大化对数似然，求得回归系数$\\omega,b$。</p>\n</li>\n</ul>\n<h3 id=\"Naive-Bayes-朴素贝叶斯\"><a href=\"#Naive-Bayes-朴素贝叶斯\" class=\"headerlink\" title=\"Naive Bayes (朴素贝叶斯)\"></a>Naive Bayes (朴素贝叶斯)</h3><ul>\n<li><h4 id=\"Bayes-Rule（贝叶斯规则）\"><a href=\"#Bayes-Rule（贝叶斯规则）\" class=\"headerlink\" title=\"Bayes Rule（贝叶斯规则）\"></a>Bayes Rule（贝叶斯规则）</h4><script type=\"math/tex; mode=display\">\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}</script><p>分子部分P(B|A)，P(A)为先验概率，要求的P(A|B)是已知B发生的情况下A的后验概率。</p>\n</li>\n<li><h4 id=\"贝叶斯规则用于分类\"><a href=\"#贝叶斯规则用于分类\" class=\"headerlink\" title=\"贝叶斯规则用于分类\"></a>贝叶斯规则用于分类</h4><p>从训练样本中估计先验概率，利用贝叶斯规则对测试样本分类，这种分类方法被称为贝叶斯分类器（Bayse Classifier）。</p>\n<p>贝叶斯公式运用到分类时具体为</p>\n<script type=\"math/tex; mode=display\">\nP(label|features)=\\frac{P(features|label)P(label)}{P(features)}</script><p>这里label是我们想要的分类结果，features是样本所具有的一些特征。$P(label)$是先验概率，$P(features|label)$是label条件下的类条件概率（class-conditional probability），$P(features)$用于归一化，与分类标签无关。</p>\n<p>贝叶斯分类的关键是如何从训练数据中估计$P(label)$和$P(features|label)$。</p>\n<p>对于$P(label)$，可以简单地根据大数定律估计：</p>\n<script type=\"math/tex; mode=display\">\nP(label)=\\frac{D_{label}}{D}</script><p>其中$D$是总的样本数，$D_{label}$是分类为label的样本数。</p>\n<p>然而要估计$P(features|label)$却没那么简单，这是所有特征的<strong>联合概率</strong>。而特征往往有很多个，假设每个特征都是二值的，总共有d个特征，则对于一个样本来说它的特征就有$2^d$个可能取值。不幸的的是，样本数往往比$2^d$要小，也就意味着有很多的可能取值会因为样本数不足而没有机会出现。所以，以大数定律粗暴估计就不可取。</p>\n<p>解决办法有两个，一个是参数估计，另外一个是假设特征条件独立。</p>\n<ul>\n<li><h5 id=\"参数估计（极大似然估计）\"><a href=\"#参数估计（极大似然估计）\" class=\"headerlink\" title=\"参数估计（极大似然估计）\"></a>参数估计（极大似然估计）</h5></li>\n</ul>\n<p>先简单了解一下什么是似然（Likelihood)。似然和概率可以说是一对互为逆反的双生子。一般情况下，我们知道一个概率分布，去估计某一具体事件发生的可能性，这时用到的是概率；但还有些时候，我们知道的是一些事件的结果，反过来要去推断概率分布的参数，这时用到的就是似然。</p>\n<p>n个独立同分布的样本$X(x_1,x_2…x_n)$，其似然是</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta;X)=\\prod_{i=1}^nP(x_i|\\theta)</script><p>在对$P(features|label)$的估计中，我们先假定样本集$D_{label}$具有某种概率分布，然后令该分布的参数之似然函数（或对数似然）最大，得到的就是该种分布其参数的最优估计。这种方法称为极大似然估计。</p>\n<ul>\n<li><h5 id=\"朴素贝叶斯分类\"><a href=\"#朴素贝叶斯分类\" class=\"headerlink\" title=\"朴素贝叶斯分类\"></a>朴素贝叶斯分类</h5></li>\n</ul>\n<p>另外一种克服直接估计$P(features|label)$困难的方法是，假设每种特征对分类的影响是独立的，即“属性条件独立性假设”（attribute conditional independence assumption）。有了这个假设，$P(features|label)$就可写开成为：</p>\n<script type=\"math/tex; mode=display\">\nP(features|label)=\\prod_{j=1}^dP(f_j|label)</script><p>其中$features(f_1,f_2…f_d)$有d个。这时，每个$P(f_j|label)$就可以用频率来估计了：</p>\n<script type=\"math/tex; mode=display\">\nP(f_j|label)=\\frac{D^{f_j}_{label}}{D_{label}}</script><p>最早朴素贝叶斯算法被用于语词分类，它无法顾及语词（每个语词是一个feature）的顺序和连用，就是因为假设每个特征之间是独立的。</p>\n</li>\n</ul>\n<h3 id=\"Surpport-Vector-Machine（支持向量机）\"><a href=\"#Surpport-Vector-Machine（支持向量机）\" class=\"headerlink\" title=\"Surpport Vector Machine（支持向量机）\"></a>Surpport Vector Machine（支持向量机）</h3><ul>\n<li><h4 id=\"SVM原理\"><a href=\"#SVM原理\" class=\"headerlink\" title=\"SVM原理\"></a>SVM原理</h4><p>找到一个超平面（hyperplane）或决策边界（decision boundary），能将样本最优分类。距离超平面最近的样本点被称为“支持向量（Support Vector）”，两个异类支持向量到超平面的距离之和称为<strong>间隔（margin）</strong>。SVM希望令间隔最大，这可以转化为一个有不等式约束条件的极值问题，用拉格朗日乘子法解决（KKT条件）。</p>\n<p><img src=\"/images/截屏2020-08-27 下午1.51.54.png\" alt=\"截屏2020-08-27 下午1.51.54\" style=\"zoom:80%;\" /></p>\n</li>\n<li><h4 id=\"SVM用于分类\"><a href=\"#SVM用于分类\" class=\"headerlink\" title=\"SVM用于分类\"></a>SVM用于分类</h4><ul>\n<li><h5 id=\"硬间隔与软间隔，MMC（Maximum-Margin-Classifier）与SVC（Support-Vector-Classifier）\"><a href=\"#硬间隔与软间隔，MMC（Maximum-Margin-Classifier）与SVC（Support-Vector-Classifier）\" class=\"headerlink\" title=\"硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）\"></a>硬间隔与软间隔，MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier）</h5></li>\n</ul>\n<p>在认识什么是SVM之前，先了解一下两个概念：MMC（Maximum Margin Classifier）与SVC（Support Vector Classifier），它们是SVM的基础。</p>\n<p>在SVM思想刚刚提出的时候，把所有样本分类<strong>正确</strong>是必须的要求，其次才是使得<strong>间隔最大</strong>。这种分类器称为Maximum Margin Classifier（MMC）。这样严格的要求限制了其应用的场景，只有非常理想的线性可分的数据集才能使用。当找不到一个超平面划分数据集时，MMC认为出现了异常值（outlier)。</p>\n<p><img src=\"/images/截屏2020-08-27 下午2.55.10.png\" alt=\"截屏2020-08-27 下午2.55.10\" style=\"zoom:80%;\" /></p>\n<p>SVC则可以容忍个别异常值，允许有个别样本点在分类边界的错误一侧。区别于严格将所有样本分类正确，这时的间隔称为<strong>软间隔（soft margin）</strong>，前者当然就是<strong>硬间隔（hard margin）</strong>。简单来说是在最小化目标函数时，额外考虑边界将样本分类错误的惩罚，在原来的严格的不等式约束前加一个惩罚系数C，C越大，对样本分类正确性的要求就越严格。这样就引入了一些“弹性”，从“硬间隔”变成了“软间隔”。</p>\n<ul>\n<li><h5 id=\"核技巧\"><a href=\"#核技巧\" class=\"headerlink\" title=\"核技巧\"></a>核技巧</h5></li>\n</ul>\n<p>当在原始输入的特征空间内不能找到一个线性的超平面来分割时，用到<strong>核技巧（kernel trick）</strong>，把原始低维的特征映射到高维，找到一个线性超平面，再在原始特征空间表示出来。用一个最简单的例子来直观感受一下：</p>\n<p><img src=\"/images/IMG_0726.JPG\" alt=\"IMG_0726\" style=\"zoom:40%;\" /></p>\n<p>在高维的特征空间用拉格朗日乘子法求解问题时会遇到一个困难。在原始特征空间内，不等式约束的极值问题转化为拉格朗日函数的对偶问题后，约化后求解需要计算$x_i^Tx_j^T$，而转化到高维特征空间，这个式子就变策成了$\\phi(x_i)^T\\phi(x_j)$，其中$\\phi(x)$是$x$映射到高维特征空间的向量。当$\\phi(x)$所在空间的维数很高时，不可能对$\\phi(x_i)^T\\phi(x_j)$直接计算。但是假如我们知道一个函数$\\kappa(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$，就可以直接在原始特征空间计算高维特征空间的内积。这个$\\kappa(x_i,x_j)$函数就称为<strong>核函数（kernel function）</strong>。</p>\n<p>实际中，我们不是先知，核函数的具体形式我们是不知道的，只能靠猜。所以核函数的选择是SVM一个非常大的不确定性因素。</p>\n<p>SVM可以用于多分类吗？</p>\n</li>\n<li><h4 id=\"SVM用于回归\"><a href=\"#SVM用于回归\" class=\"headerlink\" title=\"SVM用于回归\"></a>SVM用于回归</h4><p>SVM也可以用来处理回归问题。传统的回归问题是最小化模型输出$f(x)$样本真实值$y$之间的差距。SVM回归的区别在于它可以容忍$f(x)$和$y$之间有$\\epsilon$的偏差，小于$\\epsilon$的偏差忽略不计入损失。相当于以$f(x)$为中心，构建了一个宽度为$2\\epsilon$的间隔带，落入此间隔带的样本都被认为是预测正确的。</p>\n</li>\n</ul>\n<h3 id=\"Decision-Trees（决策树）\"><a href=\"#Decision-Trees（决策树）\" class=\"headerlink\" title=\"Decision Trees（决策树）\"></a>Decision Trees（决策树）</h3><p>面临一个多元线性问题时，决策树可以找到一个“锯齿状”的边界。这是决策树边界的一个最明显的特征。决策树算法就是用计算机找到最合适的这种边界。它从训练集中得到一个树状模型，包含一个根节点，若干内部节点和叶节点。每个分支节点都是一次决策，这样一系列的决策决定了我们的最终分类判断。</p>\n<p><img src=\"/images/IMG_0727.JPG\" alt=\"IMG_0727\" style=\"zoom:40%;\" /></p>\n<ul>\n<li><h4 id=\"Entropy（熵）\"><a href=\"#Entropy（熵）\" class=\"headerlink\" title=\"Entropy（熵）\"></a>Entropy（熵）</h4><p><strong>熵（Entropy）</strong>用来衡量一类中样本的杂质含量（impurity）。公式为：</p>\n<script type=\"math/tex; mode=display\">\nEntropy(D) = \\sum\\limits_{i}^{|\\textbf{y}|}{-P_i}log_2P_i</script><p>其中$P_i$是当前样本集D中第i类的样本所占的比例，$|\\textbf{y}|$是类别数。当所有样本均为同一类时，纯度最高，Entropy=0；当各类样本在样本集中均匀分布时，杂质含量最高，$Entropy = log_2|\\textbf{y}|$。</p>\n<p>一种更本质的理解是，熵是不确定性的度量，写成$Entropy = \\sum\\limits_{i}{P_i}log_2\\frac{1}{P_i}$，$log_2\\frac{1}{P_i}$是第i类的不确定程度/混乱程度。机器学习算法想要从海量<strong>数据</strong>（经验）中获得确定性的<strong>信息</strong>（模型），就如同沙里淘金。当沙子和金子混在一起时，纯度很低，熵很高；当沙子和金子清楚地分成两堆，这时纯度就很高，熵就很低。纯度很高的金子就是我们想要的确定性的信息，沙子则是<strong>噪音</strong>。</p>\n</li>\n<li><h4 id=\"Information-gain（信息增益）\"><a href=\"#Information-gain（信息增益）\" class=\"headerlink\" title=\"Information gain（信息增益）\"></a>Information gain（信息增益）</h4><p>如何把金子从沙堆里淘出来呢？现在我们有了评价金子纯度的方法——熵，要想办法让熵越小越好，这样金子的纯度就越高。决策树就是依次选择最优的特征进行划分，能让熵减小得越多的划分方法，就是越好的划分。<strong>信息增益（information gain）</strong>就是评价“熵减小”量的一个指标。</p>\n<p>决策树划定决策边界的依据，也就是决策树算法的核心，是要<strong>最大化信息增益</strong>。用特征a对样本集D划分的信息增益为：</p>\n<script type=\"math/tex; mode=display\">\ninformation\\ gain(D,a)=Entropy_{parent}(D)-\\sum\\limits_{v}^{V}\\frac{|D^v|}{|D|}Entropy_{children}(D^v)</script><p>其中特征a共有V个可能取值，取值为$a^v$的样本子集包含的样本数为$|D^v|$。在上一层划分的基础上，如何决定下一个分支节点用哪个特征进行划分，需要用到信息增益。举个例子：</p>\n<p><img src=\"/images/IMG_0728.PNG\" alt=\"IMG_0728\" style=\"zoom:40%;\" /></p>\n<p>假如我们想把样本集分为两类：蓝圈和红叉。根节点是样本全集。计算出根节点的信息熵为1。下面要决定下一个分支节点选用哪个特征进行划分，这里拿特征$x&gt;1$和$x&gt;2$举例。比较信息增益得知，前者更适合作为下一个分支节点。</p>\n<p>以信息增益作为判别标准的算法称为ID3（Iterative Dichotomiser）。信息增益有一个缺点，它对可能取值数多的特征有偏好。同样是在各特征取值均匀分布情况，特征可能取值数多的，信息增益会更大。为了克服这个缺点，改进的C4.5算法是以<strong>增益率</strong>作为标准。后来的CART决策树是以<strong>基尼指数（Gini index）</strong>作为标准。</p>\n</li>\n</ul>\n<h3 id=\"K-Nearest-Neighbors（K最邻近）\"><a href=\"#K-Nearest-Neighbors（K最邻近）\" class=\"headerlink\" title=\"K-Nearest Neighbors（K最邻近）\"></a>K-Nearest Neighbors（K最邻近）</h3><h3 id=\"Random-Forest（随机森林）\"><a href=\"#Random-Forest（随机森林）\" class=\"headerlink\" title=\"Random Forest（随机森林）\"></a>Random Forest（随机森林）</h3><h3 id=\"Ada-Boost\"><a href=\"#Ada-Boost\" class=\"headerlink\" title=\"Ada Boost\"></a>Ada Boost</h3><p><strong>选择算法</strong></p>\n<p>1）理解算法原理，适合哪种数据</p>\n<p>2）用测试集检验算法的表现</p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\" target=\"_blank\" rel=\"noopener\">[机器学习入门]-udacity</a></em></small></p>\n<p><small><em><a href=\"\">[机器学习西瓜书]-周志华</a></em></small></p>\n<p><small><em><a href=\"https://github.com/datawhalechina/team-learning-data-mining/tree/master/IntroductionExperienceAI\" target=\"_blank\" rel=\"noopener\">[机器学习算法（AI入门体验）开源学习资料]-Datawhale</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/pinard/p/6050306.html\" target=\"_blank\" rel=\"noopener\">[决策树算法原理]-刘建平</a></em></small></p>\n<p><small><em><a href=\"https://medium.com/analytics-vidhya/road-to-svm-maximal-margin-classifier-and-support-vector-classifier-85cb1e3dcc0a\" target=\"_blank\" rel=\"noopener\">[Road to SVM: Maximal Margin Classifier and Support Vector Classifier]-Valentina ALto</a></em></small></p>"},{"title":"Python 基础（二）","date":"2020-02-12T14:22:00.000Z","mathjax":true,"_content":"\npython 程序控制结构：选择、循环；python 函数；python基本输入输出（IO）<!--more-->\n\n\n\n---\n\n### Python 控制结构\n\n1. #### 选择 if-elif-else\n\n   ```python\n   >>>Parker = \"plane\"\n   >>>if Parker == \"car\":\n          charge = \"beatles\"\n      elif Parker == \"motor\":\n          charge = \"helmet\"\n      else:\n          charge = \"wings\"\n   >>>print (charge)\n   \n   wings\n   ```\n\n   三元运算符\n\n   ```python\n   t = x if x>=y else y\n   # 等价于\n   if x>=y：\n       t = x\n   else：\n       t = y\n   ```\n\n2. #### 循环\n\n   * ##### WHILE循环\n\n     ```python\n     while i <= 100:\n         i += 1\n     print (i)\n     ```\n\n   * ##### FOR循环\n\n     ```python\n     >>>blog = [\"what\", \"the\", \"HELL\", \"is\", \"this\", \"blog\", \"writing\"]\n     \n     # 序列项迭代\n     >>>for word in blog:\n            print (word, end = \" \")\n   \n     what the HELL is this blog writing \n   \n     # 序列索引迭代\n     >>>namelist = [('wang',16),('miao',60),('zhao',34)]\n     >>>dic = {}\n     >>>for i in range(len(namelist)):\n            name = namelist[i][0]\n            age = namelist[i][1]\n            dic[name] = age\n     >>>print(dic)\n   \n     {'wang': 16, 'miao': 60, 'zhao': 34}\n     ```\n   \n     **zip()迭代器迭代**\n   \n     ```python\n     zip()函数从多个列表中每次取一个元素\n     >>>blog = [\"B\", \"U\", \"L\", \"L\", \"S\", \"H\", \"I\",'T']\n     >>>ind = range(8)\n     >>>for word,num in zip(blog,ind):\n            print ('{0}-{1}'.format(word,num))\n        \n     B-0\n     U-1\n     L-2\n     L-3\n     S-4\n     H-5\n     I-6\n     T-7\n     ```\n   \n     **enumerate()同时得到下标和元素**\n\n     ```python\n     for (ind,word) in enumerate(blog):\n         print ind\n         print word\n     ```\n\n   * ##### 循环中的ELSE语句\n   \n     - 若从break跳出循环，不执行ELSE中的语句\n     \n     - 若正常循环结束，执行ELSE中的语句\n     \n       ```python\n       apples = 5\n       while apples !=0:\n           if apples ==1:\n               print ('only one left, continue?')\n               eat = input()\n            if eat == 'yes':\n                   apples -= 1\n                continue\n               elif eat == 'no':\n                   break\n               else:\n                   print(' ''yes'' or ''no'' ')\n                   continue\n           apples -= 1\n       else:\n           print('apples all eaten')\n       ```\n\n   * ##### 列表解析（list comprehension) -- 轻量级循环\n   \n     ```python\n     >>>print([x**2 for x in range(10) if x**2<50])\n     [0, 1, 4, 9, 16, 25, 36, 49]\n     \n     >>>print([(x,y) for x in range(2) for y in range(2)])\n     [(0, 0), (0, 1), (1, 0), (1, 1)]\n     ```\n     \n   * ##### yield生成器\n     ```python\n     # 当需要每次循环返回一个特定的值时，使用生成器比较方便，生成的是一个可迭代对象\n     def gen():\n      a = 1\n         yield a            # 第一次返回 1\n      a = 'cat'\n         yield a            # 第二次返回 cat\n         yield 99           # 第三次返回 99\n          \n     for i in gen():\n         print(i)\n              \n     # 生成器表达式\n     gen = (x for x in range(4)) # 与列表解析不同的是用()括起来，是可迭代对象\n     ```\n   \n   * ##### 循环对象（迭代器）\n   \n     一个**循环对象**包含next()方法，和StopIteration错误:\n     \n     ```python\n     f = open('test.txt')  # open返回的是一个循环对象\n        \tf.next()              # next每次返回的是下一行的内容\n        \tf.next()\n        \t\t\n     # 当到达结尾时，返回 StopIteration\n     ```\n     \n\n---\n\n### Python 函数\n\n1. #### 自定义函数\n\n   * ##### 一般定义def\n\n     ```python\n     def func(x,y):           # 定义函数func\n         print (x+y)\n     func(1,3)                # 调用函数\n     ```\n\n   * ##### lambda函数定义\n\n     ```python\n     func = lambda x,y: x+y   # 使用lambda函数定义func\n     print (func(1,3))\n     ```\n\n2. #### 不同类型的参数\n\n   * ##### 位置参数\n\n     ```python\n     def func(name,gender):\n         print('{0} is {1}'.format(name,gender))\n     func('Mia','Female')\n     ```\n\n   * ##### 关键字参数\n\n     ```python\n     def func(name,gender):\n         print('{0} is {1}'.format(name,gender))\n     func(gender='Female',name='Mia')  # 使用参数名区分参数，可以任意改变顺序\n     ```\n\n   * ##### 默认参数\n\n     ```python\n     def func(name,gender='Female'):   # 定义函数时可以设定参数默认值\n         print('{0} is {1}'.format(name,gender))\n     func('Mia')                       # 调用时不输入默认参数，则默认参数为默认值\n     func('Mia','Male')                # 默认参数也可以修改\n     ```\n\n   * ##### 可变长参数（包裹）\n\n     ```python\n     def func(name,*manywords):        # 加星号就变成了可变长参数（包裹）\n         print(name,'is',manywords)\n     func('Mia','a','girl','with','blue','eyes')\n     \n     Mia is ('a', 'girl', 'with', 'blue', 'eyes')\n     ```\n\n     ```python\n     def func(name,**manywords):       # 加两个星号变成了可变长关键字参数\n         print(name,manywords)\n     func('Mia', gender='Female', age=20, hair='blonde')\n     \n     Mia {'gender': 'Female', 'age': 20, 'hair': 'blonde'}\n     ```\n\n3. #### 函数可作为参数传递\n\n   ```python\n   def func(name,gender):\n       print('{0} is {1}'.format(name,gender))\n   \n   def test(f,a,b):\n       func(a,b)\n       \n   test(func,'Mia','Female')\n   ```\n\n4. #### 一些方便的函数\n\n   * ##### map()\n\n     ```python\n     # map函数每次从两个表中取出一个元素，带入前方的函数\n     ans = map((lambda x,y: x+y),[1,2,3],[7,8,9]) \n     print(list(ans))                        # 可以用list函数把map返回的循环对象转换为表\n     \n     [8, 10, 12]\n     ```\n\n   * ##### reduce()\n\n     ```python\n     # reduce函数累进地作用于各个参数，先取1，2，运算结果后再向后取3，……\n     ans = reduce((lambda x,y: x+y),[1,2,3,7,8,9]) \n     \n     # 本例实际运算的是(((((1+2)+3)+7)+8)+9)\n     ```\n\n   * ##### filter()\n\n     ```python\n     # filter函数用来筛选数据，仅把函数返回值为True的元素存入表（循环对象）\n     def func(a):\n         if a > 100:\n             return True\n         else:\n             return False\n     \n     ans = filter(func,[10,56,101,500])     \n     ```\n\n\n\n---\n\n### Python基本输入输出（IO）\n\n1. #### File IO\n\n   ```python\n   f = open('pybook','w+')             # 'r'只读，'w'只写，'w+'读写\n   txt = ['Hi!','Welcome to pybook!']\n   for word in txt:\n       f.write(word)\n       f.write('\\n')                   # 在写入txt文件时换行\n   f.close()                           # 记得关闭文件\n   \n   f = open('pybook','r')\n   print(f.readlines())             \n   f.close()\n   ```\n\n   常用的读写方法\n\n   ```python\n   f.read(N)          # 读取Nbytes的数据\n   f.readline()       # 读取一行\n   f.readlines()      # 读取所有行，储存在列表中，每个元素是一行\n   \n   f.write('xxx')             # 写入单个字符串'xxx'\n   f.writelines('aa\\n','bb')  # 当写入多个字符串构成列表(list)时，要用writelines\n   ```\n\n2. #### Interactive IO\n\n   ##### 输入Input()\n\n   ```python\n   >>>a = input(\"input:\")    # input('提示信息')\n   input:123 \n   \n   # 输入数值型\\列表\\字典\n   eval(input())\n   ```\n\n   ##### 输出print()\n\n   ```python\n   print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\n   # objects:输入对象，用,隔开\n   # sep: 打印时填充对象间隔，默认为' '空格\n   # end: 结束方式，\\n换行\n   ```\n\n   %格式化操作符\n\n   ```python\n   # %[(name)][flags][width].[precision]typecode\n   # (name):命名\n   # flags: +(右对齐), -(左对齐), ' '(在正数前填一个空格), '0'(填充0)\n   # width: 字符宽度\n   # precision: 小数点位数\n   # typecode: 数据类型\n   \n   print('%s is %d years old' % ('Hugo', 20))   # %s<--'Hugo', %d<--20 , 模版和填值以 % 隔开\n   \n   print('%(name)s is %(age)d years old' % {'name':'Hugo', 'age':20})   # 也可用字典填值\n   \n   print('%+10.3f,%-10.3f,%+10.3f' % (-13.2, 32, 9.44))\n      -13.200,32.000    ,000009.440\n   ```\n\n   .format()格式化\n\n   ```python\n   print('{0} and {1} and {0}'.format('a', 'b'))  # 可索引\n   a and b and a\n   \n   #{:4f}位数\n   #{>}右对齐\n   print(':{0:>4s} and {1:.3f}'.format('aa', 2.1)) # {0(索引):(位数)>(右对齐)4}\n   :  aa and 2.100\n   ```\n\n   print(f'')\n\n   ```python\n   x=3,y=4\n   print(f'x={x:d},y={y:.3f}')\n   ```\n\n   \n\n<br/><br/>\n\n<small>*参考*</small>\n\n<small>*[Vamei 博客园-Python快速教程](https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html)*</small>\n\n<small>*[RuiWo 博客园-python基础-格式化输出](https://www.cnblogs.com/qinchao0317/p/10699717.html)*</small>\n\n<small>*[Vamei 博客园-Python补充05 字符串格式化](https://www.cnblogs.com/vamei/archive/2013/03/12/2954938.html)*</small>\n\n<small>*[菜鸟教程-Python print()函数](https://www.runoob.com/python3/python-func-print.html)*</small>\n\n","source":"_posts/2020-02-12-Python-基础（二）.md","raw":"---\ntitle: Python 基础（二）\ndate: 2020-02-12 22:22:00\ncategories:\n- 计算机科学\ntags: \n- python\nmathjax: true\n\n---\n\npython 程序控制结构：选择、循环；python 函数；python基本输入输出（IO）<!--more-->\n\n\n\n---\n\n### Python 控制结构\n\n1. #### 选择 if-elif-else\n\n   ```python\n   >>>Parker = \"plane\"\n   >>>if Parker == \"car\":\n          charge = \"beatles\"\n      elif Parker == \"motor\":\n          charge = \"helmet\"\n      else:\n          charge = \"wings\"\n   >>>print (charge)\n   \n   wings\n   ```\n\n   三元运算符\n\n   ```python\n   t = x if x>=y else y\n   # 等价于\n   if x>=y：\n       t = x\n   else：\n       t = y\n   ```\n\n2. #### 循环\n\n   * ##### WHILE循环\n\n     ```python\n     while i <= 100:\n         i += 1\n     print (i)\n     ```\n\n   * ##### FOR循环\n\n     ```python\n     >>>blog = [\"what\", \"the\", \"HELL\", \"is\", \"this\", \"blog\", \"writing\"]\n     \n     # 序列项迭代\n     >>>for word in blog:\n            print (word, end = \" \")\n   \n     what the HELL is this blog writing \n   \n     # 序列索引迭代\n     >>>namelist = [('wang',16),('miao',60),('zhao',34)]\n     >>>dic = {}\n     >>>for i in range(len(namelist)):\n            name = namelist[i][0]\n            age = namelist[i][1]\n            dic[name] = age\n     >>>print(dic)\n   \n     {'wang': 16, 'miao': 60, 'zhao': 34}\n     ```\n   \n     **zip()迭代器迭代**\n   \n     ```python\n     zip()函数从多个列表中每次取一个元素\n     >>>blog = [\"B\", \"U\", \"L\", \"L\", \"S\", \"H\", \"I\",'T']\n     >>>ind = range(8)\n     >>>for word,num in zip(blog,ind):\n            print ('{0}-{1}'.format(word,num))\n        \n     B-0\n     U-1\n     L-2\n     L-3\n     S-4\n     H-5\n     I-6\n     T-7\n     ```\n   \n     **enumerate()同时得到下标和元素**\n\n     ```python\n     for (ind,word) in enumerate(blog):\n         print ind\n         print word\n     ```\n\n   * ##### 循环中的ELSE语句\n   \n     - 若从break跳出循环，不执行ELSE中的语句\n     \n     - 若正常循环结束，执行ELSE中的语句\n     \n       ```python\n       apples = 5\n       while apples !=0:\n           if apples ==1:\n               print ('only one left, continue?')\n               eat = input()\n            if eat == 'yes':\n                   apples -= 1\n                continue\n               elif eat == 'no':\n                   break\n               else:\n                   print(' ''yes'' or ''no'' ')\n                   continue\n           apples -= 1\n       else:\n           print('apples all eaten')\n       ```\n\n   * ##### 列表解析（list comprehension) -- 轻量级循环\n   \n     ```python\n     >>>print([x**2 for x in range(10) if x**2<50])\n     [0, 1, 4, 9, 16, 25, 36, 49]\n     \n     >>>print([(x,y) for x in range(2) for y in range(2)])\n     [(0, 0), (0, 1), (1, 0), (1, 1)]\n     ```\n     \n   * ##### yield生成器\n     ```python\n     # 当需要每次循环返回一个特定的值时，使用生成器比较方便，生成的是一个可迭代对象\n     def gen():\n      a = 1\n         yield a            # 第一次返回 1\n      a = 'cat'\n         yield a            # 第二次返回 cat\n         yield 99           # 第三次返回 99\n          \n     for i in gen():\n         print(i)\n              \n     # 生成器表达式\n     gen = (x for x in range(4)) # 与列表解析不同的是用()括起来，是可迭代对象\n     ```\n   \n   * ##### 循环对象（迭代器）\n   \n     一个**循环对象**包含next()方法，和StopIteration错误:\n     \n     ```python\n     f = open('test.txt')  # open返回的是一个循环对象\n        \tf.next()              # next每次返回的是下一行的内容\n        \tf.next()\n        \t\t\n     # 当到达结尾时，返回 StopIteration\n     ```\n     \n\n---\n\n### Python 函数\n\n1. #### 自定义函数\n\n   * ##### 一般定义def\n\n     ```python\n     def func(x,y):           # 定义函数func\n         print (x+y)\n     func(1,3)                # 调用函数\n     ```\n\n   * ##### lambda函数定义\n\n     ```python\n     func = lambda x,y: x+y   # 使用lambda函数定义func\n     print (func(1,3))\n     ```\n\n2. #### 不同类型的参数\n\n   * ##### 位置参数\n\n     ```python\n     def func(name,gender):\n         print('{0} is {1}'.format(name,gender))\n     func('Mia','Female')\n     ```\n\n   * ##### 关键字参数\n\n     ```python\n     def func(name,gender):\n         print('{0} is {1}'.format(name,gender))\n     func(gender='Female',name='Mia')  # 使用参数名区分参数，可以任意改变顺序\n     ```\n\n   * ##### 默认参数\n\n     ```python\n     def func(name,gender='Female'):   # 定义函数时可以设定参数默认值\n         print('{0} is {1}'.format(name,gender))\n     func('Mia')                       # 调用时不输入默认参数，则默认参数为默认值\n     func('Mia','Male')                # 默认参数也可以修改\n     ```\n\n   * ##### 可变长参数（包裹）\n\n     ```python\n     def func(name,*manywords):        # 加星号就变成了可变长参数（包裹）\n         print(name,'is',manywords)\n     func('Mia','a','girl','with','blue','eyes')\n     \n     Mia is ('a', 'girl', 'with', 'blue', 'eyes')\n     ```\n\n     ```python\n     def func(name,**manywords):       # 加两个星号变成了可变长关键字参数\n         print(name,manywords)\n     func('Mia', gender='Female', age=20, hair='blonde')\n     \n     Mia {'gender': 'Female', 'age': 20, 'hair': 'blonde'}\n     ```\n\n3. #### 函数可作为参数传递\n\n   ```python\n   def func(name,gender):\n       print('{0} is {1}'.format(name,gender))\n   \n   def test(f,a,b):\n       func(a,b)\n       \n   test(func,'Mia','Female')\n   ```\n\n4. #### 一些方便的函数\n\n   * ##### map()\n\n     ```python\n     # map函数每次从两个表中取出一个元素，带入前方的函数\n     ans = map((lambda x,y: x+y),[1,2,3],[7,8,9]) \n     print(list(ans))                        # 可以用list函数把map返回的循环对象转换为表\n     \n     [8, 10, 12]\n     ```\n\n   * ##### reduce()\n\n     ```python\n     # reduce函数累进地作用于各个参数，先取1，2，运算结果后再向后取3，……\n     ans = reduce((lambda x,y: x+y),[1,2,3,7,8,9]) \n     \n     # 本例实际运算的是(((((1+2)+3)+7)+8)+9)\n     ```\n\n   * ##### filter()\n\n     ```python\n     # filter函数用来筛选数据，仅把函数返回值为True的元素存入表（循环对象）\n     def func(a):\n         if a > 100:\n             return True\n         else:\n             return False\n     \n     ans = filter(func,[10,56,101,500])     \n     ```\n\n\n\n---\n\n### Python基本输入输出（IO）\n\n1. #### File IO\n\n   ```python\n   f = open('pybook','w+')             # 'r'只读，'w'只写，'w+'读写\n   txt = ['Hi!','Welcome to pybook!']\n   for word in txt:\n       f.write(word)\n       f.write('\\n')                   # 在写入txt文件时换行\n   f.close()                           # 记得关闭文件\n   \n   f = open('pybook','r')\n   print(f.readlines())             \n   f.close()\n   ```\n\n   常用的读写方法\n\n   ```python\n   f.read(N)          # 读取Nbytes的数据\n   f.readline()       # 读取一行\n   f.readlines()      # 读取所有行，储存在列表中，每个元素是一行\n   \n   f.write('xxx')             # 写入单个字符串'xxx'\n   f.writelines('aa\\n','bb')  # 当写入多个字符串构成列表(list)时，要用writelines\n   ```\n\n2. #### Interactive IO\n\n   ##### 输入Input()\n\n   ```python\n   >>>a = input(\"input:\")    # input('提示信息')\n   input:123 \n   \n   # 输入数值型\\列表\\字典\n   eval(input())\n   ```\n\n   ##### 输出print()\n\n   ```python\n   print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\n   # objects:输入对象，用,隔开\n   # sep: 打印时填充对象间隔，默认为' '空格\n   # end: 结束方式，\\n换行\n   ```\n\n   %格式化操作符\n\n   ```python\n   # %[(name)][flags][width].[precision]typecode\n   # (name):命名\n   # flags: +(右对齐), -(左对齐), ' '(在正数前填一个空格), '0'(填充0)\n   # width: 字符宽度\n   # precision: 小数点位数\n   # typecode: 数据类型\n   \n   print('%s is %d years old' % ('Hugo', 20))   # %s<--'Hugo', %d<--20 , 模版和填值以 % 隔开\n   \n   print('%(name)s is %(age)d years old' % {'name':'Hugo', 'age':20})   # 也可用字典填值\n   \n   print('%+10.3f,%-10.3f,%+10.3f' % (-13.2, 32, 9.44))\n      -13.200,32.000    ,000009.440\n   ```\n\n   .format()格式化\n\n   ```python\n   print('{0} and {1} and {0}'.format('a', 'b'))  # 可索引\n   a and b and a\n   \n   #{:4f}位数\n   #{>}右对齐\n   print(':{0:>4s} and {1:.3f}'.format('aa', 2.1)) # {0(索引):(位数)>(右对齐)4}\n   :  aa and 2.100\n   ```\n\n   print(f'')\n\n   ```python\n   x=3,y=4\n   print(f'x={x:d},y={y:.3f}')\n   ```\n\n   \n\n<br/><br/>\n\n<small>*参考*</small>\n\n<small>*[Vamei 博客园-Python快速教程](https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html)*</small>\n\n<small>*[RuiWo 博客园-python基础-格式化输出](https://www.cnblogs.com/qinchao0317/p/10699717.html)*</small>\n\n<small>*[Vamei 博客园-Python补充05 字符串格式化](https://www.cnblogs.com/vamei/archive/2013/03/12/2954938.html)*</small>\n\n<small>*[菜鸟教程-Python print()函数](https://www.runoob.com/python3/python-func-print.html)*</small>\n\n","slug":"2020-02-12-Python-基础（二）","published":1,"updated":"2020-03-12T16:15:07.290Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5b00062sfyg3lg3epi","content":"<p>python 程序控制结构：选择、循环；python 函数；python基本输入输出（IO）<a id=\"more\"></a></p>\n<hr>\n<h3 id=\"Python-控制结构\"><a href=\"#Python-控制结构\" class=\"headerlink\" title=\"Python 控制结构\"></a>Python 控制结构</h3><ol>\n<li><h4 id=\"选择-if-elif-else\"><a href=\"#选择-if-elif-else\" class=\"headerlink\" title=\"选择 if-elif-else\"></a>选择 if-elif-else</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;Parker = <span class=\"string\">\"plane\"</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">if</span> Parker == <span class=\"string\">\"car\"</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"beatles\"</span></span><br><span class=\"line\">   <span class=\"keyword\">elif</span> Parker == <span class=\"string\">\"motor\"</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"helmet\"</span></span><br><span class=\"line\">   <span class=\"keyword\">else</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"wings\"</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (charge)</span><br><span class=\"line\"></span><br><span class=\"line\">wings</span><br></pre></td></tr></table></figure>\n<p>三元运算符</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t = x <span class=\"keyword\">if</span> x&gt;=y <span class=\"keyword\">else</span> y</span><br><span class=\"line\"><span class=\"comment\"># 等价于</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> x&gt;=y：</span><br><span class=\"line\">    t = x</span><br><span class=\"line\"><span class=\"keyword\">else</span>：</span><br><span class=\"line\">    t = y</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"循环\"><a href=\"#循环\" class=\"headerlink\" title=\"循环\"></a>循环</h4><ul>\n<li><h5 id=\"WHILE循环\"><a href=\"#WHILE循环\" class=\"headerlink\" title=\"WHILE循环\"></a>WHILE循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> i &lt;= <span class=\"number\">100</span>:</span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (i)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"FOR循环\"><a href=\"#FOR循环\" class=\"headerlink\" title=\"FOR循环\"></a>FOR循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;blog = [<span class=\"string\">\"what\"</span>, <span class=\"string\">\"the\"</span>, <span class=\"string\">\"HELL\"</span>, <span class=\"string\">\"is\"</span>, <span class=\"string\">\"this\"</span>, <span class=\"string\">\"blog\"</span>, <span class=\"string\">\"writing\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 序列项迭代</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> blog:</span><br><span class=\"line\">       <span class=\"keyword\">print</span> (word, end = <span class=\"string\">\" \"</span>)</span><br><span class=\"line\">   </span><br><span class=\"line\">what the HELL <span class=\"keyword\">is</span> this blog writing </span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"comment\"># 序列索引迭代</span></span><br><span class=\"line\">&gt;&gt;&gt;namelist = [(<span class=\"string\">'wang'</span>,<span class=\"number\">16</span>),(<span class=\"string\">'miao'</span>,<span class=\"number\">60</span>),(<span class=\"string\">'zhao'</span>,<span class=\"number\">34</span>)]</span><br><span class=\"line\">&gt;&gt;&gt;dic = &#123;&#125;</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(namelist)):</span><br><span class=\"line\">       name = namelist[i][<span class=\"number\">0</span>]</span><br><span class=\"line\">       age = namelist[i][<span class=\"number\">1</span>]</span><br><span class=\"line\">       dic[name] = age</span><br><span class=\"line\">&gt;&gt;&gt;print(dic)</span><br><span class=\"line\">   </span><br><span class=\"line\">&#123;<span class=\"string\">'wang'</span>: <span class=\"number\">16</span>, <span class=\"string\">'miao'</span>: <span class=\"number\">60</span>, <span class=\"string\">'zhao'</span>: <span class=\"number\">34</span>&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>zip()迭代器迭代</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip()函数从多个列表中每次取一个元素</span><br><span class=\"line\">&gt;&gt;&gt;blog = [<span class=\"string\">\"B\"</span>, <span class=\"string\">\"U\"</span>, <span class=\"string\">\"L\"</span>, <span class=\"string\">\"L\"</span>, <span class=\"string\">\"S\"</span>, <span class=\"string\">\"H\"</span>, <span class=\"string\">\"I\"</span>,<span class=\"string\">'T'</span>]</span><br><span class=\"line\">&gt;&gt;&gt;ind = range(<span class=\"number\">8</span>)</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> word,num <span class=\"keyword\">in</span> zip(blog,ind):</span><br><span class=\"line\">       <span class=\"keyword\">print</span> (<span class=\"string\">'&#123;0&#125;-&#123;1&#125;'</span>.format(word,num))</span><br><span class=\"line\">   </span><br><span class=\"line\">B<span class=\"number\">-0</span></span><br><span class=\"line\">U<span class=\"number\">-1</span></span><br><span class=\"line\">L<span class=\"number\">-2</span></span><br><span class=\"line\">L<span class=\"number\">-3</span></span><br><span class=\"line\">S<span class=\"number\">-4</span></span><br><span class=\"line\">H<span class=\"number\">-5</span></span><br><span class=\"line\">I<span class=\"number\">-6</span></span><br><span class=\"line\">T<span class=\"number\">-7</span></span><br></pre></td></tr></table></figure>\n<p><strong>enumerate()同时得到下标和元素</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (ind,word) <span class=\"keyword\">in</span> enumerate(blog):</span><br><span class=\"line\">    <span class=\"keyword\">print</span> ind</span><br><span class=\"line\">    <span class=\"keyword\">print</span> word</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"循环中的ELSE语句\"><a href=\"#循环中的ELSE语句\" class=\"headerlink\" title=\"循环中的ELSE语句\"></a>循环中的ELSE语句</h5><ul>\n<li><p>若从break跳出循环，不执行ELSE中的语句</p>\n</li>\n<li><p>若正常循环结束，执行ELSE中的语句</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apples = <span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> apples !=<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> apples ==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> (<span class=\"string\">'only one left, continue?'</span>)</span><br><span class=\"line\">        eat = input()</span><br><span class=\"line\">     <span class=\"keyword\">if</span> eat == <span class=\"string\">'yes'</span>:</span><br><span class=\"line\">            apples -= <span class=\"number\">1</span></span><br><span class=\"line\">         <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> eat == <span class=\"string\">'no'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            print(<span class=\"string\">' '</span><span class=\"string\">'yes'</span><span class=\"string\">' or '</span><span class=\"string\">'no'</span><span class=\"string\">' '</span>)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    apples -= <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'apples all eaten'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h5 id=\"列表解析（list-comprehension-—-轻量级循环\"><a href=\"#列表解析（list-comprehension-—-轻量级循环\" class=\"headerlink\" title=\"列表解析（list comprehension) — 轻量级循环\"></a>列表解析（list comprehension) — 轻量级循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;print([x**<span class=\"number\">2</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>) <span class=\"keyword\">if</span> x**<span class=\"number\">2</span>&lt;<span class=\"number\">50</span>])</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>, <span class=\"number\">36</span>, <span class=\"number\">49</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print([(x,y) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>) <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>)])</span><br><span class=\"line\">[(<span class=\"number\">0</span>, <span class=\"number\">0</span>), (<span class=\"number\">0</span>, <span class=\"number\">1</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), (<span class=\"number\">1</span>, <span class=\"number\">1</span>)]</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"yield生成器\"><a href=\"#yield生成器\" class=\"headerlink\" title=\"yield生成器\"></a>yield生成器</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 当需要每次循环返回一个特定的值时，使用生成器比较方便，生成的是一个可迭代对象</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gen</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\"> a = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> a            <span class=\"comment\"># 第一次返回 1</span></span><br><span class=\"line\"> a = <span class=\"string\">'cat'</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> a            <span class=\"comment\"># 第二次返回 cat</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> <span class=\"number\">99</span>           <span class=\"comment\"># 第三次返回 99</span></span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> gen():</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\">         </span><br><span class=\"line\"><span class=\"comment\"># 生成器表达式</span></span><br><span class=\"line\">gen = (x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)) <span class=\"comment\"># 与列表解析不同的是用()括起来，是可迭代对象</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"循环对象（迭代器）\"><a href=\"#循环对象（迭代器）\" class=\"headerlink\" title=\"循环对象（迭代器）\"></a>循环对象（迭代器）</h5><p>一个<strong>循环对象</strong>包含next()方法，和StopIteration错误:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">'test.txt'</span>)  <span class=\"comment\"># open返回的是一个循环对象</span></span><br><span class=\"line\">   \tf.next()              <span class=\"comment\"># next每次返回的是下一行的内容</span></span><br><span class=\"line\">   \tf.next()</span><br><span class=\"line\">   \t\t</span><br><span class=\"line\"><span class=\"comment\"># 当到达结尾时，返回 StopIteration</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-函数\"><a href=\"#Python-函数\" class=\"headerlink\" title=\"Python 函数\"></a>Python 函数</h3><ol>\n<li><h4 id=\"自定义函数\"><a href=\"#自定义函数\" class=\"headerlink\" title=\"自定义函数\"></a>自定义函数</h4><ul>\n<li><h5 id=\"一般定义def\"><a href=\"#一般定义def\" class=\"headerlink\" title=\"一般定义def\"></a>一般定义def</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(x,y)</span>:</span>           <span class=\"comment\"># 定义函数func</span></span><br><span class=\"line\">    <span class=\"keyword\">print</span> (x+y)</span><br><span class=\"line\">func(<span class=\"number\">1</span>,<span class=\"number\">3</span>)                <span class=\"comment\"># 调用函数</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"lambda函数定义\"><a href=\"#lambda函数定义\" class=\"headerlink\" title=\"lambda函数定义\"></a>lambda函数定义</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func = <span class=\"keyword\">lambda</span> x,y: x+y   <span class=\"comment\"># 使用lambda函数定义func</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (func(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"不同类型的参数\"><a href=\"#不同类型的参数\" class=\"headerlink\" title=\"不同类型的参数\"></a>不同类型的参数</h4><ul>\n<li><h5 id=\"位置参数\"><a href=\"#位置参数\" class=\"headerlink\" title=\"位置参数\"></a>位置参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'Female'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"关键字参数\"><a href=\"#关键字参数\" class=\"headerlink\" title=\"关键字参数\"></a>关键字参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(gender=<span class=\"string\">'Female'</span>,name=<span class=\"string\">'Mia'</span>)  <span class=\"comment\"># 使用参数名区分参数，可以任意改变顺序</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"默认参数\"><a href=\"#默认参数\" class=\"headerlink\" title=\"默认参数\"></a>默认参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender=<span class=\"string\">'Female'</span>)</span>:</span>   <span class=\"comment\"># 定义函数时可以设定参数默认值</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>)                       <span class=\"comment\"># 调用时不输入默认参数，则默认参数为默认值</span></span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'Male'</span>)                <span class=\"comment\"># 默认参数也可以修改</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"可变长参数（包裹）\"><a href=\"#可变长参数（包裹）\" class=\"headerlink\" title=\"可变长参数（包裹）\"></a>可变长参数（包裹）</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,*manywords)</span>:</span>        <span class=\"comment\"># 加星号就变成了可变长参数（包裹）</span></span><br><span class=\"line\">    print(name,<span class=\"string\">'is'</span>,manywords)</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'a'</span>,<span class=\"string\">'girl'</span>,<span class=\"string\">'with'</span>,<span class=\"string\">'blue'</span>,<span class=\"string\">'eyes'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">Mia <span class=\"keyword\">is</span> (<span class=\"string\">'a'</span>, <span class=\"string\">'girl'</span>, <span class=\"string\">'with'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'eyes'</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,**manywords)</span>:</span>       <span class=\"comment\"># 加两个星号变成了可变长关键字参数</span></span><br><span class=\"line\">    print(name,manywords)</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>, gender=<span class=\"string\">'Female'</span>, age=<span class=\"number\">20</span>, hair=<span class=\"string\">'blonde'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">Mia &#123;<span class=\"string\">'gender'</span>: <span class=\"string\">'Female'</span>, <span class=\"string\">'age'</span>: <span class=\"number\">20</span>, <span class=\"string\">'hair'</span>: <span class=\"string\">'blonde'</span>&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"函数可作为参数传递\"><a href=\"#函数可作为参数传递\" class=\"headerlink\" title=\"函数可作为参数传递\"></a>函数可作为参数传递</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test</span><span class=\"params\">(f,a,b)</span>:</span></span><br><span class=\"line\">    func(a,b)</span><br><span class=\"line\">    </span><br><span class=\"line\">test(func,<span class=\"string\">'Mia'</span>,<span class=\"string\">'Female'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"一些方便的函数\"><a href=\"#一些方便的函数\" class=\"headerlink\" title=\"一些方便的函数\"></a>一些方便的函数</h4><ul>\n<li><h5 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map()\"></a>map()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># map函数每次从两个表中取出一个元素，带入前方的函数</span></span><br><span class=\"line\">ans = map((<span class=\"keyword\">lambda</span> x,y: x+y),[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]) </span><br><span class=\"line\">print(list(ans))                        <span class=\"comment\"># 可以用list函数把map返回的循环对象转换为表</span></span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"number\">8</span>, <span class=\"number\">10</span>, <span class=\"number\">12</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce()\"></a>reduce()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># reduce函数累进地作用于各个参数，先取1，2，运算结果后再向后取3，……</span></span><br><span class=\"line\">ans = reduce((<span class=\"keyword\">lambda</span> x,y: x+y),[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 本例实际运算的是(((((1+2)+3)+7)+8)+9)</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter()\"></a>filter()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># filter函数用来筛选数据，仅把函数返回值为True的元素存入表（循环对象）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(a)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> a &gt; <span class=\"number\">100</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">ans = filter(func,[<span class=\"number\">10</span>,<span class=\"number\">56</span>,<span class=\"number\">101</span>,<span class=\"number\">500</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python基本输入输出（IO）\"><a href=\"#Python基本输入输出（IO）\" class=\"headerlink\" title=\"Python基本输入输出（IO）\"></a>Python基本输入输出（IO）</h3><ol>\n<li><h4 id=\"File-IO\"><a href=\"#File-IO\" class=\"headerlink\" title=\"File IO\"></a>File IO</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">'pybook'</span>,<span class=\"string\">'w+'</span>)             <span class=\"comment\"># 'r'只读，'w'只写，'w+'读写</span></span><br><span class=\"line\">txt = [<span class=\"string\">'Hi!'</span>,<span class=\"string\">'Welcome to pybook!'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> txt:</span><br><span class=\"line\">    f.write(word)</span><br><span class=\"line\">    f.write(<span class=\"string\">'\\n'</span>)                   <span class=\"comment\"># 在写入txt文件时换行</span></span><br><span class=\"line\">f.close()                           <span class=\"comment\"># 记得关闭文件</span></span><br><span class=\"line\"></span><br><span class=\"line\">f = open(<span class=\"string\">'pybook'</span>,<span class=\"string\">'r'</span>)</span><br><span class=\"line\">print(f.readlines())             </span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>常用的读写方法</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f.read(N)          <span class=\"comment\"># 读取Nbytes的数据</span></span><br><span class=\"line\">f.readline()       <span class=\"comment\"># 读取一行</span></span><br><span class=\"line\">f.readlines()      <span class=\"comment\"># 读取所有行，储存在列表中，每个元素是一行</span></span><br><span class=\"line\"></span><br><span class=\"line\">f.write(<span class=\"string\">'xxx'</span>)             <span class=\"comment\"># 写入单个字符串'xxx'</span></span><br><span class=\"line\">f.writelines(<span class=\"string\">'aa\\n'</span>,<span class=\"string\">'bb'</span>)  <span class=\"comment\"># 当写入多个字符串构成列表(list)时，要用writelines</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"Interactive-IO\"><a href=\"#Interactive-IO\" class=\"headerlink\" title=\"Interactive IO\"></a>Interactive IO</h4><h5 id=\"输入Input\"><a href=\"#输入Input\" class=\"headerlink\" title=\"输入Input()\"></a>输入Input()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = input(<span class=\"string\">\"input:\"</span>)    <span class=\"comment\"># input('提示信息')</span></span><br><span class=\"line\">input:<span class=\"number\">123</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输入数值型\\列表\\字典</span></span><br><span class=\"line\">eval(input())</span><br></pre></td></tr></table></figure>\n<h5 id=\"输出print\"><a href=\"#输出print\" class=\"headerlink\" title=\"输出print()\"></a>输出print()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(*objects, sep=<span class=\"string\">' '</span>, end=<span class=\"string\">'\\n'</span>, file=sys.stdout, flush=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># objects:输入对象，用,隔开</span></span><br><span class=\"line\"><span class=\"comment\"># sep: 打印时填充对象间隔，默认为' '空格</span></span><br><span class=\"line\"><span class=\"comment\"># end: 结束方式，\\n换行</span></span><br></pre></td></tr></table></figure>\n<p>%格式化操作符</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># %[(name)][flags][width].[precision]typecode</span></span><br><span class=\"line\"><span class=\"comment\"># (name):命名</span></span><br><span class=\"line\"><span class=\"comment\"># flags: +(右对齐), -(左对齐), ' '(在正数前填一个空格), '0'(填充0)</span></span><br><span class=\"line\"><span class=\"comment\"># width: 字符宽度</span></span><br><span class=\"line\"><span class=\"comment\"># precision: 小数点位数</span></span><br><span class=\"line\"><span class=\"comment\"># typecode: 数据类型</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%s is %d years old'</span> % (<span class=\"string\">'Hugo'</span>, <span class=\"number\">20</span>))   <span class=\"comment\"># %s&lt;--'Hugo', %d&lt;--20 , 模版和填值以 % 隔开</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%(name)s is %(age)d years old'</span> % &#123;<span class=\"string\">'name'</span>:<span class=\"string\">'Hugo'</span>, <span class=\"string\">'age'</span>:<span class=\"number\">20</span>&#125;)   <span class=\"comment\"># 也可用字典填值</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%+10.3f,%-10.3f,%+10.3f'</span> % (<span class=\"number\">-13.2</span>, <span class=\"number\">32</span>, <span class=\"number\">9.44</span>))</span><br><span class=\"line\">   <span class=\"number\">-13.200</span>,<span class=\"number\">32.000</span>    ,<span class=\"number\">000009.440</span></span><br></pre></td></tr></table></figure>\n<p>.format()格式化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(<span class=\"string\">'&#123;0&#125; and &#123;1&#125; and &#123;0&#125;'</span>.format(<span class=\"string\">'a'</span>, <span class=\"string\">'b'</span>))  <span class=\"comment\"># 可索引</span></span><br><span class=\"line\">a <span class=\"keyword\">and</span> b <span class=\"keyword\">and</span> a</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#&#123;:4f&#125;位数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;&gt;&#125;右对齐</span></span><br><span class=\"line\">print(<span class=\"string\">':&#123;0:&gt;4s&#125; and &#123;1:.3f&#125;'</span>.format(<span class=\"string\">'aa'</span>, <span class=\"number\">2.1</span>)) <span class=\"comment\"># &#123;0(索引):(位数)&gt;(右对齐)4&#125;</span></span><br><span class=\"line\">:  aa <span class=\"keyword\">and</span> <span class=\"number\">2.100</span></span><br></pre></td></tr></table></figure>\n<p>print(f’’)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=<span class=\"number\">3</span>,y=<span class=\"number\">4</span></span><br><span class=\"line\">print(<span class=\"string\">f'x=<span class=\"subst\">&#123;x:d&#125;</span>,y=<span class=\"subst\">&#123;y:<span class=\"number\">.3</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python快速教程</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/qinchao0317/p/10699717.html\" target=\"_blank\" rel=\"noopener\">RuiWo 博客园-python基础-格式化输出</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2013/03/12/2954938.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python补充05 字符串格式化</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/python3/python-func-print.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程-Python print()函数</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>python 程序控制结构：选择、循环；python 函数；python基本输入输出（IO）","more":"</p>\n<hr>\n<h3 id=\"Python-控制结构\"><a href=\"#Python-控制结构\" class=\"headerlink\" title=\"Python 控制结构\"></a>Python 控制结构</h3><ol>\n<li><h4 id=\"选择-if-elif-else\"><a href=\"#选择-if-elif-else\" class=\"headerlink\" title=\"选择 if-elif-else\"></a>选择 if-elif-else</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;Parker = <span class=\"string\">\"plane\"</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">if</span> Parker == <span class=\"string\">\"car\"</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"beatles\"</span></span><br><span class=\"line\">   <span class=\"keyword\">elif</span> Parker == <span class=\"string\">\"motor\"</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"helmet\"</span></span><br><span class=\"line\">   <span class=\"keyword\">else</span>:</span><br><span class=\"line\">       charge = <span class=\"string\">\"wings\"</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (charge)</span><br><span class=\"line\"></span><br><span class=\"line\">wings</span><br></pre></td></tr></table></figure>\n<p>三元运算符</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t = x <span class=\"keyword\">if</span> x&gt;=y <span class=\"keyword\">else</span> y</span><br><span class=\"line\"><span class=\"comment\"># 等价于</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> x&gt;=y：</span><br><span class=\"line\">    t = x</span><br><span class=\"line\"><span class=\"keyword\">else</span>：</span><br><span class=\"line\">    t = y</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"循环\"><a href=\"#循环\" class=\"headerlink\" title=\"循环\"></a>循环</h4><ul>\n<li><h5 id=\"WHILE循环\"><a href=\"#WHILE循环\" class=\"headerlink\" title=\"WHILE循环\"></a>WHILE循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> i &lt;= <span class=\"number\">100</span>:</span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (i)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"FOR循环\"><a href=\"#FOR循环\" class=\"headerlink\" title=\"FOR循环\"></a>FOR循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;blog = [<span class=\"string\">\"what\"</span>, <span class=\"string\">\"the\"</span>, <span class=\"string\">\"HELL\"</span>, <span class=\"string\">\"is\"</span>, <span class=\"string\">\"this\"</span>, <span class=\"string\">\"blog\"</span>, <span class=\"string\">\"writing\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 序列项迭代</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> blog:</span><br><span class=\"line\">       <span class=\"keyword\">print</span> (word, end = <span class=\"string\">\" \"</span>)</span><br><span class=\"line\">   </span><br><span class=\"line\">what the HELL <span class=\"keyword\">is</span> this blog writing </span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"comment\"># 序列索引迭代</span></span><br><span class=\"line\">&gt;&gt;&gt;namelist = [(<span class=\"string\">'wang'</span>,<span class=\"number\">16</span>),(<span class=\"string\">'miao'</span>,<span class=\"number\">60</span>),(<span class=\"string\">'zhao'</span>,<span class=\"number\">34</span>)]</span><br><span class=\"line\">&gt;&gt;&gt;dic = &#123;&#125;</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(namelist)):</span><br><span class=\"line\">       name = namelist[i][<span class=\"number\">0</span>]</span><br><span class=\"line\">       age = namelist[i][<span class=\"number\">1</span>]</span><br><span class=\"line\">       dic[name] = age</span><br><span class=\"line\">&gt;&gt;&gt;print(dic)</span><br><span class=\"line\">   </span><br><span class=\"line\">&#123;<span class=\"string\">'wang'</span>: <span class=\"number\">16</span>, <span class=\"string\">'miao'</span>: <span class=\"number\">60</span>, <span class=\"string\">'zhao'</span>: <span class=\"number\">34</span>&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>zip()迭代器迭代</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip()函数从多个列表中每次取一个元素</span><br><span class=\"line\">&gt;&gt;&gt;blog = [<span class=\"string\">\"B\"</span>, <span class=\"string\">\"U\"</span>, <span class=\"string\">\"L\"</span>, <span class=\"string\">\"L\"</span>, <span class=\"string\">\"S\"</span>, <span class=\"string\">\"H\"</span>, <span class=\"string\">\"I\"</span>,<span class=\"string\">'T'</span>]</span><br><span class=\"line\">&gt;&gt;&gt;ind = range(<span class=\"number\">8</span>)</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">for</span> word,num <span class=\"keyword\">in</span> zip(blog,ind):</span><br><span class=\"line\">       <span class=\"keyword\">print</span> (<span class=\"string\">'&#123;0&#125;-&#123;1&#125;'</span>.format(word,num))</span><br><span class=\"line\">   </span><br><span class=\"line\">B<span class=\"number\">-0</span></span><br><span class=\"line\">U<span class=\"number\">-1</span></span><br><span class=\"line\">L<span class=\"number\">-2</span></span><br><span class=\"line\">L<span class=\"number\">-3</span></span><br><span class=\"line\">S<span class=\"number\">-4</span></span><br><span class=\"line\">H<span class=\"number\">-5</span></span><br><span class=\"line\">I<span class=\"number\">-6</span></span><br><span class=\"line\">T<span class=\"number\">-7</span></span><br></pre></td></tr></table></figure>\n<p><strong>enumerate()同时得到下标和元素</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (ind,word) <span class=\"keyword\">in</span> enumerate(blog):</span><br><span class=\"line\">    <span class=\"keyword\">print</span> ind</span><br><span class=\"line\">    <span class=\"keyword\">print</span> word</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"循环中的ELSE语句\"><a href=\"#循环中的ELSE语句\" class=\"headerlink\" title=\"循环中的ELSE语句\"></a>循环中的ELSE语句</h5><ul>\n<li><p>若从break跳出循环，不执行ELSE中的语句</p>\n</li>\n<li><p>若正常循环结束，执行ELSE中的语句</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apples = <span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> apples !=<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> apples ==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> (<span class=\"string\">'only one left, continue?'</span>)</span><br><span class=\"line\">        eat = input()</span><br><span class=\"line\">     <span class=\"keyword\">if</span> eat == <span class=\"string\">'yes'</span>:</span><br><span class=\"line\">            apples -= <span class=\"number\">1</span></span><br><span class=\"line\">         <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> eat == <span class=\"string\">'no'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            print(<span class=\"string\">' '</span><span class=\"string\">'yes'</span><span class=\"string\">' or '</span><span class=\"string\">'no'</span><span class=\"string\">' '</span>)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    apples -= <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'apples all eaten'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h5 id=\"列表解析（list-comprehension-—-轻量级循环\"><a href=\"#列表解析（list-comprehension-—-轻量级循环\" class=\"headerlink\" title=\"列表解析（list comprehension) — 轻量级循环\"></a>列表解析（list comprehension) — 轻量级循环</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;print([x**<span class=\"number\">2</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>) <span class=\"keyword\">if</span> x**<span class=\"number\">2</span>&lt;<span class=\"number\">50</span>])</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>, <span class=\"number\">36</span>, <span class=\"number\">49</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print([(x,y) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>) <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>)])</span><br><span class=\"line\">[(<span class=\"number\">0</span>, <span class=\"number\">0</span>), (<span class=\"number\">0</span>, <span class=\"number\">1</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), (<span class=\"number\">1</span>, <span class=\"number\">1</span>)]</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"yield生成器\"><a href=\"#yield生成器\" class=\"headerlink\" title=\"yield生成器\"></a>yield生成器</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 当需要每次循环返回一个特定的值时，使用生成器比较方便，生成的是一个可迭代对象</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gen</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\"> a = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> a            <span class=\"comment\"># 第一次返回 1</span></span><br><span class=\"line\"> a = <span class=\"string\">'cat'</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> a            <span class=\"comment\"># 第二次返回 cat</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> <span class=\"number\">99</span>           <span class=\"comment\"># 第三次返回 99</span></span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> gen():</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\">         </span><br><span class=\"line\"><span class=\"comment\"># 生成器表达式</span></span><br><span class=\"line\">gen = (x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)) <span class=\"comment\"># 与列表解析不同的是用()括起来，是可迭代对象</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"循环对象（迭代器）\"><a href=\"#循环对象（迭代器）\" class=\"headerlink\" title=\"循环对象（迭代器）\"></a>循环对象（迭代器）</h5><p>一个<strong>循环对象</strong>包含next()方法，和StopIteration错误:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">'test.txt'</span>)  <span class=\"comment\"># open返回的是一个循环对象</span></span><br><span class=\"line\">   \tf.next()              <span class=\"comment\"># next每次返回的是下一行的内容</span></span><br><span class=\"line\">   \tf.next()</span><br><span class=\"line\">   \t\t</span><br><span class=\"line\"><span class=\"comment\"># 当到达结尾时，返回 StopIteration</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-函数\"><a href=\"#Python-函数\" class=\"headerlink\" title=\"Python 函数\"></a>Python 函数</h3><ol>\n<li><h4 id=\"自定义函数\"><a href=\"#自定义函数\" class=\"headerlink\" title=\"自定义函数\"></a>自定义函数</h4><ul>\n<li><h5 id=\"一般定义def\"><a href=\"#一般定义def\" class=\"headerlink\" title=\"一般定义def\"></a>一般定义def</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(x,y)</span>:</span>           <span class=\"comment\"># 定义函数func</span></span><br><span class=\"line\">    <span class=\"keyword\">print</span> (x+y)</span><br><span class=\"line\">func(<span class=\"number\">1</span>,<span class=\"number\">3</span>)                <span class=\"comment\"># 调用函数</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"lambda函数定义\"><a href=\"#lambda函数定义\" class=\"headerlink\" title=\"lambda函数定义\"></a>lambda函数定义</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func = <span class=\"keyword\">lambda</span> x,y: x+y   <span class=\"comment\"># 使用lambda函数定义func</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (func(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"不同类型的参数\"><a href=\"#不同类型的参数\" class=\"headerlink\" title=\"不同类型的参数\"></a>不同类型的参数</h4><ul>\n<li><h5 id=\"位置参数\"><a href=\"#位置参数\" class=\"headerlink\" title=\"位置参数\"></a>位置参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'Female'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"关键字参数\"><a href=\"#关键字参数\" class=\"headerlink\" title=\"关键字参数\"></a>关键字参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(gender=<span class=\"string\">'Female'</span>,name=<span class=\"string\">'Mia'</span>)  <span class=\"comment\"># 使用参数名区分参数，可以任意改变顺序</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"默认参数\"><a href=\"#默认参数\" class=\"headerlink\" title=\"默认参数\"></a>默认参数</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender=<span class=\"string\">'Female'</span>)</span>:</span>   <span class=\"comment\"># 定义函数时可以设定参数默认值</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>)                       <span class=\"comment\"># 调用时不输入默认参数，则默认参数为默认值</span></span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'Male'</span>)                <span class=\"comment\"># 默认参数也可以修改</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"可变长参数（包裹）\"><a href=\"#可变长参数（包裹）\" class=\"headerlink\" title=\"可变长参数（包裹）\"></a>可变长参数（包裹）</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,*manywords)</span>:</span>        <span class=\"comment\"># 加星号就变成了可变长参数（包裹）</span></span><br><span class=\"line\">    print(name,<span class=\"string\">'is'</span>,manywords)</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>,<span class=\"string\">'a'</span>,<span class=\"string\">'girl'</span>,<span class=\"string\">'with'</span>,<span class=\"string\">'blue'</span>,<span class=\"string\">'eyes'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">Mia <span class=\"keyword\">is</span> (<span class=\"string\">'a'</span>, <span class=\"string\">'girl'</span>, <span class=\"string\">'with'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'eyes'</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,**manywords)</span>:</span>       <span class=\"comment\"># 加两个星号变成了可变长关键字参数</span></span><br><span class=\"line\">    print(name,manywords)</span><br><span class=\"line\">func(<span class=\"string\">'Mia'</span>, gender=<span class=\"string\">'Female'</span>, age=<span class=\"number\">20</span>, hair=<span class=\"string\">'blonde'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">Mia &#123;<span class=\"string\">'gender'</span>: <span class=\"string\">'Female'</span>, <span class=\"string\">'age'</span>: <span class=\"number\">20</span>, <span class=\"string\">'hair'</span>: <span class=\"string\">'blonde'</span>&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"函数可作为参数传递\"><a href=\"#函数可作为参数传递\" class=\"headerlink\" title=\"函数可作为参数传递\"></a>函数可作为参数传递</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(name,gender)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'&#123;0&#125; is &#123;1&#125;'</span>.format(name,gender))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test</span><span class=\"params\">(f,a,b)</span>:</span></span><br><span class=\"line\">    func(a,b)</span><br><span class=\"line\">    </span><br><span class=\"line\">test(func,<span class=\"string\">'Mia'</span>,<span class=\"string\">'Female'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"一些方便的函数\"><a href=\"#一些方便的函数\" class=\"headerlink\" title=\"一些方便的函数\"></a>一些方便的函数</h4><ul>\n<li><h5 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map()\"></a>map()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># map函数每次从两个表中取出一个元素，带入前方的函数</span></span><br><span class=\"line\">ans = map((<span class=\"keyword\">lambda</span> x,y: x+y),[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]) </span><br><span class=\"line\">print(list(ans))                        <span class=\"comment\"># 可以用list函数把map返回的循环对象转换为表</span></span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"number\">8</span>, <span class=\"number\">10</span>, <span class=\"number\">12</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce()\"></a>reduce()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># reduce函数累进地作用于各个参数，先取1，2，运算结果后再向后取3，……</span></span><br><span class=\"line\">ans = reduce((<span class=\"keyword\">lambda</span> x,y: x+y),[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 本例实际运算的是(((((1+2)+3)+7)+8)+9)</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter()\"></a>filter()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># filter函数用来筛选数据，仅把函数返回值为True的元素存入表（循环对象）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(a)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> a &gt; <span class=\"number\">100</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">ans = filter(func,[<span class=\"number\">10</span>,<span class=\"number\">56</span>,<span class=\"number\">101</span>,<span class=\"number\">500</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python基本输入输出（IO）\"><a href=\"#Python基本输入输出（IO）\" class=\"headerlink\" title=\"Python基本输入输出（IO）\"></a>Python基本输入输出（IO）</h3><ol>\n<li><h4 id=\"File-IO\"><a href=\"#File-IO\" class=\"headerlink\" title=\"File IO\"></a>File IO</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">'pybook'</span>,<span class=\"string\">'w+'</span>)             <span class=\"comment\"># 'r'只读，'w'只写，'w+'读写</span></span><br><span class=\"line\">txt = [<span class=\"string\">'Hi!'</span>,<span class=\"string\">'Welcome to pybook!'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> txt:</span><br><span class=\"line\">    f.write(word)</span><br><span class=\"line\">    f.write(<span class=\"string\">'\\n'</span>)                   <span class=\"comment\"># 在写入txt文件时换行</span></span><br><span class=\"line\">f.close()                           <span class=\"comment\"># 记得关闭文件</span></span><br><span class=\"line\"></span><br><span class=\"line\">f = open(<span class=\"string\">'pybook'</span>,<span class=\"string\">'r'</span>)</span><br><span class=\"line\">print(f.readlines())             </span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>常用的读写方法</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f.read(N)          <span class=\"comment\"># 读取Nbytes的数据</span></span><br><span class=\"line\">f.readline()       <span class=\"comment\"># 读取一行</span></span><br><span class=\"line\">f.readlines()      <span class=\"comment\"># 读取所有行，储存在列表中，每个元素是一行</span></span><br><span class=\"line\"></span><br><span class=\"line\">f.write(<span class=\"string\">'xxx'</span>)             <span class=\"comment\"># 写入单个字符串'xxx'</span></span><br><span class=\"line\">f.writelines(<span class=\"string\">'aa\\n'</span>,<span class=\"string\">'bb'</span>)  <span class=\"comment\"># 当写入多个字符串构成列表(list)时，要用writelines</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"Interactive-IO\"><a href=\"#Interactive-IO\" class=\"headerlink\" title=\"Interactive IO\"></a>Interactive IO</h4><h5 id=\"输入Input\"><a href=\"#输入Input\" class=\"headerlink\" title=\"输入Input()\"></a>输入Input()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = input(<span class=\"string\">\"input:\"</span>)    <span class=\"comment\"># input('提示信息')</span></span><br><span class=\"line\">input:<span class=\"number\">123</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输入数值型\\列表\\字典</span></span><br><span class=\"line\">eval(input())</span><br></pre></td></tr></table></figure>\n<h5 id=\"输出print\"><a href=\"#输出print\" class=\"headerlink\" title=\"输出print()\"></a>输出print()</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(*objects, sep=<span class=\"string\">' '</span>, end=<span class=\"string\">'\\n'</span>, file=sys.stdout, flush=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># objects:输入对象，用,隔开</span></span><br><span class=\"line\"><span class=\"comment\"># sep: 打印时填充对象间隔，默认为' '空格</span></span><br><span class=\"line\"><span class=\"comment\"># end: 结束方式，\\n换行</span></span><br></pre></td></tr></table></figure>\n<p>%格式化操作符</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># %[(name)][flags][width].[precision]typecode</span></span><br><span class=\"line\"><span class=\"comment\"># (name):命名</span></span><br><span class=\"line\"><span class=\"comment\"># flags: +(右对齐), -(左对齐), ' '(在正数前填一个空格), '0'(填充0)</span></span><br><span class=\"line\"><span class=\"comment\"># width: 字符宽度</span></span><br><span class=\"line\"><span class=\"comment\"># precision: 小数点位数</span></span><br><span class=\"line\"><span class=\"comment\"># typecode: 数据类型</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%s is %d years old'</span> % (<span class=\"string\">'Hugo'</span>, <span class=\"number\">20</span>))   <span class=\"comment\"># %s&lt;--'Hugo', %d&lt;--20 , 模版和填值以 % 隔开</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%(name)s is %(age)d years old'</span> % &#123;<span class=\"string\">'name'</span>:<span class=\"string\">'Hugo'</span>, <span class=\"string\">'age'</span>:<span class=\"number\">20</span>&#125;)   <span class=\"comment\"># 也可用字典填值</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'%+10.3f,%-10.3f,%+10.3f'</span> % (<span class=\"number\">-13.2</span>, <span class=\"number\">32</span>, <span class=\"number\">9.44</span>))</span><br><span class=\"line\">   <span class=\"number\">-13.200</span>,<span class=\"number\">32.000</span>    ,<span class=\"number\">000009.440</span></span><br></pre></td></tr></table></figure>\n<p>.format()格式化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(<span class=\"string\">'&#123;0&#125; and &#123;1&#125; and &#123;0&#125;'</span>.format(<span class=\"string\">'a'</span>, <span class=\"string\">'b'</span>))  <span class=\"comment\"># 可索引</span></span><br><span class=\"line\">a <span class=\"keyword\">and</span> b <span class=\"keyword\">and</span> a</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#&#123;:4f&#125;位数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;&gt;&#125;右对齐</span></span><br><span class=\"line\">print(<span class=\"string\">':&#123;0:&gt;4s&#125; and &#123;1:.3f&#125;'</span>.format(<span class=\"string\">'aa'</span>, <span class=\"number\">2.1</span>)) <span class=\"comment\"># &#123;0(索引):(位数)&gt;(右对齐)4&#125;</span></span><br><span class=\"line\">:  aa <span class=\"keyword\">and</span> <span class=\"number\">2.100</span></span><br></pre></td></tr></table></figure>\n<p>print(f’’)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=<span class=\"number\">3</span>,y=<span class=\"number\">4</span></span><br><span class=\"line\">print(<span class=\"string\">f'x=<span class=\"subst\">&#123;x:d&#125;</span>,y=<span class=\"subst\">&#123;y:<span class=\"number\">.3</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python快速教程</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/qinchao0317/p/10699717.html\" target=\"_blank\" rel=\"noopener\">RuiWo 博客园-python基础-格式化输出</a></em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2013/03/12/2954938.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python补充05 字符串格式化</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/python3/python-func-print.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程-Python print()函数</a></em></small></p>"},{"title":"AI学习笔记--强化学习","date":"2020-10-20T15:45:00.000Z","mathjax":true,"_content":"\n强化学习在“机器”与“环境”的交互中完成。机器对环境采取一个动作（Action），环境状态（State）会相应作出改变；环境状态的变化对应了一个奖励（Reward），来反馈给机器。强化学习要解决的问题就是：为机器找到一个最优的策略（Policy），这个策略对应的动作序列使得长期累积的奖赏最大。<!--more-->\n\n<img src=\"/images/IMG_0768.jpeg\" alt=\"IMG_0768\" style=\"zoom:50%;\" />\n\n- 监督学习：样本已标记；样本独立（IID，独立同分布）\n\n- 强化学习：延迟标记（奖励）；样本通常是序列\n\n**序列决策（Sequential Decision Making）**\n\n选择一个动作序列，使得这个动作序列获得总的奖励最大。奖励可能是即时的，也可能是延迟获得的。\n\nHistory：由观测、动作、奖励构成的序列\n$$\nH_t =O_1,R_1,A_1,...A_{t-1},O_t,R_{t}\n$$\nState：History决定了当前状态，用来决策下面的动作。\n$$\nS_t = f(H_t)\n$$\n环境状态和机器状态\n\nPolicy\n\n* Deterministic\n\n* Stochastic\n\n\n\n### \t第二章 马尔可夫决策过程（MDP）\n\n#### 1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\n\n##### 1.1 马尔可夫链（Markov Chain）\n\n下一时刻的状态只取决于当前时刻的状态，而与过去的历史状态无关，称为**马尔可夫特征**。 具有马尔可夫特征的一系列状态构成一条**马尔可夫链**。\n\n<img src=\"/images/MarkovChain.png\" alt=\"MarkovChain\" style=\"zoom:50%;\" />\n\n##### 1.2 马尔可夫奖励过程（Markov Reward Process）\n\n对不同的状态s设定**奖励**R(s)，就形成了对某些状态的偏好。\n\n给定一条马尔可夫链，从某一时刻t开始，到最大时步T结束，得到的总奖励值称为**回报（Return）**或**累积奖励**。常用的累积奖励形式有\n$$\nT步累积奖励：G_t=\\frac{1}{T}(R_{t+1}+R_{t+2}+...+R_T)\n$$\n\n$$\n\\gamma折扣累积奖励：G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...\n$$\n\n某一状态s在t时刻的**价值函数(value function)**是t时刻从该状态出发得到回报的期望\n$$\nV_t(s)=E(G_t|s_t=s)\n$$\n定义了价值函数，我们的目标就具象了起来：想要更多的回报，就要令价值函数越大越好。\n\n价值函数的计算有两种方法：\n\n* Monte Carlo采样（采样得到足够多的马尔可夫链样本，计算回报的期望）\n\n* Bellman Equation（迭代式）\n\n$$\nV(s)=R(s)+\\gamma \\sum_{s' \\in S}P(s'|s)V(s')\n$$\n\n<img src=\"/images/Bellman.png\" alt=\"Bellman\" style=\"zoom:60%;\" />\n\n这里面的$P(s'|s)$是**状态转移函数**，含义是从当前状态s向下一个状态s‘转移的概率。\n\n假设马尔可夫链无限长，即终止时间T无限大，则$V(s)=V(s')$，Bellman equation可以写成矩阵形式\n$$\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n=\\left[\n\\begin{matrix}\nR{(s_1)}\n\\\\\nR{(s_2)}\n\\\\\n\\vdots \\\\\nR{(s_n)}\n\\\\\n\\end{matrix}\n\\right]+\\gamma\n\\left[\n\\begin{matrix}\nP{(s_1|s_1)}&\nP{(s_2|s_1)}&\n\\cdots&\nP{(s_n|s_1)}\\\\\nP{(s_1|s_2)}&\nP{(s_2|s_2)}&\n\\cdots&\nP{(s_n|s_2)}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nP{(s_1|s_n)}&\nP{(s_2|s_n)}&\n\\cdots&\nP{(s_n|s_n)}\\\\\n\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n\\\\\nV=R+\\gamma PV\n$$\n解析解为$V=(I-\\gamma P)^{-1}R$，实际求解方法有Dynamic Programming、Monte-Carlo、Temporal-Difference learning等。\n\n##### 1.3 马尔可夫决策过程（Markov Decision Process）\n\n与马尔可夫奖励过程相比，马尔可夫决策过程从当前状态向下一状态的转移由**动作（Action）**决定。加入动作之后，MRP中状态的奖励$R(s)$在MDP里变为$R(s,a)$。这种说法其实并不严谨，当前状态的奖励怎么会除了状态本身相关，还受下一步要采取的动作影响呢？下面会解释它们之间细微的差别。\n\n我们制定一个**策略（Policy）**来决定一系列动作\n$$\n\\pi (a|s)=P(a_t=a|s_t=s)\n$$\n如果策略确定，马尔可夫决策问题就可以转化为马尔可夫奖励问题\n$$\nP^{\\pi}(s'|s)=\\sum_{a\\in A}\\pi(a|s)P(s'|s,a)\\\\\nR^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)\n$$\n其中$P(s'|s,a)$是转移函数（Transition function）。根据2式我们可以体会到，为什么$R(s,a)$的定义是不自然的：它是为了引入动作（确切地说是策略$\\pi(a|s)$）而构造出的一个定义，可以理解为把状态s的奖励按概率$\\pi(a|s)$掰开得到的一个量。\n\n<img src=\"/images/MRPMDP.png\" alt=\"MRPMDP\" style=\"zoom:50%;\" />\n\n<center><small>MRP（左） MDP（右）</small></center>\n\n确定策略的MDP问题已经退化为MRP问题，则可以定义类似的**状态值函数**（state-value function）\n$$\nV^{\\pi}(s)=E_{\\pi}(G_t|s_t=s)\n$$\n写出Bell equation\n\n\n$$\n\\begin{aligned}\nV^{\\pi}(s)&=R^{\\pi}(s)+\\gamma \\sum_{s' \\in S}P^{\\pi}(s'|s)V^{\\pi}(s')\\\\\n&=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s'))\n\\end{aligned}\n$$\n\n我们新定义一个**动作-状态值函数**（action-value function）$q^{\\pi}(s,a)$，令\n$$\nq^{\\pi}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s')\n$$\n类似地，$q^{\\pi}(s,a)$是把状态值函数$V^{\\pi}(s)$按动作概率$\\pi(a|s)$掰开得到的，它的含义是在状态s采取动作a，之后的策略仍然确定，所能获得的回报的期望。\n\n#### 2. 策略评估\n\n之前提到，定义了价值函数让我们的目标具象了起来，强化学习是一个MDP过程，令价值函数最大，理论上就能得到一个最优的策略。所以，给定一个策略$\\pi$，计算该策略下的价值函数$V^{\\pi}$，$V^{\\pi}$的大小可以反映策略$\\pi$的优劣。这个过程叫**策略评估（Policy evaluation）**。\n\n计算价值函数的方法我们前面也有提及，最常用的做法是利用Bellman Equation迭代至收敛\n$$\nV^{\\pi}_{t+1}(s)\n=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}_{t}(s'))\n$$\n这实际上就是一种动态规划算法，从$t_0$时刻出发，迭代一次得到单步奖赏$V^{\\pi}_{t_1}$，继续迭代得到$V^{\\pi}_{t_2},V^{\\pi}_{t_3}...$，直至收敛就得到各个状态的价值函数$V^{\\pi}$。这里迭代至收敛是对应*$\\gamma$折扣累积奖励*而言的，如果是*T步累积奖励*则只需迭代T轮。\n\n#### 3. 马尔可夫决策控制\n\n现在我们知道了怎么计算单个策略对应的价值函数（策略评估），下一步就是在众多策略中找到价值函数最大的那个（最优策略）。这一步叫**马尔可夫决策控制（MDP control）**。\n\n一个最简单粗暴的方法就是穷举，把所有可能的策略都试一遍。或者写出价值函数的解析解形式，$V^{\\pi}$是状态和策略的函数，理论上可以令价值函数最大求解。但这两种思路实际几乎不可操做，下面介绍更可行的方法。\n\n##### 3.1 策略迭代（Policy Iteration）\n\n策略迭代使用贪心算法找最优策略。先给定一个初始策略$\\pi_0$，计算价值函数$V^{\\pi_0}$；下一个动作选择当前状态下的最优动作——即令动作-状态值函数$q^{\\pi_0}(s,a)$最大；这样得到了新的策略$\\pi_1=\\underset{a}{\\arg\\max}\\  q^{\\pi_0}(s,a)$，再计算价值函数$V^{\\pi_1}$，...迭代直至收敛。(注意这里使用的是deterministic policy)\n\n<img src=\"/images/policy_iter.png\" alt=\"policy_iter\" style=\"zoom:55%;\" />\n\n可以证明，这种贪心迭代的过程中价值函数是单调递增的，因为\n$$\nq^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)\n$$\n这样最终迭代收敛得到的\n$$\nV^{*}(s)=\\max_{a} q^*(s,a)\\\\\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)\n$$\n就是最大价值函数和最优策略。\n\n##### 3.2 价值迭代（Value Iteration）\n\n在策略迭代中，令动作-状态值函数最大来改进策略的过程\n$$\n\\pi_{i+1}=\\underset{a}{\\arg\\max}\\  q^{\\pi_i}(s,a)\n$$\n其实与改进值函数是一致的，因为我们选择determinist policy，所以值函数\n$$\nV^{\\pi_{i+1}}(s)=q^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)\n$$\n既然是一致的，我们就可以直接对值函数迭代，省去策略迭代里每次策略更新后都要进行的策略评估过程。用Bellman optimality equation\n$$\nV^{{i+1}}(s)=\\max_{a}q^{i}(s,a)\n$$\n迭代值函数至收敛，然后重构出最佳策略\n$$\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)\n$$\n\n\n\n\n\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[introRL-周博磊]](https://github.com/zhoubolei/introRL)*</small>\n\n","source":"_posts/2020-10-20-AI_RL.md","raw":"---\ntitle: AI学习笔记--强化学习\ndate: 2020-10-20 23:45:00\ncategories:\n- 计算机科学\ntags: \n- 人工智能\n- python\n- 算法\nmathjax: true\n---\n\n强化学习在“机器”与“环境”的交互中完成。机器对环境采取一个动作（Action），环境状态（State）会相应作出改变；环境状态的变化对应了一个奖励（Reward），来反馈给机器。强化学习要解决的问题就是：为机器找到一个最优的策略（Policy），这个策略对应的动作序列使得长期累积的奖赏最大。<!--more-->\n\n<img src=\"/images/IMG_0768.jpeg\" alt=\"IMG_0768\" style=\"zoom:50%;\" />\n\n- 监督学习：样本已标记；样本独立（IID，独立同分布）\n\n- 强化学习：延迟标记（奖励）；样本通常是序列\n\n**序列决策（Sequential Decision Making）**\n\n选择一个动作序列，使得这个动作序列获得总的奖励最大。奖励可能是即时的，也可能是延迟获得的。\n\nHistory：由观测、动作、奖励构成的序列\n$$\nH_t =O_1,R_1,A_1,...A_{t-1},O_t,R_{t}\n$$\nState：History决定了当前状态，用来决策下面的动作。\n$$\nS_t = f(H_t)\n$$\n环境状态和机器状态\n\nPolicy\n\n* Deterministic\n\n* Stochastic\n\n\n\n### \t第二章 马尔可夫决策过程（MDP）\n\n#### 1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\n\n##### 1.1 马尔可夫链（Markov Chain）\n\n下一时刻的状态只取决于当前时刻的状态，而与过去的历史状态无关，称为**马尔可夫特征**。 具有马尔可夫特征的一系列状态构成一条**马尔可夫链**。\n\n<img src=\"/images/MarkovChain.png\" alt=\"MarkovChain\" style=\"zoom:50%;\" />\n\n##### 1.2 马尔可夫奖励过程（Markov Reward Process）\n\n对不同的状态s设定**奖励**R(s)，就形成了对某些状态的偏好。\n\n给定一条马尔可夫链，从某一时刻t开始，到最大时步T结束，得到的总奖励值称为**回报（Return）**或**累积奖励**。常用的累积奖励形式有\n$$\nT步累积奖励：G_t=\\frac{1}{T}(R_{t+1}+R_{t+2}+...+R_T)\n$$\n\n$$\n\\gamma折扣累积奖励：G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...\n$$\n\n某一状态s在t时刻的**价值函数(value function)**是t时刻从该状态出发得到回报的期望\n$$\nV_t(s)=E(G_t|s_t=s)\n$$\n定义了价值函数，我们的目标就具象了起来：想要更多的回报，就要令价值函数越大越好。\n\n价值函数的计算有两种方法：\n\n* Monte Carlo采样（采样得到足够多的马尔可夫链样本，计算回报的期望）\n\n* Bellman Equation（迭代式）\n\n$$\nV(s)=R(s)+\\gamma \\sum_{s' \\in S}P(s'|s)V(s')\n$$\n\n<img src=\"/images/Bellman.png\" alt=\"Bellman\" style=\"zoom:60%;\" />\n\n这里面的$P(s'|s)$是**状态转移函数**，含义是从当前状态s向下一个状态s‘转移的概率。\n\n假设马尔可夫链无限长，即终止时间T无限大，则$V(s)=V(s')$，Bellman equation可以写成矩阵形式\n$$\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n=\\left[\n\\begin{matrix}\nR{(s_1)}\n\\\\\nR{(s_2)}\n\\\\\n\\vdots \\\\\nR{(s_n)}\n\\\\\n\\end{matrix}\n\\right]+\\gamma\n\\left[\n\\begin{matrix}\nP{(s_1|s_1)}&\nP{(s_2|s_1)}&\n\\cdots&\nP{(s_n|s_1)}\\\\\nP{(s_1|s_2)}&\nP{(s_2|s_2)}&\n\\cdots&\nP{(s_n|s_2)}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nP{(s_1|s_n)}&\nP{(s_2|s_n)}&\n\\cdots&\nP{(s_n|s_n)}\\\\\n\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n\\\\\nV=R+\\gamma PV\n$$\n解析解为$V=(I-\\gamma P)^{-1}R$，实际求解方法有Dynamic Programming、Monte-Carlo、Temporal-Difference learning等。\n\n##### 1.3 马尔可夫决策过程（Markov Decision Process）\n\n与马尔可夫奖励过程相比，马尔可夫决策过程从当前状态向下一状态的转移由**动作（Action）**决定。加入动作之后，MRP中状态的奖励$R(s)$在MDP里变为$R(s,a)$。这种说法其实并不严谨，当前状态的奖励怎么会除了状态本身相关，还受下一步要采取的动作影响呢？下面会解释它们之间细微的差别。\n\n我们制定一个**策略（Policy）**来决定一系列动作\n$$\n\\pi (a|s)=P(a_t=a|s_t=s)\n$$\n如果策略确定，马尔可夫决策问题就可以转化为马尔可夫奖励问题\n$$\nP^{\\pi}(s'|s)=\\sum_{a\\in A}\\pi(a|s)P(s'|s,a)\\\\\nR^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)\n$$\n其中$P(s'|s,a)$是转移函数（Transition function）。根据2式我们可以体会到，为什么$R(s,a)$的定义是不自然的：它是为了引入动作（确切地说是策略$\\pi(a|s)$）而构造出的一个定义，可以理解为把状态s的奖励按概率$\\pi(a|s)$掰开得到的一个量。\n\n<img src=\"/images/MRPMDP.png\" alt=\"MRPMDP\" style=\"zoom:50%;\" />\n\n<center><small>MRP（左） MDP（右）</small></center>\n\n确定策略的MDP问题已经退化为MRP问题，则可以定义类似的**状态值函数**（state-value function）\n$$\nV^{\\pi}(s)=E_{\\pi}(G_t|s_t=s)\n$$\n写出Bell equation\n\n\n$$\n\\begin{aligned}\nV^{\\pi}(s)&=R^{\\pi}(s)+\\gamma \\sum_{s' \\in S}P^{\\pi}(s'|s)V^{\\pi}(s')\\\\\n&=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s'))\n\\end{aligned}\n$$\n\n我们新定义一个**动作-状态值函数**（action-value function）$q^{\\pi}(s,a)$，令\n$$\nq^{\\pi}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s')\n$$\n类似地，$q^{\\pi}(s,a)$是把状态值函数$V^{\\pi}(s)$按动作概率$\\pi(a|s)$掰开得到的，它的含义是在状态s采取动作a，之后的策略仍然确定，所能获得的回报的期望。\n\n#### 2. 策略评估\n\n之前提到，定义了价值函数让我们的目标具象了起来，强化学习是一个MDP过程，令价值函数最大，理论上就能得到一个最优的策略。所以，给定一个策略$\\pi$，计算该策略下的价值函数$V^{\\pi}$，$V^{\\pi}$的大小可以反映策略$\\pi$的优劣。这个过程叫**策略评估（Policy evaluation）**。\n\n计算价值函数的方法我们前面也有提及，最常用的做法是利用Bellman Equation迭代至收敛\n$$\nV^{\\pi}_{t+1}(s)\n=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}_{t}(s'))\n$$\n这实际上就是一种动态规划算法，从$t_0$时刻出发，迭代一次得到单步奖赏$V^{\\pi}_{t_1}$，继续迭代得到$V^{\\pi}_{t_2},V^{\\pi}_{t_3}...$，直至收敛就得到各个状态的价值函数$V^{\\pi}$。这里迭代至收敛是对应*$\\gamma$折扣累积奖励*而言的，如果是*T步累积奖励*则只需迭代T轮。\n\n#### 3. 马尔可夫决策控制\n\n现在我们知道了怎么计算单个策略对应的价值函数（策略评估），下一步就是在众多策略中找到价值函数最大的那个（最优策略）。这一步叫**马尔可夫决策控制（MDP control）**。\n\n一个最简单粗暴的方法就是穷举，把所有可能的策略都试一遍。或者写出价值函数的解析解形式，$V^{\\pi}$是状态和策略的函数，理论上可以令价值函数最大求解。但这两种思路实际几乎不可操做，下面介绍更可行的方法。\n\n##### 3.1 策略迭代（Policy Iteration）\n\n策略迭代使用贪心算法找最优策略。先给定一个初始策略$\\pi_0$，计算价值函数$V^{\\pi_0}$；下一个动作选择当前状态下的最优动作——即令动作-状态值函数$q^{\\pi_0}(s,a)$最大；这样得到了新的策略$\\pi_1=\\underset{a}{\\arg\\max}\\  q^{\\pi_0}(s,a)$，再计算价值函数$V^{\\pi_1}$，...迭代直至收敛。(注意这里使用的是deterministic policy)\n\n<img src=\"/images/policy_iter.png\" alt=\"policy_iter\" style=\"zoom:55%;\" />\n\n可以证明，这种贪心迭代的过程中价值函数是单调递增的，因为\n$$\nq^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)\n$$\n这样最终迭代收敛得到的\n$$\nV^{*}(s)=\\max_{a} q^*(s,a)\\\\\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)\n$$\n就是最大价值函数和最优策略。\n\n##### 3.2 价值迭代（Value Iteration）\n\n在策略迭代中，令动作-状态值函数最大来改进策略的过程\n$$\n\\pi_{i+1}=\\underset{a}{\\arg\\max}\\  q^{\\pi_i}(s,a)\n$$\n其实与改进值函数是一致的，因为我们选择determinist policy，所以值函数\n$$\nV^{\\pi_{i+1}}(s)=q^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)\n$$\n既然是一致的，我们就可以直接对值函数迭代，省去策略迭代里每次策略更新后都要进行的策略评估过程。用Bellman optimality equation\n$$\nV^{{i+1}}(s)=\\max_{a}q^{i}(s,a)\n$$\n迭代值函数至收敛，然后重构出最佳策略\n$$\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)\n$$\n\n\n\n\n\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[introRL-周博磊]](https://github.com/zhoubolei/introRL)*</small>\n\n","slug":"2020-10-20-AI_RL","published":1,"updated":"2020-11-03T07:52:49.539Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5e00082sfy4nj7h0cx","content":"<p>强化学习在“机器”与“环境”的交互中完成。机器对环境采取一个动作（Action），环境状态（State）会相应作出改变；环境状态的变化对应了一个奖励（Reward），来反馈给机器。强化学习要解决的问题就是：为机器找到一个最优的策略（Policy），这个策略对应的动作序列使得长期累积的奖赏最大。<a id=\"more\"></a></p>\n<p><img src=\"/images/IMG_0768.jpeg\" alt=\"IMG_0768\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><p>监督学习：样本已标记；样本独立（IID，独立同分布）</p>\n</li>\n<li><p>强化学习：延迟标记（奖励）；样本通常是序列</p>\n</li>\n</ul>\n<p><strong>序列决策（Sequential Decision Making）</strong></p>\n<p>选择一个动作序列，使得这个动作序列获得总的奖励最大。奖励可能是即时的，也可能是延迟获得的。</p>\n<p>History：由观测、动作、奖励构成的序列</p>\n<script type=\"math/tex; mode=display\">\nH_t =O_1,R_1,A_1,...A_{t-1},O_t,R_{t}</script><p>State：History决定了当前状态，用来决策下面的动作。</p>\n<script type=\"math/tex; mode=display\">\nS_t = f(H_t)</script><p>环境状态和机器状态</p>\n<p>Policy</p>\n<ul>\n<li><p>Deterministic</p>\n</li>\n<li><p>Stochastic</p>\n</li>\n</ul>\n<h3 id=\"第二章-马尔可夫决策过程（MDP）\"><a href=\"#第二章-马尔可夫决策过程（MDP）\" class=\"headerlink\" title=\"第二章 马尔可夫决策过程（MDP）\"></a>第二章 马尔可夫决策过程（MDP）</h3><h4 id=\"1-马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\"><a href=\"#1-马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\" class=\"headerlink\" title=\"1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\"></a>1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程</h4><h5 id=\"1-1-马尔可夫链（Markov-Chain）\"><a href=\"#1-1-马尔可夫链（Markov-Chain）\" class=\"headerlink\" title=\"1.1 马尔可夫链（Markov Chain）\"></a>1.1 马尔可夫链（Markov Chain）</h5><p>下一时刻的状态只取决于当前时刻的状态，而与过去的历史状态无关，称为<strong>马尔可夫特征</strong>。 具有马尔可夫特征的一系列状态构成一条<strong>马尔可夫链</strong>。</p>\n<p><img src=\"/images/MarkovChain.png\" alt=\"MarkovChain\" style=\"zoom:50%;\" /></p>\n<h5 id=\"1-2-马尔可夫奖励过程（Markov-Reward-Process）\"><a href=\"#1-2-马尔可夫奖励过程（Markov-Reward-Process）\" class=\"headerlink\" title=\"1.2 马尔可夫奖励过程（Markov Reward Process）\"></a>1.2 马尔可夫奖励过程（Markov Reward Process）</h5><p>对不同的状态s设定<strong>奖励</strong>R(s)，就形成了对某些状态的偏好。</p>\n<p>给定一条马尔可夫链，从某一时刻t开始，到最大时步T结束，得到的总奖励值称为<strong>回报（Return）</strong>或<strong>累积奖励</strong>。常用的累积奖励形式有</p>\n<script type=\"math/tex; mode=display\">\nT步累积奖励：G_t=\\frac{1}{T}(R_{t+1}+R_{t+2}+...+R_T)</script><script type=\"math/tex; mode=display\">\n\\gamma折扣累积奖励：G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...</script><p>某一状态s在t时刻的<strong>价值函数(value function)</strong>是t时刻从该状态出发得到回报的期望</p>\n<script type=\"math/tex; mode=display\">\nV_t(s)=E(G_t|s_t=s)</script><p>定义了价值函数，我们的目标就具象了起来：想要更多的回报，就要令价值函数越大越好。</p>\n<p>价值函数的计算有两种方法：</p>\n<ul>\n<li><p>Monte Carlo采样（采样得到足够多的马尔可夫链样本，计算回报的期望）</p>\n</li>\n<li><p>Bellman Equation（迭代式）</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nV(s)=R(s)+\\gamma \\sum_{s' \\in S}P(s'|s)V(s')</script><p><img src=\"/images/Bellman.png\" alt=\"Bellman\" style=\"zoom:60%;\" /></p>\n<p>这里面的$P(s’|s)$是<strong>状态转移函数</strong>，含义是从当前状态s向下一个状态s‘转移的概率。</p>\n<p>假设马尔可夫链无限长，即终止时间T无限大，则$V(s)=V(s’)$，Bellman equation可以写成矩阵形式</p>\n<script type=\"math/tex; mode=display\">\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n=\\left[\n\\begin{matrix}\nR{(s_1)}\n\\\\\nR{(s_2)}\n\\\\\n\\vdots \\\\\nR{(s_n)}\n\\\\\n\\end{matrix}\n\\right]+\\gamma\n\\left[\n\\begin{matrix}\nP{(s_1|s_1)}&\nP{(s_2|s_1)}&\n\\cdots&\nP{(s_n|s_1)}\\\\\nP{(s_1|s_2)}&\nP{(s_2|s_2)}&\n\\cdots&\nP{(s_n|s_2)}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nP{(s_1|s_n)}&\nP{(s_2|s_n)}&\n\\cdots&\nP{(s_n|s_n)}\\\\\n\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n\\\\\nV=R+\\gamma PV</script><p>解析解为$V=(I-\\gamma P)^{-1}R$，实际求解方法有Dynamic Programming、Monte-Carlo、Temporal-Difference learning等。</p>\n<h5 id=\"1-3-马尔可夫决策过程（Markov-Decision-Process）\"><a href=\"#1-3-马尔可夫决策过程（Markov-Decision-Process）\" class=\"headerlink\" title=\"1.3 马尔可夫决策过程（Markov Decision Process）\"></a>1.3 马尔可夫决策过程（Markov Decision Process）</h5><p>与马尔可夫奖励过程相比，马尔可夫决策过程从当前状态向下一状态的转移由<strong>动作（Action）</strong>决定。加入动作之后，MRP中状态的奖励$R(s)$在MDP里变为$R(s,a)$。这种说法其实并不严谨，当前状态的奖励怎么会除了状态本身相关，还受下一步要采取的动作影响呢？下面会解释它们之间细微的差别。</p>\n<p>我们制定一个<strong>策略（Policy）</strong>来决定一系列动作</p>\n<script type=\"math/tex; mode=display\">\n\\pi (a|s)=P(a_t=a|s_t=s)</script><p>如果策略确定，马尔可夫决策问题就可以转化为马尔可夫奖励问题</p>\n<script type=\"math/tex; mode=display\">\nP^{\\pi}(s'|s)=\\sum_{a\\in A}\\pi(a|s)P(s'|s,a)\\\\\nR^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)</script><p>其中$P(s’|s,a)$是转移函数（Transition function）。根据2式我们可以体会到，为什么$R(s,a)$的定义是不自然的：它是为了引入动作（确切地说是策略$\\pi(a|s)$）而构造出的一个定义，可以理解为把状态s的奖励按概率$\\pi(a|s)$掰开得到的一个量。</p>\n<p><img src=\"/images/MRPMDP.png\" alt=\"MRPMDP\" style=\"zoom:50%;\" /></p>\n<center><small>MRP（左） MDP（右）</small></center>\n\n<p>确定策略的MDP问题已经退化为MRP问题，则可以定义类似的<strong>状态值函数</strong>（state-value function）</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi}(s)=E_{\\pi}(G_t|s_t=s)</script><p>写出Bell equation</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nV^{\\pi}(s)&=R^{\\pi}(s)+\\gamma \\sum_{s' \\in S}P^{\\pi}(s'|s)V^{\\pi}(s')\\\\\n&=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s'))\n\\end{aligned}</script><p>我们新定义一个<strong>动作-状态值函数</strong>（action-value function）$q^{\\pi}(s,a)$，令</p>\n<script type=\"math/tex; mode=display\">\nq^{\\pi}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s')</script><p>类似地，$q^{\\pi}(s,a)$是把状态值函数$V^{\\pi}(s)$按动作概率$\\pi(a|s)$掰开得到的，它的含义是在状态s采取动作a，之后的策略仍然确定，所能获得的回报的期望。</p>\n<h4 id=\"2-策略评估\"><a href=\"#2-策略评估\" class=\"headerlink\" title=\"2. 策略评估\"></a>2. 策略评估</h4><p>之前提到，定义了价值函数让我们的目标具象了起来，强化学习是一个MDP过程，令价值函数最大，理论上就能得到一个最优的策略。所以，给定一个策略$\\pi$，计算该策略下的价值函数$V^{\\pi}$，$V^{\\pi}$的大小可以反映策略$\\pi$的优劣。这个过程叫<strong>策略评估（Policy evaluation）</strong>。</p>\n<p>计算价值函数的方法我们前面也有提及，最常用的做法是利用Bellman Equation迭代至收敛</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi}_{t+1}(s)\n=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}_{t}(s'))</script><p>这实际上就是一种动态规划算法，从$t_0$时刻出发，迭代一次得到单步奖赏$V^{\\pi}_{t_1}$，继续迭代得到$V^{\\pi}_{t_2},V^{\\pi}_{t_3}…$，直至收敛就得到各个状态的价值函数$V^{\\pi}$。这里迭代至收敛是对应<em>$\\gamma$折扣累积奖励</em>而言的，如果是<em>T步累积奖励</em>则只需迭代T轮。</p>\n<h4 id=\"3-马尔可夫决策控制\"><a href=\"#3-马尔可夫决策控制\" class=\"headerlink\" title=\"3. 马尔可夫决策控制\"></a>3. 马尔可夫决策控制</h4><p>现在我们知道了怎么计算单个策略对应的价值函数（策略评估），下一步就是在众多策略中找到价值函数最大的那个（最优策略）。这一步叫<strong>马尔可夫决策控制（MDP control）</strong>。</p>\n<p>一个最简单粗暴的方法就是穷举，把所有可能的策略都试一遍。或者写出价值函数的解析解形式，$V^{\\pi}$是状态和策略的函数，理论上可以令价值函数最大求解。但这两种思路实际几乎不可操做，下面介绍更可行的方法。</p>\n<h5 id=\"3-1-策略迭代（Policy-Iteration）\"><a href=\"#3-1-策略迭代（Policy-Iteration）\" class=\"headerlink\" title=\"3.1 策略迭代（Policy Iteration）\"></a>3.1 策略迭代（Policy Iteration）</h5><p>策略迭代使用贪心算法找最优策略。先给定一个初始策略$\\pi_0$，计算价值函数$V^{\\pi_0}$；下一个动作选择当前状态下的最优动作——即令动作-状态值函数$q^{\\pi_0}(s,a)$最大；这样得到了新的策略$\\pi_1=\\underset{a}{\\arg\\max}\\  q^{\\pi_0}(s,a)$，再计算价值函数$V^{\\pi_1}$，…迭代直至收敛。(注意这里使用的是deterministic policy)</p>\n<p><img src=\"/images/policy_iter.png\" alt=\"policy_iter\" style=\"zoom:55%;\" /></p>\n<p>可以证明，这种贪心迭代的过程中价值函数是单调递增的，因为</p>\n<script type=\"math/tex; mode=display\">\nq^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)</script><p>这样最终迭代收敛得到的</p>\n<script type=\"math/tex; mode=display\">\nV^{*}(s)=\\max_{a} q^*(s,a)\\\\\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)</script><p>就是最大价值函数和最优策略。</p>\n<h5 id=\"3-2-价值迭代（Value-Iteration）\"><a href=\"#3-2-价值迭代（Value-Iteration）\" class=\"headerlink\" title=\"3.2 价值迭代（Value Iteration）\"></a>3.2 价值迭代（Value Iteration）</h5><p>在策略迭代中，令动作-状态值函数最大来改进策略的过程</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{i+1}=\\underset{a}{\\arg\\max}\\  q^{\\pi_i}(s,a)</script><p>其实与改进值函数是一致的，因为我们选择determinist policy，所以值函数</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi_{i+1}}(s)=q^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)</script><p>既然是一致的，我们就可以直接对值函数迭代，省去策略迭代里每次策略更新后都要进行的策略评估过程。用Bellman optimality equation</p>\n<script type=\"math/tex; mode=display\">\nV^NaN(s)=\\max_{a}q^{i}(s,a)</script><p>迭代值函数至收敛，然后重构出最佳策略</p>\n<script type=\"math/tex; mode=display\">\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://github.com/zhoubolei/introRL\" target=\"_blank\" rel=\"noopener\">[introRL-周博磊]</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>强化学习在“机器”与“环境”的交互中完成。机器对环境采取一个动作（Action），环境状态（State）会相应作出改变；环境状态的变化对应了一个奖励（Reward），来反馈给机器。强化学习要解决的问题就是：为机器找到一个最优的策略（Policy），这个策略对应的动作序列使得长期累积的奖赏最大。","more":"</p>\n<p><img src=\"/images/IMG_0768.jpeg\" alt=\"IMG_0768\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><p>监督学习：样本已标记；样本独立（IID，独立同分布）</p>\n</li>\n<li><p>强化学习：延迟标记（奖励）；样本通常是序列</p>\n</li>\n</ul>\n<p><strong>序列决策（Sequential Decision Making）</strong></p>\n<p>选择一个动作序列，使得这个动作序列获得总的奖励最大。奖励可能是即时的，也可能是延迟获得的。</p>\n<p>History：由观测、动作、奖励构成的序列</p>\n<script type=\"math/tex; mode=display\">\nH_t =O_1,R_1,A_1,...A_{t-1},O_t,R_{t}</script><p>State：History决定了当前状态，用来决策下面的动作。</p>\n<script type=\"math/tex; mode=display\">\nS_t = f(H_t)</script><p>环境状态和机器状态</p>\n<p>Policy</p>\n<ul>\n<li><p>Deterministic</p>\n</li>\n<li><p>Stochastic</p>\n</li>\n</ul>\n<h3 id=\"第二章-马尔可夫决策过程（MDP）\"><a href=\"#第二章-马尔可夫决策过程（MDP）\" class=\"headerlink\" title=\"第二章 马尔可夫决策过程（MDP）\"></a>第二章 马尔可夫决策过程（MDP）</h3><h4 id=\"1-马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\"><a href=\"#1-马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\" class=\"headerlink\" title=\"1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程\"></a>1. 马尔可夫链、马尔可夫奖励过程、马尔可夫决策过程</h4><h5 id=\"1-1-马尔可夫链（Markov-Chain）\"><a href=\"#1-1-马尔可夫链（Markov-Chain）\" class=\"headerlink\" title=\"1.1 马尔可夫链（Markov Chain）\"></a>1.1 马尔可夫链（Markov Chain）</h5><p>下一时刻的状态只取决于当前时刻的状态，而与过去的历史状态无关，称为<strong>马尔可夫特征</strong>。 具有马尔可夫特征的一系列状态构成一条<strong>马尔可夫链</strong>。</p>\n<p><img src=\"/images/MarkovChain.png\" alt=\"MarkovChain\" style=\"zoom:50%;\" /></p>\n<h5 id=\"1-2-马尔可夫奖励过程（Markov-Reward-Process）\"><a href=\"#1-2-马尔可夫奖励过程（Markov-Reward-Process）\" class=\"headerlink\" title=\"1.2 马尔可夫奖励过程（Markov Reward Process）\"></a>1.2 马尔可夫奖励过程（Markov Reward Process）</h5><p>对不同的状态s设定<strong>奖励</strong>R(s)，就形成了对某些状态的偏好。</p>\n<p>给定一条马尔可夫链，从某一时刻t开始，到最大时步T结束，得到的总奖励值称为<strong>回报（Return）</strong>或<strong>累积奖励</strong>。常用的累积奖励形式有</p>\n<script type=\"math/tex; mode=display\">\nT步累积奖励：G_t=\\frac{1}{T}(R_{t+1}+R_{t+2}+...+R_T)</script><script type=\"math/tex; mode=display\">\n\\gamma折扣累积奖励：G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...</script><p>某一状态s在t时刻的<strong>价值函数(value function)</strong>是t时刻从该状态出发得到回报的期望</p>\n<script type=\"math/tex; mode=display\">\nV_t(s)=E(G_t|s_t=s)</script><p>定义了价值函数，我们的目标就具象了起来：想要更多的回报，就要令价值函数越大越好。</p>\n<p>价值函数的计算有两种方法：</p>\n<ul>\n<li><p>Monte Carlo采样（采样得到足够多的马尔可夫链样本，计算回报的期望）</p>\n</li>\n<li><p>Bellman Equation（迭代式）</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nV(s)=R(s)+\\gamma \\sum_{s' \\in S}P(s'|s)V(s')</script><p><img src=\"/images/Bellman.png\" alt=\"Bellman\" style=\"zoom:60%;\" /></p>\n<p>这里面的$P(s’|s)$是<strong>状态转移函数</strong>，含义是从当前状态s向下一个状态s‘转移的概率。</p>\n<p>假设马尔可夫链无限长，即终止时间T无限大，则$V(s)=V(s’)$，Bellman equation可以写成矩阵形式</p>\n<script type=\"math/tex; mode=display\">\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n=\\left[\n\\begin{matrix}\nR{(s_1)}\n\\\\\nR{(s_2)}\n\\\\\n\\vdots \\\\\nR{(s_n)}\n\\\\\n\\end{matrix}\n\\right]+\\gamma\n\\left[\n\\begin{matrix}\nP{(s_1|s_1)}&\nP{(s_2|s_1)}&\n\\cdots&\nP{(s_n|s_1)}\\\\\nP{(s_1|s_2)}&\nP{(s_2|s_2)}&\n\\cdots&\nP{(s_n|s_2)}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nP{(s_1|s_n)}&\nP{(s_2|s_n)}&\n\\cdots&\nP{(s_n|s_n)}\\\\\n\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nV{(s_1)}\n\\\\\nV{(s_2)}\n\\\\\n\\vdots \\\\\nV{(s_n)}\n\\\\\n\\end{matrix}\n\\right]\n\\\\\nV=R+\\gamma PV</script><p>解析解为$V=(I-\\gamma P)^{-1}R$，实际求解方法有Dynamic Programming、Monte-Carlo、Temporal-Difference learning等。</p>\n<h5 id=\"1-3-马尔可夫决策过程（Markov-Decision-Process）\"><a href=\"#1-3-马尔可夫决策过程（Markov-Decision-Process）\" class=\"headerlink\" title=\"1.3 马尔可夫决策过程（Markov Decision Process）\"></a>1.3 马尔可夫决策过程（Markov Decision Process）</h5><p>与马尔可夫奖励过程相比，马尔可夫决策过程从当前状态向下一状态的转移由<strong>动作（Action）</strong>决定。加入动作之后，MRP中状态的奖励$R(s)$在MDP里变为$R(s,a)$。这种说法其实并不严谨，当前状态的奖励怎么会除了状态本身相关，还受下一步要采取的动作影响呢？下面会解释它们之间细微的差别。</p>\n<p>我们制定一个<strong>策略（Policy）</strong>来决定一系列动作</p>\n<script type=\"math/tex; mode=display\">\n\\pi (a|s)=P(a_t=a|s_t=s)</script><p>如果策略确定，马尔可夫决策问题就可以转化为马尔可夫奖励问题</p>\n<script type=\"math/tex; mode=display\">\nP^{\\pi}(s'|s)=\\sum_{a\\in A}\\pi(a|s)P(s'|s,a)\\\\\nR^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)</script><p>其中$P(s’|s,a)$是转移函数（Transition function）。根据2式我们可以体会到，为什么$R(s,a)$的定义是不自然的：它是为了引入动作（确切地说是策略$\\pi(a|s)$）而构造出的一个定义，可以理解为把状态s的奖励按概率$\\pi(a|s)$掰开得到的一个量。</p>\n<p><img src=\"/images/MRPMDP.png\" alt=\"MRPMDP\" style=\"zoom:50%;\" /></p>\n<center><small>MRP（左） MDP（右）</small></center>\n\n<p>确定策略的MDP问题已经退化为MRP问题，则可以定义类似的<strong>状态值函数</strong>（state-value function）</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi}(s)=E_{\\pi}(G_t|s_t=s)</script><p>写出Bell equation</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nV^{\\pi}(s)&=R^{\\pi}(s)+\\gamma \\sum_{s' \\in S}P^{\\pi}(s'|s)V^{\\pi}(s')\\\\\n&=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s'))\n\\end{aligned}</script><p>我们新定义一个<strong>动作-状态值函数</strong>（action-value function）$q^{\\pi}(s,a)$，令</p>\n<script type=\"math/tex; mode=display\">\nq^{\\pi}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s')</script><p>类似地，$q^{\\pi}(s,a)$是把状态值函数$V^{\\pi}(s)$按动作概率$\\pi(a|s)$掰开得到的，它的含义是在状态s采取动作a，之后的策略仍然确定，所能获得的回报的期望。</p>\n<h4 id=\"2-策略评估\"><a href=\"#2-策略评估\" class=\"headerlink\" title=\"2. 策略评估\"></a>2. 策略评估</h4><p>之前提到，定义了价值函数让我们的目标具象了起来，强化学习是一个MDP过程，令价值函数最大，理论上就能得到一个最优的策略。所以，给定一个策略$\\pi$，计算该策略下的价值函数$V^{\\pi}$，$V^{\\pi}$的大小可以反映策略$\\pi$的优劣。这个过程叫<strong>策略评估（Policy evaluation）</strong>。</p>\n<p>计算价值函数的方法我们前面也有提及，最常用的做法是利用Bellman Equation迭代至收敛</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi}_{t+1}(s)\n=\\sum_{a\\in A}\\pi(a|s)(R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}_{t}(s'))</script><p>这实际上就是一种动态规划算法，从$t_0$时刻出发，迭代一次得到单步奖赏$V^{\\pi}_{t_1}$，继续迭代得到$V^{\\pi}_{t_2},V^{\\pi}_{t_3}…$，直至收敛就得到各个状态的价值函数$V^{\\pi}$。这里迭代至收敛是对应<em>$\\gamma$折扣累积奖励</em>而言的，如果是<em>T步累积奖励</em>则只需迭代T轮。</p>\n<h4 id=\"3-马尔可夫决策控制\"><a href=\"#3-马尔可夫决策控制\" class=\"headerlink\" title=\"3. 马尔可夫决策控制\"></a>3. 马尔可夫决策控制</h4><p>现在我们知道了怎么计算单个策略对应的价值函数（策略评估），下一步就是在众多策略中找到价值函数最大的那个（最优策略）。这一步叫<strong>马尔可夫决策控制（MDP control）</strong>。</p>\n<p>一个最简单粗暴的方法就是穷举，把所有可能的策略都试一遍。或者写出价值函数的解析解形式，$V^{\\pi}$是状态和策略的函数，理论上可以令价值函数最大求解。但这两种思路实际几乎不可操做，下面介绍更可行的方法。</p>\n<h5 id=\"3-1-策略迭代（Policy-Iteration）\"><a href=\"#3-1-策略迭代（Policy-Iteration）\" class=\"headerlink\" title=\"3.1 策略迭代（Policy Iteration）\"></a>3.1 策略迭代（Policy Iteration）</h5><p>策略迭代使用贪心算法找最优策略。先给定一个初始策略$\\pi_0$，计算价值函数$V^{\\pi_0}$；下一个动作选择当前状态下的最优动作——即令动作-状态值函数$q^{\\pi_0}(s,a)$最大；这样得到了新的策略$\\pi_1=\\underset{a}{\\arg\\max}\\  q^{\\pi_0}(s,a)$，再计算价值函数$V^{\\pi_1}$，…迭代直至收敛。(注意这里使用的是deterministic policy)</p>\n<p><img src=\"/images/policy_iter.png\" alt=\"policy_iter\" style=\"zoom:55%;\" /></p>\n<p>可以证明，这种贪心迭代的过程中价值函数是单调递增的，因为</p>\n<script type=\"math/tex; mode=display\">\nq^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)</script><p>这样最终迭代收敛得到的</p>\n<script type=\"math/tex; mode=display\">\nV^{*}(s)=\\max_{a} q^*(s,a)\\\\\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)</script><p>就是最大价值函数和最优策略。</p>\n<h5 id=\"3-2-价值迭代（Value-Iteration）\"><a href=\"#3-2-价值迭代（Value-Iteration）\" class=\"headerlink\" title=\"3.2 价值迭代（Value Iteration）\"></a>3.2 价值迭代（Value Iteration）</h5><p>在策略迭代中，令动作-状态值函数最大来改进策略的过程</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{i+1}=\\underset{a}{\\arg\\max}\\  q^{\\pi_i}(s,a)</script><p>其实与改进值函数是一致的，因为我们选择determinist policy，所以值函数</p>\n<script type=\"math/tex; mode=display\">\nV^{\\pi_{i+1}}(s)=q^{\\pi_{i+1}}(s,\\pi_{i+1}(s))=\\max_{a}q^{\\pi_i}(s,a)\\geq q^{\\pi_i}(s,\\pi_i(s))=V^{\\pi_i}(s)</script><p>既然是一致的，我们就可以直接对值函数迭代，省去策略迭代里每次策略更新后都要进行的策略评估过程。用Bellman optimality equation</p>\n<script type=\"math/tex; mode=display\">\nV^NaN(s)=\\max_{a}q^{i}(s,a)</script><p>迭代值函数至收敛，然后重构出最佳策略</p>\n<script type=\"math/tex; mode=display\">\n\\pi^*=\\underset{a}{\\arg\\max}\\ q^*(s,a)</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://github.com/zhoubolei/introRL\" target=\"_blank\" rel=\"noopener\">[introRL-周博磊]</a></em></small></p>"},{"title":"拉格朗日乘子法","date":"2020-08-11T14:07:00.000Z","mathjax":true,"_content":"\n拉格朗日乘子法（Lagrange Multiplier Method）是求解带约束条件的极值问题最常见的方法之一，以大数学家Joseph Lagrange的名字命名。<!--more-->\n\n首先回忆一下几个基础的概念：法向量，方向导数，梯度。\n\n### 法向量（Normal Vector）\n\n法向量最常见到的使用场景是在三维空间，直观的几何意义就是：**垂直于一个平面的向量**。\n\n<img src=\"/images/截屏2020-08-11 下午6.09.28.png\" style=\"zoom:50%;\" />\n\n设蓝色平面上有一点A，过A点该平面的法向量为$\\omega(\\omega_1,\\omega_2,\\omega_3)$，根据法向量的定义，该平面上任意一点B与A点连线组成的直线应与$\\omega$垂直。所以有：\n$$\n\\omega^T\\vec{AB}=0 \\\\\n或 \\ \n\\omega_1(x-x_0)+\\omega_2(y-y_0)+\\omega_3(z-z_0)=0\n$$\n分量形式还可写为：\n$$\n\\omega_1x+\\omega_2y+\\omega_3z-(\\omega_1x_0+\\omega_2y_0+\\omega_3z_0)=0\n$$\n事实上，这就是一个平面的定义式。一个平面可由其法向量和离开原点的距离确定，在n维空间，一个超平面的标准定义式写为：\n$$\nf(\\textbf{x})=\\omega^T\\textbf{x}+\\textbf{b}=0\n$$\n其中${\\omega}(\\omega_1,\\omega_2...\\omega_n)$是n维法向量，决定超平面的方向，$\\textbf{b}$决定超平面离开原点的距离。\n\n<img src=\"/images/截屏2020-08-11 下午6.09.39.png\" alt=\"截屏2020-08-11 下午6.09.39\" style=\"zoom:45%;\" />\n\n对于曲面上一点，法向量为该点所在切平面的法向量。\n\n<img src=\"/images/normalxy.png\" alt=\"normalxy\" style=\"zoom:50%;\" />\n\n当维度减少到最小的二维时，一条直线的法线就是该直线的垂线，曲线上一点的法线就是该点处切线的法线。\n\n反过来，知道了一个平面的方程，也可以求得法向量。对f求偏导就可以了：\n$$\n\\omega = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})\n$$\n这里有一点需要特别提醒，法向量是一个向量，向量只有一个朝向，但其实一个法向量的反向向量也是法向量。所以$-\\omega$也是该平面的法向量。法向量总是对偶出现，一条法线，两个法向量（不考虑向量的大小）。这和梯度很不一样，下面将解释是什么造成了这种差别。\n\n### 方向导数（Directional Derivative）\n\n同样从三维空间开始来引入方向导数的概念。对于三维空间中的任一个面\n$$\nz=f(x,y)\n$$\n如下图所示\n\n<img src=\"/images/截屏2020-08-11 下午7.25.15.png\" alt=\"截屏2020-08-11 下午7.25.15\" style=\"zoom:80%;\" />\n\n方向导数的定义为**目标函数在某一方向上的变化率**。\n\n具体到我们的小黄鸭函数z=f(x,y)，z是目标函数、因变量；x,y是自变量。因为自变量只有两个维度，所以这里的“方向”一定是在二维平面上的。我们用单位向量$\\mu$来表示“某一方向”，则z=f(x,y)在$\\mu$方向上的方向导数定义为：从某一点（假设为A）出发，沿$\\mu$方向走t长度（t趋于无穷小）后，函数值z的变化。\n$$\n\\begin{aligned}\n\\frac{df(x_0,y_0)}{dt}&=\\lim_{t \\to 0}\\frac{f(x_0+tcos\\theta,y_0+tsin\\theta)-f(x_0,y_0)}{t}\\\\&=f_x(x_0,y_0)cos\\theta+f_y(x_0,y_0)sin\\theta\\\\\n&=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu\n\\end{aligned}\n$$\n其中$f_x,f_y$为f对x,y的偏导数。\n\n### 梯度（Gradient）\n\n知道了方向导数是什么，梯度的概念就顺水推舟了。梯度是一个向量，**它指向目标函数值增大最快的方向**。\n\n由方向导数$\\frac{df(x_0,y_0)}{dt}=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu$知，当$\\mu$的方向与$(f_x(x_0,y_0),f_y(x_0,y_0))$的方向相同时，方向导数最大，函数值增长最快。所以梯度的定义为\n$$\n\\nabla f=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y})\n$$\n拓展到n维，$z=f(x_1,x_2,...x_n)$\n$$\n\\nabla f=(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})\n$$\n\n\n<img src=\"/images/截屏2020-08-11 下午8.25.18.png\" alt=\"截屏2020-08-11 下午8.25.18\" style=\"zoom:80%;\" />\n\n看起来，梯度和法向量的式子是一样的？梯度就是法向量？\n\n这两个概念有一个重要的区别。同样的小黄鸭，法向量是一个三维的向量，而且一正一反有两个，而梯度却是一个二维的向量，且方向只有一个。这是为什么？关键的区别在于函数f。我们求法向量时的函数f是一个隐函数，形式为$f(x,y,z)=0$，而求梯度时f却是显函数，形式为$z=f(x,y)$。后者意味着我们有一个“目标”，根据梯度的定义，使这个“目标”更大决定了梯度只能有一个方向。而没有“目标”的法向量表示，两头都可以，只要垂直就完事了。\n\n### 拉格朗日乘子法（Lagrange Multiplier Method）\n\n#### 等式约束条件下的极值问题\n\n拉格朗日乘子法用来解决带约束条件的极值问题。在二维平面中举个简单的例子，假设我们的目标函数是f(x,y)，在约束$g(x,y)=0$的条件下，要求f(x,y)的最小值。\n$$\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0\n$$\n不妨设$f(x,y)=x²+y²$，$g(x,y)=0$是如下图的一条蓝色曲线\n\n<img src=\"/images/截屏2020-08-11 下午9.37.12.png\" alt=\"截屏2020-08-11 下午9.37.12\" style=\"zoom:60%;\" />\n\n红色的圈圈是我们目标函数$z=f(x,y)$的等高线，由约束条件，只允许点(x,y)在$g(x,y)=0$上滑动。可以直观地看出，当红圈和蓝线相切时，圆的半径最小，也就是f(x,y)最小。相切也意味着，**切点处红圈的梯度$\\nabla f$与蓝线的法向量$\\nabla g$在同一条直线上**。\n\n于是，加入了新条件的方程组为：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g &  \\\\\n            \n             g(x,y)=0&  \n             \\end{array}\n\\right.\n$$\n这里的$\\lambda$就是拉格朗日乘子。\n\n或者改写为另一种形式，构造**拉格朗日函数**$\\mathcal{L}(x,y,\\lambda)=f(x,y)+\\lambda g(x,y)$，则上面的方程组等价于\n$$\n\\nabla \\mathcal{L}=0\n$$\n写开就是\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\frac{\\partial \\mathcal{L}}{\\partial x}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial y}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=0 &    \n             \\end{array}\n\\right.\n$$\n对于有多个限制条件的情况\n$$\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0,\\ h(x,y)=0\n$$\n目标函数的梯度是约束条件法向量的线性组合：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g+\\mu \\nabla h &  \\\\\n            \n             g(x,y)=0&  \\\\\n             h(x,y)=0&\n             \\end{array}\n\\right.\n$$\n\n#### 不等式约束条件下的极值问题\n\n对于有不等式约束下的条件极值问题，为了讨论的方便，我们统一形式为：\n$$\nmin\\ f(x,y)\\\\\ns.t. h(x,y)\\leq 0\n$$\n考虑可行域$h(x,y)\\leq 0$，作用在$f(x,y)$上有两种情况。\n\n1）$f(x,y)$的无约束最优解本身就在可行域内，这时约束条件无效；\n\n2）约束条件有效，$f(x,y)$的最优解在可行域边界$h(x,y)=0$处。如下图：\n\n<img src=\"/images/截屏2020-08-12 上午12.51.00.png\" alt=\"截屏2020-08-12 上午12.51.00\" style=\"zoom:70%;\" />\n\n对于情况1），方程组可列为\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=0 &  \\\\\n            \n            \n             h(x,y)\\leq 0（可忽略）&\n             \\end{array}\n\\right.\n$$\n对于情况2），由于我们规定了不等式约束的形式为$h\\leq 0$，所以边界处的梯度$\\nabla h$总是朝向可行域外部；而目标函数$f$的梯度$\\nabla f$则朝向可行域内部（假如朝向外部，就变为了情况1）。所以当$f(x,y)$与可行域边界$h(x,y)=0$相切时，切点处$\\nabla f$和$\\nabla h$一定方向相反。\n\n这时问题转化为等式约束条件下的极值问题，只不过增加了一个条件：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\mu \\nabla h=0 &  \\\\\n            \n             h(x,y)=0&  \\\\\n             \\mu\\geq0&\n             \\end{array}\n\\right.\n$$\n综合情况1）和2），再加上等式约束，推广到多个约束条件，就得到了**KKT条件**（Karush–Kuhn–Tucker conditions）\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\sum_\\limits{i=1}^m\\lambda_i \\nabla g_i+\\sum_\\limits{j=1}^n\\mu_j \\nabla h_j=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.\n$$\n写成拉格朗日函数的形式，$\\mathcal{L}(\\mathcal{x},\\mathcal{\\lambda},\\mathcal{\\mu})=f(\\mathcal{x})+\\mathcal{\\lambda}^Tg(\\mathcal{x})+\\mathcal{\\mu}^Th(\\mathcal{x})$，第一个方程变为只对拉格朗日乘子外的自变量求偏导等于0：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla_x \\mathcal{L}=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.\n$$\n\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[直观理解梯度，以及偏导数、方向导数和法向量等]-shine-lee](https://www.cnblogs.com/shine-lee/p/11715033.html)*</small>\n\n<small>*[[如何理解拉格朗日乘子法？]-马同学](https://www.matongxue.com/madocs/939.html)*</small>\n\n","source":"_posts/2020-02-26-拉格朗日乘子法.md","raw":"---\n\ntitle: 拉格朗日乘子法\ndate: 2020-08-11 22:07:00\ncategories:\n- 最优化\ntags: \n- 数学\n- 最优化\nmathjax: true\n---\n\n拉格朗日乘子法（Lagrange Multiplier Method）是求解带约束条件的极值问题最常见的方法之一，以大数学家Joseph Lagrange的名字命名。<!--more-->\n\n首先回忆一下几个基础的概念：法向量，方向导数，梯度。\n\n### 法向量（Normal Vector）\n\n法向量最常见到的使用场景是在三维空间，直观的几何意义就是：**垂直于一个平面的向量**。\n\n<img src=\"/images/截屏2020-08-11 下午6.09.28.png\" style=\"zoom:50%;\" />\n\n设蓝色平面上有一点A，过A点该平面的法向量为$\\omega(\\omega_1,\\omega_2,\\omega_3)$，根据法向量的定义，该平面上任意一点B与A点连线组成的直线应与$\\omega$垂直。所以有：\n$$\n\\omega^T\\vec{AB}=0 \\\\\n或 \\ \n\\omega_1(x-x_0)+\\omega_2(y-y_0)+\\omega_3(z-z_0)=0\n$$\n分量形式还可写为：\n$$\n\\omega_1x+\\omega_2y+\\omega_3z-(\\omega_1x_0+\\omega_2y_0+\\omega_3z_0)=0\n$$\n事实上，这就是一个平面的定义式。一个平面可由其法向量和离开原点的距离确定，在n维空间，一个超平面的标准定义式写为：\n$$\nf(\\textbf{x})=\\omega^T\\textbf{x}+\\textbf{b}=0\n$$\n其中${\\omega}(\\omega_1,\\omega_2...\\omega_n)$是n维法向量，决定超平面的方向，$\\textbf{b}$决定超平面离开原点的距离。\n\n<img src=\"/images/截屏2020-08-11 下午6.09.39.png\" alt=\"截屏2020-08-11 下午6.09.39\" style=\"zoom:45%;\" />\n\n对于曲面上一点，法向量为该点所在切平面的法向量。\n\n<img src=\"/images/normalxy.png\" alt=\"normalxy\" style=\"zoom:50%;\" />\n\n当维度减少到最小的二维时，一条直线的法线就是该直线的垂线，曲线上一点的法线就是该点处切线的法线。\n\n反过来，知道了一个平面的方程，也可以求得法向量。对f求偏导就可以了：\n$$\n\\omega = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})\n$$\n这里有一点需要特别提醒，法向量是一个向量，向量只有一个朝向，但其实一个法向量的反向向量也是法向量。所以$-\\omega$也是该平面的法向量。法向量总是对偶出现，一条法线，两个法向量（不考虑向量的大小）。这和梯度很不一样，下面将解释是什么造成了这种差别。\n\n### 方向导数（Directional Derivative）\n\n同样从三维空间开始来引入方向导数的概念。对于三维空间中的任一个面\n$$\nz=f(x,y)\n$$\n如下图所示\n\n<img src=\"/images/截屏2020-08-11 下午7.25.15.png\" alt=\"截屏2020-08-11 下午7.25.15\" style=\"zoom:80%;\" />\n\n方向导数的定义为**目标函数在某一方向上的变化率**。\n\n具体到我们的小黄鸭函数z=f(x,y)，z是目标函数、因变量；x,y是自变量。因为自变量只有两个维度，所以这里的“方向”一定是在二维平面上的。我们用单位向量$\\mu$来表示“某一方向”，则z=f(x,y)在$\\mu$方向上的方向导数定义为：从某一点（假设为A）出发，沿$\\mu$方向走t长度（t趋于无穷小）后，函数值z的变化。\n$$\n\\begin{aligned}\n\\frac{df(x_0,y_0)}{dt}&=\\lim_{t \\to 0}\\frac{f(x_0+tcos\\theta,y_0+tsin\\theta)-f(x_0,y_0)}{t}\\\\&=f_x(x_0,y_0)cos\\theta+f_y(x_0,y_0)sin\\theta\\\\\n&=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu\n\\end{aligned}\n$$\n其中$f_x,f_y$为f对x,y的偏导数。\n\n### 梯度（Gradient）\n\n知道了方向导数是什么，梯度的概念就顺水推舟了。梯度是一个向量，**它指向目标函数值增大最快的方向**。\n\n由方向导数$\\frac{df(x_0,y_0)}{dt}=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu$知，当$\\mu$的方向与$(f_x(x_0,y_0),f_y(x_0,y_0))$的方向相同时，方向导数最大，函数值增长最快。所以梯度的定义为\n$$\n\\nabla f=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y})\n$$\n拓展到n维，$z=f(x_1,x_2,...x_n)$\n$$\n\\nabla f=(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})\n$$\n\n\n<img src=\"/images/截屏2020-08-11 下午8.25.18.png\" alt=\"截屏2020-08-11 下午8.25.18\" style=\"zoom:80%;\" />\n\n看起来，梯度和法向量的式子是一样的？梯度就是法向量？\n\n这两个概念有一个重要的区别。同样的小黄鸭，法向量是一个三维的向量，而且一正一反有两个，而梯度却是一个二维的向量，且方向只有一个。这是为什么？关键的区别在于函数f。我们求法向量时的函数f是一个隐函数，形式为$f(x,y,z)=0$，而求梯度时f却是显函数，形式为$z=f(x,y)$。后者意味着我们有一个“目标”，根据梯度的定义，使这个“目标”更大决定了梯度只能有一个方向。而没有“目标”的法向量表示，两头都可以，只要垂直就完事了。\n\n### 拉格朗日乘子法（Lagrange Multiplier Method）\n\n#### 等式约束条件下的极值问题\n\n拉格朗日乘子法用来解决带约束条件的极值问题。在二维平面中举个简单的例子，假设我们的目标函数是f(x,y)，在约束$g(x,y)=0$的条件下，要求f(x,y)的最小值。\n$$\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0\n$$\n不妨设$f(x,y)=x²+y²$，$g(x,y)=0$是如下图的一条蓝色曲线\n\n<img src=\"/images/截屏2020-08-11 下午9.37.12.png\" alt=\"截屏2020-08-11 下午9.37.12\" style=\"zoom:60%;\" />\n\n红色的圈圈是我们目标函数$z=f(x,y)$的等高线，由约束条件，只允许点(x,y)在$g(x,y)=0$上滑动。可以直观地看出，当红圈和蓝线相切时，圆的半径最小，也就是f(x,y)最小。相切也意味着，**切点处红圈的梯度$\\nabla f$与蓝线的法向量$\\nabla g$在同一条直线上**。\n\n于是，加入了新条件的方程组为：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g &  \\\\\n            \n             g(x,y)=0&  \n             \\end{array}\n\\right.\n$$\n这里的$\\lambda$就是拉格朗日乘子。\n\n或者改写为另一种形式，构造**拉格朗日函数**$\\mathcal{L}(x,y,\\lambda)=f(x,y)+\\lambda g(x,y)$，则上面的方程组等价于\n$$\n\\nabla \\mathcal{L}=0\n$$\n写开就是\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\frac{\\partial \\mathcal{L}}{\\partial x}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial y}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=0 &    \n             \\end{array}\n\\right.\n$$\n对于有多个限制条件的情况\n$$\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0,\\ h(x,y)=0\n$$\n目标函数的梯度是约束条件法向量的线性组合：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g+\\mu \\nabla h &  \\\\\n            \n             g(x,y)=0&  \\\\\n             h(x,y)=0&\n             \\end{array}\n\\right.\n$$\n\n#### 不等式约束条件下的极值问题\n\n对于有不等式约束下的条件极值问题，为了讨论的方便，我们统一形式为：\n$$\nmin\\ f(x,y)\\\\\ns.t. h(x,y)\\leq 0\n$$\n考虑可行域$h(x,y)\\leq 0$，作用在$f(x,y)$上有两种情况。\n\n1）$f(x,y)$的无约束最优解本身就在可行域内，这时约束条件无效；\n\n2）约束条件有效，$f(x,y)$的最优解在可行域边界$h(x,y)=0$处。如下图：\n\n<img src=\"/images/截屏2020-08-12 上午12.51.00.png\" alt=\"截屏2020-08-12 上午12.51.00\" style=\"zoom:70%;\" />\n\n对于情况1），方程组可列为\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=0 &  \\\\\n            \n            \n             h(x,y)\\leq 0（可忽略）&\n             \\end{array}\n\\right.\n$$\n对于情况2），由于我们规定了不等式约束的形式为$h\\leq 0$，所以边界处的梯度$\\nabla h$总是朝向可行域外部；而目标函数$f$的梯度$\\nabla f$则朝向可行域内部（假如朝向外部，就变为了情况1）。所以当$f(x,y)$与可行域边界$h(x,y)=0$相切时，切点处$\\nabla f$和$\\nabla h$一定方向相反。\n\n这时问题转化为等式约束条件下的极值问题，只不过增加了一个条件：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\mu \\nabla h=0 &  \\\\\n            \n             h(x,y)=0&  \\\\\n             \\mu\\geq0&\n             \\end{array}\n\\right.\n$$\n综合情况1）和2），再加上等式约束，推广到多个约束条件，就得到了**KKT条件**（Karush–Kuhn–Tucker conditions）\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\sum_\\limits{i=1}^m\\lambda_i \\nabla g_i+\\sum_\\limits{j=1}^n\\mu_j \\nabla h_j=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.\n$$\n写成拉格朗日函数的形式，$\\mathcal{L}(\\mathcal{x},\\mathcal{\\lambda},\\mathcal{\\mu})=f(\\mathcal{x})+\\mathcal{\\lambda}^Tg(\\mathcal{x})+\\mathcal{\\mu}^Th(\\mathcal{x})$，第一个方程变为只对拉格朗日乘子外的自变量求偏导等于0：\n$$\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla_x \\mathcal{L}=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.\n$$\n\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[[直观理解梯度，以及偏导数、方向导数和法向量等]-shine-lee](https://www.cnblogs.com/shine-lee/p/11715033.html)*</small>\n\n<small>*[[如何理解拉格朗日乘子法？]-马同学](https://www.matongxue.com/madocs/939.html)*</small>\n\n","slug":"2020-02-26-拉格朗日乘子法","published":1,"updated":"2020-11-12T02:40:15.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5f00092sfyf1bnde42","content":"<p>拉格朗日乘子法（Lagrange Multiplier Method）是求解带约束条件的极值问题最常见的方法之一，以大数学家Joseph Lagrange的名字命名。<a id=\"more\"></a></p>\n<p>首先回忆一下几个基础的概念：法向量，方向导数，梯度。</p>\n<h3 id=\"法向量（Normal-Vector）\"><a href=\"#法向量（Normal-Vector）\" class=\"headerlink\" title=\"法向量（Normal Vector）\"></a>法向量（Normal Vector）</h3><p>法向量最常见到的使用场景是在三维空间，直观的几何意义就是：<strong>垂直于一个平面的向量</strong>。</p>\n<p><img src=\"/images/截屏2020-08-11 下午6.09.28.png\" style=\"zoom:50%;\" /></p>\n<p>设蓝色平面上有一点A，过A点该平面的法向量为$\\omega(\\omega_1,\\omega_2,\\omega_3)$，根据法向量的定义，该平面上任意一点B与A点连线组成的直线应与$\\omega$垂直。所以有：</p>\n<script type=\"math/tex; mode=display\">\n\\omega^T\\vec{AB}=0 \\\\\n或 \\ \n\\omega_1(x-x_0)+\\omega_2(y-y_0)+\\omega_3(z-z_0)=0</script><p>分量形式还可写为：</p>\n<script type=\"math/tex; mode=display\">\n\\omega_1x+\\omega_2y+\\omega_3z-(\\omega_1x_0+\\omega_2y_0+\\omega_3z_0)=0</script><p>事实上，这就是一个平面的定义式。一个平面可由其法向量和离开原点的距离确定，在n维空间，一个超平面的标准定义式写为：</p>\n<script type=\"math/tex; mode=display\">\nf(\\textbf{x})=\\omega^T\\textbf{x}+\\textbf{b}=0</script><p>其中${\\omega}(\\omega_1,\\omega_2…\\omega_n)$是n维法向量，决定超平面的方向，$\\textbf{b}$决定超平面离开原点的距离。</p>\n<p><img src=\"/images/截屏2020-08-11 下午6.09.39.png\" alt=\"截屏2020-08-11 下午6.09.39\" style=\"zoom:45%;\" /></p>\n<p>对于曲面上一点，法向量为该点所在切平面的法向量。</p>\n<p><img src=\"/images/normalxy.png\" alt=\"normalxy\" style=\"zoom:50%;\" /></p>\n<p>当维度减少到最小的二维时，一条直线的法线就是该直线的垂线，曲线上一点的法线就是该点处切线的法线。</p>\n<p>反过来，知道了一个平面的方程，也可以求得法向量。对f求偏导就可以了：</p>\n<script type=\"math/tex; mode=display\">\n\\omega = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})</script><p>这里有一点需要特别提醒，法向量是一个向量，向量只有一个朝向，但其实一个法向量的反向向量也是法向量。所以$-\\omega$也是该平面的法向量。法向量总是对偶出现，一条法线，两个法向量（不考虑向量的大小）。这和梯度很不一样，下面将解释是什么造成了这种差别。</p>\n<h3 id=\"方向导数（Directional-Derivative）\"><a href=\"#方向导数（Directional-Derivative）\" class=\"headerlink\" title=\"方向导数（Directional Derivative）\"></a>方向导数（Directional Derivative）</h3><p>同样从三维空间开始来引入方向导数的概念。对于三维空间中的任一个面</p>\n<script type=\"math/tex; mode=display\">\nz=f(x,y)</script><p>如下图所示</p>\n<p><img src=\"/images/截屏2020-08-11 下午7.25.15.png\" alt=\"截屏2020-08-11 下午7.25.15\" style=\"zoom:80%;\" /></p>\n<p>方向导数的定义为<strong>目标函数在某一方向上的变化率</strong>。</p>\n<p>具体到我们的小黄鸭函数z=f(x,y)，z是目标函数、因变量；x,y是自变量。因为自变量只有两个维度，所以这里的“方向”一定是在二维平面上的。我们用单位向量$\\mu$来表示“某一方向”，则z=f(x,y)在$\\mu$方向上的方向导数定义为：从某一点（假设为A）出发，沿$\\mu$方向走t长度（t趋于无穷小）后，函数值z的变化。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{df(x_0,y_0)}{dt}&=\\lim_{t \\to 0}\\frac{f(x_0+tcos\\theta,y_0+tsin\\theta)-f(x_0,y_0)}{t}\\\\&=f_x(x_0,y_0)cos\\theta+f_y(x_0,y_0)sin\\theta\\\\\n&=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu\n\\end{aligned}</script><p>其中$f_x,f_y$为f对x,y的偏导数。</p>\n<h3 id=\"梯度（Gradient）\"><a href=\"#梯度（Gradient）\" class=\"headerlink\" title=\"梯度（Gradient）\"></a>梯度（Gradient）</h3><p>知道了方向导数是什么，梯度的概念就顺水推舟了。梯度是一个向量，<strong>它指向目标函数值增大最快的方向</strong>。</p>\n<p>由方向导数$\\frac{df(x_0,y_0)}{dt}=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu$知，当$\\mu$的方向与$(f_x(x_0,y_0),f_y(x_0,y_0))$的方向相同时，方向导数最大，函数值增长最快。所以梯度的定义为</p>\n<script type=\"math/tex; mode=display\">\n\\nabla f=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y})</script><p>拓展到n维，$z=f(x_1,x_2,…x_n)$</p>\n<script type=\"math/tex; mode=display\">\n\\nabla f=(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})</script><p><img src=\"/images/截屏2020-08-11 下午8.25.18.png\" alt=\"截屏2020-08-11 下午8.25.18\" style=\"zoom:80%;\" /></p>\n<p>看起来，梯度和法向量的式子是一样的？梯度就是法向量？</p>\n<p>这两个概念有一个重要的区别。同样的小黄鸭，法向量是一个三维的向量，而且一正一反有两个，而梯度却是一个二维的向量，且方向只有一个。这是为什么？关键的区别在于函数f。我们求法向量时的函数f是一个隐函数，形式为$f(x,y,z)=0$，而求梯度时f却是显函数，形式为$z=f(x,y)$。后者意味着我们有一个“目标”，根据梯度的定义，使这个“目标”更大决定了梯度只能有一个方向。而没有“目标”的法向量表示，两头都可以，只要垂直就完事了。</p>\n<h3 id=\"拉格朗日乘子法（Lagrange-Multiplier-Method）\"><a href=\"#拉格朗日乘子法（Lagrange-Multiplier-Method）\" class=\"headerlink\" title=\"拉格朗日乘子法（Lagrange Multiplier Method）\"></a>拉格朗日乘子法（Lagrange Multiplier Method）</h3><h4 id=\"等式约束条件下的极值问题\"><a href=\"#等式约束条件下的极值问题\" class=\"headerlink\" title=\"等式约束条件下的极值问题\"></a>等式约束条件下的极值问题</h4><p>拉格朗日乘子法用来解决带约束条件的极值问题。在二维平面中举个简单的例子，假设我们的目标函数是f(x,y)，在约束$g(x,y)=0$的条件下，要求f(x,y)的最小值。</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0</script><p>不妨设$f(x,y)=x²+y²$，$g(x,y)=0$是如下图的一条蓝色曲线</p>\n<p><img src=\"/images/截屏2020-08-11 下午9.37.12.png\" alt=\"截屏2020-08-11 下午9.37.12\" style=\"zoom:60%;\" /></p>\n<p>红色的圈圈是我们目标函数$z=f(x,y)$的等高线，由约束条件，只允许点(x,y)在$g(x,y)=0$上滑动。可以直观地看出，当红圈和蓝线相切时，圆的半径最小，也就是f(x,y)最小。相切也意味着，<strong>切点处红圈的梯度$\\nabla f$与蓝线的法向量$\\nabla g$在同一条直线上</strong>。</p>\n<p>于是，加入了新条件的方程组为：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g &  \\\\\n\n             g(x,y)=0&  \n             \\end{array}\n\\right.</script><p>这里的$\\lambda$就是拉格朗日乘子。</p>\n<p>或者改写为另一种形式，构造<strong>拉格朗日函数</strong>$\\mathcal{L}(x,y,\\lambda)=f(x,y)+\\lambda g(x,y)$，则上面的方程组等价于</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\mathcal{L}=0</script><p>写开就是</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\frac{\\partial \\mathcal{L}}{\\partial x}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial y}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=0 &    \n             \\end{array}\n\\right.</script><p>对于有多个限制条件的情况</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0,\\ h(x,y)=0</script><p>目标函数的梯度是约束条件法向量的线性组合：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g+\\mu \\nabla h &  \\\\\n\n             g(x,y)=0&  \\\\\n             h(x,y)=0&\n             \\end{array}\n\\right.</script><h4 id=\"不等式约束条件下的极值问题\"><a href=\"#不等式约束条件下的极值问题\" class=\"headerlink\" title=\"不等式约束条件下的极值问题\"></a>不等式约束条件下的极值问题</h4><p>对于有不等式约束下的条件极值问题，为了讨论的方便，我们统一形式为：</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. h(x,y)\\leq 0</script><p>考虑可行域$h(x,y)\\leq 0$，作用在$f(x,y)$上有两种情况。</p>\n<p>1）$f(x,y)$的无约束最优解本身就在可行域内，这时约束条件无效；</p>\n<p>2）约束条件有效，$f(x,y)$的最优解在可行域边界$h(x,y)=0$处。如下图：</p>\n<p><img src=\"/images/截屏2020-08-12 上午12.51.00.png\" alt=\"截屏2020-08-12 上午12.51.00\" style=\"zoom:70%;\" /></p>\n<p>对于情况1），方程组可列为</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=0 &  \\\\\n\n\n             h(x,y)\\leq 0（可忽略）&\n             \\end{array}\n\\right.</script><p>对于情况2），由于我们规定了不等式约束的形式为$h\\leq 0$，所以边界处的梯度$\\nabla h$总是朝向可行域外部；而目标函数$f$的梯度$\\nabla f$则朝向可行域内部（假如朝向外部，就变为了情况1）。所以当$f(x,y)$与可行域边界$h(x,y)=0$相切时，切点处$\\nabla f$和$\\nabla h$一定方向相反。</p>\n<p>这时问题转化为等式约束条件下的极值问题，只不过增加了一个条件：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\mu \\nabla h=0 &  \\\\\n\n             h(x,y)=0&  \\\\\n             \\mu\\geq0&\n             \\end{array}\n\\right.</script><p>综合情况1）和2），再加上等式约束，推广到多个约束条件，就得到了<strong>KKT条件</strong>（Karush–Kuhn–Tucker conditions）</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\sum_\\limits{i=1}^m\\lambda_i \\nabla g_i+\\sum_\\limits{j=1}^n\\mu_j \\nabla h_j=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.</script><p>写成拉格朗日函数的形式，$\\mathcal{L}(\\mathcal{x},\\mathcal{\\lambda},\\mathcal{\\mu})=f(\\mathcal{x})+\\mathcal{\\lambda}^Tg(\\mathcal{x})+\\mathcal{\\mu}^Th(\\mathcal{x})$，第一个方程变为只对拉格朗日乘子外的自变量求偏导等于0：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla_x \\mathcal{L}=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/shine-lee/p/11715033.html\" target=\"_blank\" rel=\"noopener\">[直观理解梯度，以及偏导数、方向导数和法向量等]-shine-lee</a></em></small></p>\n<p><small><em><a href=\"https://www.matongxue.com/madocs/939.html\" target=\"_blank\" rel=\"noopener\">[如何理解拉格朗日乘子法？]-马同学</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>拉格朗日乘子法（Lagrange Multiplier Method）是求解带约束条件的极值问题最常见的方法之一，以大数学家Joseph Lagrange的名字命名。","more":"</p>\n<p>首先回忆一下几个基础的概念：法向量，方向导数，梯度。</p>\n<h3 id=\"法向量（Normal-Vector）\"><a href=\"#法向量（Normal-Vector）\" class=\"headerlink\" title=\"法向量（Normal Vector）\"></a>法向量（Normal Vector）</h3><p>法向量最常见到的使用场景是在三维空间，直观的几何意义就是：<strong>垂直于一个平面的向量</strong>。</p>\n<p><img src=\"/images/截屏2020-08-11 下午6.09.28.png\" style=\"zoom:50%;\" /></p>\n<p>设蓝色平面上有一点A，过A点该平面的法向量为$\\omega(\\omega_1,\\omega_2,\\omega_3)$，根据法向量的定义，该平面上任意一点B与A点连线组成的直线应与$\\omega$垂直。所以有：</p>\n<script type=\"math/tex; mode=display\">\n\\omega^T\\vec{AB}=0 \\\\\n或 \\ \n\\omega_1(x-x_0)+\\omega_2(y-y_0)+\\omega_3(z-z_0)=0</script><p>分量形式还可写为：</p>\n<script type=\"math/tex; mode=display\">\n\\omega_1x+\\omega_2y+\\omega_3z-(\\omega_1x_0+\\omega_2y_0+\\omega_3z_0)=0</script><p>事实上，这就是一个平面的定义式。一个平面可由其法向量和离开原点的距离确定，在n维空间，一个超平面的标准定义式写为：</p>\n<script type=\"math/tex; mode=display\">\nf(\\textbf{x})=\\omega^T\\textbf{x}+\\textbf{b}=0</script><p>其中${\\omega}(\\omega_1,\\omega_2…\\omega_n)$是n维法向量，决定超平面的方向，$\\textbf{b}$决定超平面离开原点的距离。</p>\n<p><img src=\"/images/截屏2020-08-11 下午6.09.39.png\" alt=\"截屏2020-08-11 下午6.09.39\" style=\"zoom:45%;\" /></p>\n<p>对于曲面上一点，法向量为该点所在切平面的法向量。</p>\n<p><img src=\"/images/normalxy.png\" alt=\"normalxy\" style=\"zoom:50%;\" /></p>\n<p>当维度减少到最小的二维时，一条直线的法线就是该直线的垂线，曲线上一点的法线就是该点处切线的法线。</p>\n<p>反过来，知道了一个平面的方程，也可以求得法向量。对f求偏导就可以了：</p>\n<script type=\"math/tex; mode=display\">\n\\omega = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})</script><p>这里有一点需要特别提醒，法向量是一个向量，向量只有一个朝向，但其实一个法向量的反向向量也是法向量。所以$-\\omega$也是该平面的法向量。法向量总是对偶出现，一条法线，两个法向量（不考虑向量的大小）。这和梯度很不一样，下面将解释是什么造成了这种差别。</p>\n<h3 id=\"方向导数（Directional-Derivative）\"><a href=\"#方向导数（Directional-Derivative）\" class=\"headerlink\" title=\"方向导数（Directional Derivative）\"></a>方向导数（Directional Derivative）</h3><p>同样从三维空间开始来引入方向导数的概念。对于三维空间中的任一个面</p>\n<script type=\"math/tex; mode=display\">\nz=f(x,y)</script><p>如下图所示</p>\n<p><img src=\"/images/截屏2020-08-11 下午7.25.15.png\" alt=\"截屏2020-08-11 下午7.25.15\" style=\"zoom:80%;\" /></p>\n<p>方向导数的定义为<strong>目标函数在某一方向上的变化率</strong>。</p>\n<p>具体到我们的小黄鸭函数z=f(x,y)，z是目标函数、因变量；x,y是自变量。因为自变量只有两个维度，所以这里的“方向”一定是在二维平面上的。我们用单位向量$\\mu$来表示“某一方向”，则z=f(x,y)在$\\mu$方向上的方向导数定义为：从某一点（假设为A）出发，沿$\\mu$方向走t长度（t趋于无穷小）后，函数值z的变化。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{df(x_0,y_0)}{dt}&=\\lim_{t \\to 0}\\frac{f(x_0+tcos\\theta,y_0+tsin\\theta)-f(x_0,y_0)}{t}\\\\&=f_x(x_0,y_0)cos\\theta+f_y(x_0,y_0)sin\\theta\\\\\n&=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu\n\\end{aligned}</script><p>其中$f_x,f_y$为f对x,y的偏导数。</p>\n<h3 id=\"梯度（Gradient）\"><a href=\"#梯度（Gradient）\" class=\"headerlink\" title=\"梯度（Gradient）\"></a>梯度（Gradient）</h3><p>知道了方向导数是什么，梯度的概念就顺水推舟了。梯度是一个向量，<strong>它指向目标函数值增大最快的方向</strong>。</p>\n<p>由方向导数$\\frac{df(x_0,y_0)}{dt}=(f_x(x_0,y_0),f_y(x_0,y_0))·\\mu$知，当$\\mu$的方向与$(f_x(x_0,y_0),f_y(x_0,y_0))$的方向相同时，方向导数最大，函数值增长最快。所以梯度的定义为</p>\n<script type=\"math/tex; mode=display\">\n\\nabla f=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y})</script><p>拓展到n维，$z=f(x_1,x_2,…x_n)$</p>\n<script type=\"math/tex; mode=display\">\n\\nabla f=(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...\\frac{\\partial f}{\\partial x_n})</script><p><img src=\"/images/截屏2020-08-11 下午8.25.18.png\" alt=\"截屏2020-08-11 下午8.25.18\" style=\"zoom:80%;\" /></p>\n<p>看起来，梯度和法向量的式子是一样的？梯度就是法向量？</p>\n<p>这两个概念有一个重要的区别。同样的小黄鸭，法向量是一个三维的向量，而且一正一反有两个，而梯度却是一个二维的向量，且方向只有一个。这是为什么？关键的区别在于函数f。我们求法向量时的函数f是一个隐函数，形式为$f(x,y,z)=0$，而求梯度时f却是显函数，形式为$z=f(x,y)$。后者意味着我们有一个“目标”，根据梯度的定义，使这个“目标”更大决定了梯度只能有一个方向。而没有“目标”的法向量表示，两头都可以，只要垂直就完事了。</p>\n<h3 id=\"拉格朗日乘子法（Lagrange-Multiplier-Method）\"><a href=\"#拉格朗日乘子法（Lagrange-Multiplier-Method）\" class=\"headerlink\" title=\"拉格朗日乘子法（Lagrange Multiplier Method）\"></a>拉格朗日乘子法（Lagrange Multiplier Method）</h3><h4 id=\"等式约束条件下的极值问题\"><a href=\"#等式约束条件下的极值问题\" class=\"headerlink\" title=\"等式约束条件下的极值问题\"></a>等式约束条件下的极值问题</h4><p>拉格朗日乘子法用来解决带约束条件的极值问题。在二维平面中举个简单的例子，假设我们的目标函数是f(x,y)，在约束$g(x,y)=0$的条件下，要求f(x,y)的最小值。</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0</script><p>不妨设$f(x,y)=x²+y²$，$g(x,y)=0$是如下图的一条蓝色曲线</p>\n<p><img src=\"/images/截屏2020-08-11 下午9.37.12.png\" alt=\"截屏2020-08-11 下午9.37.12\" style=\"zoom:60%;\" /></p>\n<p>红色的圈圈是我们目标函数$z=f(x,y)$的等高线，由约束条件，只允许点(x,y)在$g(x,y)=0$上滑动。可以直观地看出，当红圈和蓝线相切时，圆的半径最小，也就是f(x,y)最小。相切也意味着，<strong>切点处红圈的梯度$\\nabla f$与蓝线的法向量$\\nabla g$在同一条直线上</strong>。</p>\n<p>于是，加入了新条件的方程组为：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g &  \\\\\n\n             g(x,y)=0&  \n             \\end{array}\n\\right.</script><p>这里的$\\lambda$就是拉格朗日乘子。</p>\n<p>或者改写为另一种形式，构造<strong>拉格朗日函数</strong>$\\mathcal{L}(x,y,\\lambda)=f(x,y)+\\lambda g(x,y)$，则上面的方程组等价于</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\mathcal{L}=0</script><p>写开就是</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\frac{\\partial \\mathcal{L}}{\\partial x}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial y}=0 &  \\\\\n             \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=0 &    \n             \\end{array}\n\\right.</script><p>对于有多个限制条件的情况</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. g(x,y)=0,\\ h(x,y)=0</script><p>目标函数的梯度是约束条件法向量的线性组合：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=\\lambda\\nabla g+\\mu \\nabla h &  \\\\\n\n             g(x,y)=0&  \\\\\n             h(x,y)=0&\n             \\end{array}\n\\right.</script><h4 id=\"不等式约束条件下的极值问题\"><a href=\"#不等式约束条件下的极值问题\" class=\"headerlink\" title=\"不等式约束条件下的极值问题\"></a>不等式约束条件下的极值问题</h4><p>对于有不等式约束下的条件极值问题，为了讨论的方便，我们统一形式为：</p>\n<script type=\"math/tex; mode=display\">\nmin\\ f(x,y)\\\\\ns.t. h(x,y)\\leq 0</script><p>考虑可行域$h(x,y)\\leq 0$，作用在$f(x,y)$上有两种情况。</p>\n<p>1）$f(x,y)$的无约束最优解本身就在可行域内，这时约束条件无效；</p>\n<p>2）约束条件有效，$f(x,y)$的最优解在可行域边界$h(x,y)=0$处。如下图：</p>\n<p><img src=\"/images/截屏2020-08-12 上午12.51.00.png\" alt=\"截屏2020-08-12 上午12.51.00\" style=\"zoom:70%;\" /></p>\n<p>对于情况1），方程组可列为</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f=0 &  \\\\\n\n\n             h(x,y)\\leq 0（可忽略）&\n             \\end{array}\n\\right.</script><p>对于情况2），由于我们规定了不等式约束的形式为$h\\leq 0$，所以边界处的梯度$\\nabla h$总是朝向可行域外部；而目标函数$f$的梯度$\\nabla f$则朝向可行域内部（假如朝向外部，就变为了情况1）。所以当$f(x,y)$与可行域边界$h(x,y)=0$相切时，切点处$\\nabla f$和$\\nabla h$一定方向相反。</p>\n<p>这时问题转化为等式约束条件下的极值问题，只不过增加了一个条件：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\mu \\nabla h=0 &  \\\\\n\n             h(x,y)=0&  \\\\\n             \\mu\\geq0&\n             \\end{array}\n\\right.</script><p>综合情况1）和2），再加上等式约束，推广到多个约束条件，就得到了<strong>KKT条件</strong>（Karush–Kuhn–Tucker conditions）</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla f+\\sum_\\limits{i=1}^m\\lambda_i \\nabla g_i+\\sum_\\limits{j=1}^n\\mu_j \\nabla h_j=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.</script><p>写成拉格朗日函数的形式，$\\mathcal{L}(\\mathcal{x},\\mathcal{\\lambda},\\mathcal{\\mu})=f(\\mathcal{x})+\\mathcal{\\lambda}^Tg(\\mathcal{x})+\\mathcal{\\mu}^Th(\\mathcal{x})$，第一个方程变为只对拉格朗日乘子外的自变量求偏导等于0：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\n             \\begin{array}{lr}\n             \\nabla_x \\mathcal{L}=0 &  \\\\\n            g_i=0&  \\\\\n             h_j\\leq0&  \\\\\n             \\mu_j\\geq0& \\\\\n             \\mu_j h_j=0& \n             \\end{array}\n\\right.</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/shine-lee/p/11715033.html\" target=\"_blank\" rel=\"noopener\">[直观理解梯度，以及偏导数、方向导数和法向量等]-shine-lee</a></em></small></p>\n<p><small><em><a href=\"https://www.matongxue.com/madocs/939.html\" target=\"_blank\" rel=\"noopener\">[如何理解拉格朗日乘子法？]-马同学</a></em></small></p>"},{"title":"AI学习笔记--Tensorflow自定义","date":"2021-06-28T07:00:01.000Z","mathjax":true,"_content":"\n\n\nTensorflow框架下自定义loss，metrics，training_loop，layer的一些实践经验。<!--more-->\n\n### 自定义损失函数（Loss）\n\n#### 1. Custom loss function with extra arguments\n\nKeras库的损失函数标准的输入参数形式是`loss(y_true, y_pred)`，面向的使用场景是最常见的预测与目标（或标签）的比较。而我希望计算损失函数时用到模型的input作为输入参数，所以需要自定义一个损失函数。\n\n在tensorflow[官方doc](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)中，给出了两种自定义损失函数的方法。一种是定义一个loss function，另一种是继承`tf.keras.losses.Loss`类。其中第二种允许输入除`(y_true, y_pred)`之外的其他参量。\n\n```python\n# custom loss function with extra arguments\nclass LocLoss(keras.losses.Loss):\n    def __init__(self, kg_raw, model_size, nobs, batch_size, name=\"custom_loss\"):\n        super().__init__(name=name)\n        # these are some extra arguments:\n        self.kg_raw = kg_raw \n        self.model_size = model_size\n        self.nobs = nobs\n        self.batch_size =  batch_size\n\n    def call(self, kg_true, loc_f):\n        k = tf.constant([1, 1, self.nobs, 1], tf.int32)\n        kg_pred = tf.math.multiply(self.kg_raw, tf.tile(tf.reshape(loc_f, [self.batch_size,self.model_size,1,1]), k))\n        mse = tf.math.reduce_mean(tf.square(kg_true - kg_pred))\n        return mse\n      \n# create a model\n# specify model structure...\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss=LocLoss(inputs,model_size,nobs,batch_size), # model inputs as extra args\n    experimental_run_tf_function=False, # neccecarry !\n)\n\n# train the model\nhistory = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n\n```\n\n模型的input是kg_raw，output是一个函数loc_f，target是kg_true，loss是$MSE(kg_{raw}*loc_f, kg_{true})$。注意为了能够让tensorflow自动求导，所有loss中的参量都要以tensorflow tensor的形式计算。\n\n在自定义LocLoss class中，`__init__`方法接受extra arguments（kg_raw, model_size, nobs, batch_size）；`call`方法计算loss，它的输入参数仍然是经典的`(y_true, y_pred)`，只不过这里的y_pred是模型输出的loc_f，计算时需要用到的extra arguments用self.*args调用。\n\n在compile model时将inputs作为输入参数，就实现了含其他参数的损失函数的自定义。这里会碰到一个问题，如果不设置**experimental_run_tf_function=False**，编译会报错（Check [this](https://github.com/tensorflow/tensorflow/issues/32142) for more information）\n\n```\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor '...' shape=(...) dtype=float32>]\n```\n\n\n\n#### 2. Weighted loss\n\n待填坑。\n\n#### 3. Load pre-trained model and predict\n\n上述含model inputs作为参数自定义loss的训练好的模型，想要保存下来并且之后用来预测。若将整个模型保存，之后load时会报错：\n\n```python\nmodel.save('/save_dir')\n\ntrained_model = tf.keras.models.load_model('/save_dir',custom_objects={'loss': LocLoss(inputs)})\n\n```\n\n解决办法是只保存模型的weights，load时也只load weights：\n\n```python\nmodel.save_weights('/save_dir')\n\nmodel.load_weights('/save_dir')\n\n```\n\n在load之前要预先定义好模型并且compile。\n\n【附】自定义损失函数的一些常见参考问题：\n\n- [Tensorflow2.0中复杂损失函数实现(custom layer + add_loss)](https://zhuanlan.zhihu.com/p/74009996)\n- [Tensorflow 2.0 Custom loss function with multiple inputs / mutiple loss](https://stackoverflow.com/questions/58022713/tensorflow-2-0-custom-loss-function-with-multiple-inputs)\n- [Tensorflow2.0与Keras搭建个性化神经网络模型](https://www.cnblogs.com/qizhou/p/13530440.html)\n- [Loading a keras model with custom loss based on input](https://stackoverflow.com/questions/59729481/loading-a-keras-model-with-custom-loss-based-on-input)\n- [Load custom loss with extra input in keras](https://stackoverflow.com/questions/57897080/load-custom-loss-with-extra-input-in-keras)\n\n<br/>\n\n### 自定义评价函数（Metrics）\n\ntensorflow官方[doc](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics)中也给出了自定义metrics的方法，同自定义loss类似，可以继承`keras.metrics.Metric`类，写好后放进`model.compile()`里编译。\n\n我自定义metrics的动机是想记录模型训练过程中gradient norm的变化，来检查最优化有没有卡住。gradient在tensorflow 2.0之后可以方便的从`GradientTape()`得到，所以需要go lower level，看看在训练时发生了什么。具体需要改写的是`keras.Model`类中的`train_step`方法，该方法决定了`model.fit()`时计算gradient, loss, metrics以及梯度下降的过程。tensorflow官方[doc](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)有详细解释。\n\n```python\n# custom metrics \nclass GradientNorm(tf.keras.metrics.Metric):\n  def __init__(self, name='gradient_norm', **kwargs):\n    super().__init__(name=name, **kwargs)\n    self.gd_norm = self.add_weight(name='gdnorm', initializer='zeros') \t# variable must be assigned by add_weight method\n\n  def update_state(self, y, gradients, sample_weight=None):\n    norm = tf.norm(gradients, ord='euclidean') / tf.cast(tf.size(gradients, out_type=tf.int32), tf.float32)\n    self.gd_norm.assign(norm) # variable must be updated by add_weight method\n\n  def result(self):\n    return self.gd_norm\n\n  def reset_states(self):\n    self.gd_norm.assign(0)\n \n# custom what happened during training by overriding train_step()\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics\n        self.compiled_metrics.reset_states()\n        self.compiled_metrics.update_state(y, gradients)\n\n        # Return a dict mapping metric names to current value\n        return_metrics = {m.name: m.result() for m in self.metrics}\n        return return_metrics\n\n# create a model\n# specify model structure...\nmodel = CustomModel(inputs=inputs, outputs=outputs)\n\n# compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss=LocLoss(inputs,model_size,nobs,batch_size), # model inputs as extra args\n    experimental_run_tf_function=False, # neccecarry !\n    metrics=[GradientNorm()], # custom metrics\n)\n\n```\n\n这种方法是通过compiled_loss, compiled_metrics计算和更新。如果想要添加多个metrics，而且不同的metrics有不同的输入参量（上面的方法compiled_metrics固定输入参量只能是(y, gradients)），则需要下面的方法，这种方法不在`model.compile()`时指定loss和metrics。\n\n```python\ngd_norm = GradientNorm()\n# PSNR is another custom metric, which takes (y_true, y_pred) as input\npsnr_metric = PSNR() \nloss_tracker = keras.metrics.Mean(name=\"loss\")\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            loss = keras.losses.mean_squared_error(y, y_pred)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        gd_norm.update_state(y, gradients)\n        psnr_metric.update_state(y, y_pred)\n        loss_tracker.update_state(loss)\n\n        # Return a dict mapping metric names to current value\n        return_metrics = {m.name: m.result() for m in self.metrics}\n        return return_metrics\n\n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [loss_tracker, gd_norm, psnr_metric]\n\n```\n\n不过这种办法只在tensorflow 2.2以上版本支持，否则会报错（check [this](https://github.com/tensorflow/tensorflow/issues/40041) for more info）\n\n```\nValueError: The model cannot be compiled because it has no loss to optimize.\n```\n\n<br/>\n\n### 自定义训练过程（Training Loop)\n\n\n\n<br/>\n\n### 自定义层（Layer）\n\n在tensorflow自定义layer有两种选择：继承Layer类或继承Lambda类。这两种都属于`tensorflow.keras.layers`类。\n\ntensorflow官方更推荐继承Layer类而不是Lambda类。原因可参考[官方文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda)，和[这篇](https://github.com/stellargraph/stellargraph/issues/709)。通常来说，只有非常简单的操作才推荐用Lambda层，有两个原因，一是因为Lambda层在save model和load model时需要相同的环境配置，这使得模型没那么便携；另一个是用了Lambda层的model在debug的时候很不方便。\n\n下面是一个继承Layer类编写自定义层的例子。\n\n```python\nfrom tensorflow.keras import layers\n\nclass Loc_by_1D(layers.Layer):\n        def __init__(self, obs_dens, model_size, nobs, batch_size, **kwargs):\n            super(Loc_by_1D, self).__init__(**kwargs)\n            self.obs_dens = obs_dens\n            self.model_size = model_size\n            self.nobs = nobs\n            self.batch_size = batch_size\n\n        def rotate(self, matrix, shifts):\n            \"\"\"\"here are some codes\"\"\"\n            \n        def call(self, inputs):\n            kg_f = tf.squeeze(inputs[0]) # multiple inputs as a list\n            loc_f = tf.squeeze(inputs[1]) # multiple inputs as a list\n\n\t\t\t\t\t\t\"\"\"some operations on inputs\"\"\"\n            return kg_pred\n          \n        def compute_output_shape(self, input_shape):\n            shape = list(input_shape)\n            assert len(shape) == 2\n            return tuple([shape[0], self.model_size, self.nobs])\n\n        def get_config(self):\n            config = {'obs_dens': self.obs_dens, 'model_size': self.model_size,\n                      'nobs': self.nobs, 'batch_size': self.batch_size}\n            base_config = super(Loc_by_1D, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n          \n# the last layer of my keras model\noutputs = Loc_by_1D(obs_dens=obs_dens, model_size=model_size, nobs=nobs, batch_size=kwargs['batch_size'])([inputs, x5]) # here the inputs is a list [inputs, x5]\n\n```\n\n这里我自定义了一个叫Loc_by_1D的层，其中包含几个主要的函数\n\n- \\__init\\__用来初始化，指定一些参数；\n- call里一般写的是这个layer的核心功能。当输入给call function的inputs不只一个时，最好用list的形式传递参数（如例子中那样），参考[这篇](https://stackoverflow.com/questions/61891181/how-to-use-multiple-inputs-in-tensorflow-2-x-keras-custom-layer)。\n- compute_output_shape用来计算该层输出张量的shape，不是必须的，除了输出shape是动态的情况，都会自动计算；\n- get_config用来检查model的时候改层有哪些指定参数。\n\n在自定义的layer里，各种操作也只能用tensorflow的函数库，比如`tf.squeeze()`对应numpy里的`np.squeeze()`，一般来说，numpy里一些简单常用的操作都能在tensorflow找到对应，只不过操作的对象不再是`np.array`而是`tensor`，后者是tensorflow里最常用的数组变量。\n\n<br/>\n\n<br/>\n\n<br/>","source":"_posts/20200528_ai_custom_tensorflow.md","raw":"---\ntitle: AI学习笔记--Tensorflow自定义\ndate: 2021-06-28 15:00:01\ncategories:\n- 计算机科学\ntags: \n- python\n- tensorflow\n- 人工智能\nmathjax: true\n---\n\n\n\nTensorflow框架下自定义loss，metrics，training_loop，layer的一些实践经验。<!--more-->\n\n### 自定义损失函数（Loss）\n\n#### 1. Custom loss function with extra arguments\n\nKeras库的损失函数标准的输入参数形式是`loss(y_true, y_pred)`，面向的使用场景是最常见的预测与目标（或标签）的比较。而我希望计算损失函数时用到模型的input作为输入参数，所以需要自定义一个损失函数。\n\n在tensorflow[官方doc](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)中，给出了两种自定义损失函数的方法。一种是定义一个loss function，另一种是继承`tf.keras.losses.Loss`类。其中第二种允许输入除`(y_true, y_pred)`之外的其他参量。\n\n```python\n# custom loss function with extra arguments\nclass LocLoss(keras.losses.Loss):\n    def __init__(self, kg_raw, model_size, nobs, batch_size, name=\"custom_loss\"):\n        super().__init__(name=name)\n        # these are some extra arguments:\n        self.kg_raw = kg_raw \n        self.model_size = model_size\n        self.nobs = nobs\n        self.batch_size =  batch_size\n\n    def call(self, kg_true, loc_f):\n        k = tf.constant([1, 1, self.nobs, 1], tf.int32)\n        kg_pred = tf.math.multiply(self.kg_raw, tf.tile(tf.reshape(loc_f, [self.batch_size,self.model_size,1,1]), k))\n        mse = tf.math.reduce_mean(tf.square(kg_true - kg_pred))\n        return mse\n      \n# create a model\n# specify model structure...\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss=LocLoss(inputs,model_size,nobs,batch_size), # model inputs as extra args\n    experimental_run_tf_function=False, # neccecarry !\n)\n\n# train the model\nhistory = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n\n```\n\n模型的input是kg_raw，output是一个函数loc_f，target是kg_true，loss是$MSE(kg_{raw}*loc_f, kg_{true})$。注意为了能够让tensorflow自动求导，所有loss中的参量都要以tensorflow tensor的形式计算。\n\n在自定义LocLoss class中，`__init__`方法接受extra arguments（kg_raw, model_size, nobs, batch_size）；`call`方法计算loss，它的输入参数仍然是经典的`(y_true, y_pred)`，只不过这里的y_pred是模型输出的loc_f，计算时需要用到的extra arguments用self.*args调用。\n\n在compile model时将inputs作为输入参数，就实现了含其他参数的损失函数的自定义。这里会碰到一个问题，如果不设置**experimental_run_tf_function=False**，编译会报错（Check [this](https://github.com/tensorflow/tensorflow/issues/32142) for more information）\n\n```\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor '...' shape=(...) dtype=float32>]\n```\n\n\n\n#### 2. Weighted loss\n\n待填坑。\n\n#### 3. Load pre-trained model and predict\n\n上述含model inputs作为参数自定义loss的训练好的模型，想要保存下来并且之后用来预测。若将整个模型保存，之后load时会报错：\n\n```python\nmodel.save('/save_dir')\n\ntrained_model = tf.keras.models.load_model('/save_dir',custom_objects={'loss': LocLoss(inputs)})\n\n```\n\n解决办法是只保存模型的weights，load时也只load weights：\n\n```python\nmodel.save_weights('/save_dir')\n\nmodel.load_weights('/save_dir')\n\n```\n\n在load之前要预先定义好模型并且compile。\n\n【附】自定义损失函数的一些常见参考问题：\n\n- [Tensorflow2.0中复杂损失函数实现(custom layer + add_loss)](https://zhuanlan.zhihu.com/p/74009996)\n- [Tensorflow 2.0 Custom loss function with multiple inputs / mutiple loss](https://stackoverflow.com/questions/58022713/tensorflow-2-0-custom-loss-function-with-multiple-inputs)\n- [Tensorflow2.0与Keras搭建个性化神经网络模型](https://www.cnblogs.com/qizhou/p/13530440.html)\n- [Loading a keras model with custom loss based on input](https://stackoverflow.com/questions/59729481/loading-a-keras-model-with-custom-loss-based-on-input)\n- [Load custom loss with extra input in keras](https://stackoverflow.com/questions/57897080/load-custom-loss-with-extra-input-in-keras)\n\n<br/>\n\n### 自定义评价函数（Metrics）\n\ntensorflow官方[doc](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics)中也给出了自定义metrics的方法，同自定义loss类似，可以继承`keras.metrics.Metric`类，写好后放进`model.compile()`里编译。\n\n我自定义metrics的动机是想记录模型训练过程中gradient norm的变化，来检查最优化有没有卡住。gradient在tensorflow 2.0之后可以方便的从`GradientTape()`得到，所以需要go lower level，看看在训练时发生了什么。具体需要改写的是`keras.Model`类中的`train_step`方法，该方法决定了`model.fit()`时计算gradient, loss, metrics以及梯度下降的过程。tensorflow官方[doc](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)有详细解释。\n\n```python\n# custom metrics \nclass GradientNorm(tf.keras.metrics.Metric):\n  def __init__(self, name='gradient_norm', **kwargs):\n    super().__init__(name=name, **kwargs)\n    self.gd_norm = self.add_weight(name='gdnorm', initializer='zeros') \t# variable must be assigned by add_weight method\n\n  def update_state(self, y, gradients, sample_weight=None):\n    norm = tf.norm(gradients, ord='euclidean') / tf.cast(tf.size(gradients, out_type=tf.int32), tf.float32)\n    self.gd_norm.assign(norm) # variable must be updated by add_weight method\n\n  def result(self):\n    return self.gd_norm\n\n  def reset_states(self):\n    self.gd_norm.assign(0)\n \n# custom what happened during training by overriding train_step()\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics\n        self.compiled_metrics.reset_states()\n        self.compiled_metrics.update_state(y, gradients)\n\n        # Return a dict mapping metric names to current value\n        return_metrics = {m.name: m.result() for m in self.metrics}\n        return return_metrics\n\n# create a model\n# specify model structure...\nmodel = CustomModel(inputs=inputs, outputs=outputs)\n\n# compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss=LocLoss(inputs,model_size,nobs,batch_size), # model inputs as extra args\n    experimental_run_tf_function=False, # neccecarry !\n    metrics=[GradientNorm()], # custom metrics\n)\n\n```\n\n这种方法是通过compiled_loss, compiled_metrics计算和更新。如果想要添加多个metrics，而且不同的metrics有不同的输入参量（上面的方法compiled_metrics固定输入参量只能是(y, gradients)），则需要下面的方法，这种方法不在`model.compile()`时指定loss和metrics。\n\n```python\ngd_norm = GradientNorm()\n# PSNR is another custom metric, which takes (y_true, y_pred) as input\npsnr_metric = PSNR() \nloss_tracker = keras.metrics.Mean(name=\"loss\")\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            loss = keras.losses.mean_squared_error(y, y_pred)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        gd_norm.update_state(y, gradients)\n        psnr_metric.update_state(y, y_pred)\n        loss_tracker.update_state(loss)\n\n        # Return a dict mapping metric names to current value\n        return_metrics = {m.name: m.result() for m in self.metrics}\n        return return_metrics\n\n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [loss_tracker, gd_norm, psnr_metric]\n\n```\n\n不过这种办法只在tensorflow 2.2以上版本支持，否则会报错（check [this](https://github.com/tensorflow/tensorflow/issues/40041) for more info）\n\n```\nValueError: The model cannot be compiled because it has no loss to optimize.\n```\n\n<br/>\n\n### 自定义训练过程（Training Loop)\n\n\n\n<br/>\n\n### 自定义层（Layer）\n\n在tensorflow自定义layer有两种选择：继承Layer类或继承Lambda类。这两种都属于`tensorflow.keras.layers`类。\n\ntensorflow官方更推荐继承Layer类而不是Lambda类。原因可参考[官方文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda)，和[这篇](https://github.com/stellargraph/stellargraph/issues/709)。通常来说，只有非常简单的操作才推荐用Lambda层，有两个原因，一是因为Lambda层在save model和load model时需要相同的环境配置，这使得模型没那么便携；另一个是用了Lambda层的model在debug的时候很不方便。\n\n下面是一个继承Layer类编写自定义层的例子。\n\n```python\nfrom tensorflow.keras import layers\n\nclass Loc_by_1D(layers.Layer):\n        def __init__(self, obs_dens, model_size, nobs, batch_size, **kwargs):\n            super(Loc_by_1D, self).__init__(**kwargs)\n            self.obs_dens = obs_dens\n            self.model_size = model_size\n            self.nobs = nobs\n            self.batch_size = batch_size\n\n        def rotate(self, matrix, shifts):\n            \"\"\"\"here are some codes\"\"\"\n            \n        def call(self, inputs):\n            kg_f = tf.squeeze(inputs[0]) # multiple inputs as a list\n            loc_f = tf.squeeze(inputs[1]) # multiple inputs as a list\n\n\t\t\t\t\t\t\"\"\"some operations on inputs\"\"\"\n            return kg_pred\n          \n        def compute_output_shape(self, input_shape):\n            shape = list(input_shape)\n            assert len(shape) == 2\n            return tuple([shape[0], self.model_size, self.nobs])\n\n        def get_config(self):\n            config = {'obs_dens': self.obs_dens, 'model_size': self.model_size,\n                      'nobs': self.nobs, 'batch_size': self.batch_size}\n            base_config = super(Loc_by_1D, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n          \n# the last layer of my keras model\noutputs = Loc_by_1D(obs_dens=obs_dens, model_size=model_size, nobs=nobs, batch_size=kwargs['batch_size'])([inputs, x5]) # here the inputs is a list [inputs, x5]\n\n```\n\n这里我自定义了一个叫Loc_by_1D的层，其中包含几个主要的函数\n\n- \\__init\\__用来初始化，指定一些参数；\n- call里一般写的是这个layer的核心功能。当输入给call function的inputs不只一个时，最好用list的形式传递参数（如例子中那样），参考[这篇](https://stackoverflow.com/questions/61891181/how-to-use-multiple-inputs-in-tensorflow-2-x-keras-custom-layer)。\n- compute_output_shape用来计算该层输出张量的shape，不是必须的，除了输出shape是动态的情况，都会自动计算；\n- get_config用来检查model的时候改层有哪些指定参数。\n\n在自定义的layer里，各种操作也只能用tensorflow的函数库，比如`tf.squeeze()`对应numpy里的`np.squeeze()`，一般来说，numpy里一些简单常用的操作都能在tensorflow找到对应，只不过操作的对象不再是`np.array`而是`tensor`，后者是tensorflow里最常用的数组变量。\n\n<br/>\n\n<br/>\n\n<br/>","slug":"20200528_ai_custom_tensorflow","published":1,"updated":"2022-01-10T04:26:10.539Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5h000c2sfy3dt1bo5l","content":"<p>Tensorflow框架下自定义loss，metrics，training_loop，layer的一些实践经验。<a id=\"more\"></a></p>\n<h3 id=\"自定义损失函数（Loss）\"><a href=\"#自定义损失函数（Loss）\" class=\"headerlink\" title=\"自定义损失函数（Loss）\"></a>自定义损失函数（Loss）</h3><h4 id=\"1-Custom-loss-function-with-extra-arguments\"><a href=\"#1-Custom-loss-function-with-extra-arguments\" class=\"headerlink\" title=\"1. Custom loss function with extra arguments\"></a>1. Custom loss function with extra arguments</h4><p>Keras库的损失函数标准的输入参数形式是<code>loss(y_true, y_pred)</code>，面向的使用场景是最常见的预测与目标（或标签）的比较。而我希望计算损失函数时用到模型的input作为输入参数，所以需要自定义一个损失函数。</p>\n<p>在tensorflow<a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses\" target=\"_blank\" rel=\"noopener\">官方doc</a>中，给出了两种自定义损失函数的方法。一种是定义一个loss function，另一种是继承<code>tf.keras.losses.Loss</code>类。其中第二种允许输入除<code>(y_true, y_pred)</code>之外的其他参量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># custom loss function with extra arguments</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LocLoss</span><span class=\"params\">(keras.losses.Loss)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, kg_raw, model_size, nobs, batch_size, name=<span class=\"string\">\"custom_loss\"</span>)</span>:</span></span><br><span class=\"line\">        super().__init__(name=name)</span><br><span class=\"line\">        <span class=\"comment\"># these are some extra arguments:</span></span><br><span class=\"line\">        self.kg_raw = kg_raw </span><br><span class=\"line\">        self.model_size = model_size</span><br><span class=\"line\">        self.nobs = nobs</span><br><span class=\"line\">        self.batch_size =  batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, kg_true, loc_f)</span>:</span></span><br><span class=\"line\">        k = tf.constant([<span class=\"number\">1</span>, <span class=\"number\">1</span>, self.nobs, <span class=\"number\">1</span>], tf.int32)</span><br><span class=\"line\">        kg_pred = tf.math.multiply(self.kg_raw, tf.tile(tf.reshape(loc_f, [self.batch_size,self.model_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>]), k))</span><br><span class=\"line\">        mse = tf.math.reduce_mean(tf.square(kg_true - kg_pred))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mse</span><br><span class=\"line\">      </span><br><span class=\"line\"><span class=\"comment\"># create a model</span></span><br><span class=\"line\"><span class=\"comment\"># specify model structure...</span></span><br><span class=\"line\">model = keras.Model(inputs=inputs, outputs=outputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the model</span></span><br><span class=\"line\">model.compile(</span><br><span class=\"line\">    optimizer=keras.optimizers.Adam(learning_rate=<span class=\"number\">0.0001</span>),</span><br><span class=\"line\">    loss=LocLoss(inputs,model_size,nobs,batch_size), <span class=\"comment\"># model inputs as extra args</span></span><br><span class=\"line\">    experimental_run_tf_function=<span class=\"literal\">False</span>, <span class=\"comment\"># neccecarry !</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train the model</span></span><br><span class=\"line\">history = model.fit(train_dataset, epochs=<span class=\"number\">10</span>, validation_data=test_dataset)</span><br></pre></td></tr></table></figure>\n<p>模型的input是kg_raw，output是一个函数loc_f，target是kg_true，loss是$MSE(kg_{raw}*loc_f, kg_{true})$。注意为了能够让tensorflow自动求导，所有loss中的参量都要以tensorflow tensor的形式计算。</p>\n<p>在自定义LocLoss class中，<code>__init__</code>方法接受extra arguments（kg_raw, model_size, nobs, batch_size）；<code>call</code>方法计算loss，它的输入参数仍然是经典的<code>(y_true, y_pred)</code>，只不过这里的y_pred是模型输出的loc_f，计算时需要用到的extra arguments用self.*args调用。</p>\n<p>在compile model时将inputs作为输入参数，就实现了含其他参数的损失函数的自定义。这里会碰到一个问题，如果不设置<strong>experimental_run_tf_function=False</strong>，编译会报错（Check <a href=\"https://github.com/tensorflow/tensorflow/issues/32142\" target=\"_blank\" rel=\"noopener\">this</a> for more information）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor &#39;...&#39; shape&#x3D;(...) dtype&#x3D;float32&gt;]</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-Weighted-loss\"><a href=\"#2-Weighted-loss\" class=\"headerlink\" title=\"2. Weighted loss\"></a>2. Weighted loss</h4><p>待填坑。</p>\n<h4 id=\"3-Load-pre-trained-model-and-predict\"><a href=\"#3-Load-pre-trained-model-and-predict\" class=\"headerlink\" title=\"3. Load pre-trained model and predict\"></a>3. Load pre-trained model and predict</h4><p>上述含model inputs作为参数自定义loss的训练好的模型，想要保存下来并且之后用来预测。若将整个模型保存，之后load时会报错：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.save(<span class=\"string\">'/save_dir'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">trained_model = tf.keras.models.load_model(<span class=\"string\">'/save_dir'</span>,custom_objects=&#123;<span class=\"string\">'loss'</span>: LocLoss(inputs)&#125;)</span><br></pre></td></tr></table></figure>\n<p>解决办法是只保存模型的weights，load时也只load weights：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.save_weights(<span class=\"string\">'/save_dir'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model.load_weights(<span class=\"string\">'/save_dir'</span>)</span><br></pre></td></tr></table></figure>\n<p>在load之前要预先定义好模型并且compile。</p>\n<p>【附】自定义损失函数的一些常见参考问题：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/74009996\" target=\"_blank\" rel=\"noopener\">Tensorflow2.0中复杂损失函数实现(custom layer + add_loss)</a></li>\n<li><a href=\"https://stackoverflow.com/questions/58022713/tensorflow-2-0-custom-loss-function-with-multiple-inputs\" target=\"_blank\" rel=\"noopener\">Tensorflow 2.0 Custom loss function with multiple inputs / mutiple loss</a></li>\n<li><a href=\"https://www.cnblogs.com/qizhou/p/13530440.html\" target=\"_blank\" rel=\"noopener\">Tensorflow2.0与Keras搭建个性化神经网络模型</a></li>\n<li><a href=\"https://stackoverflow.com/questions/59729481/loading-a-keras-model-with-custom-loss-based-on-input\" target=\"_blank\" rel=\"noopener\">Loading a keras model with custom loss based on input</a></li>\n<li><a href=\"https://stackoverflow.com/questions/57897080/load-custom-loss-with-extra-input-in-keras\" target=\"_blank\" rel=\"noopener\">Load custom loss with extra input in keras</a></li>\n</ul>\n<p><br/></p>\n<h3 id=\"自定义评价函数（Metrics）\"><a href=\"#自定义评价函数（Metrics）\" class=\"headerlink\" title=\"自定义评价函数（Metrics）\"></a>自定义评价函数（Metrics）</h3><p>tensorflow官方<a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics\" target=\"_blank\" rel=\"noopener\">doc</a>中也给出了自定义metrics的方法，同自定义loss类似，可以继承<code>keras.metrics.Metric</code>类，写好后放进<code>model.compile()</code>里编译。</p>\n<p>我自定义metrics的动机是想记录模型训练过程中gradient norm的变化，来检查最优化有没有卡住。gradient在tensorflow 2.0之后可以方便的从<code>GradientTape()</code>得到，所以需要go lower level，看看在训练时发生了什么。具体需要改写的是<code>keras.Model</code>类中的<code>train_step</code>方法，该方法决定了<code>model.fit()</code>时计算gradient, loss, metrics以及梯度下降的过程。tensorflow官方<a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\" target=\"_blank\" rel=\"noopener\">doc</a>有详细解释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># custom metrics </span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GradientNorm</span><span class=\"params\">(tf.keras.metrics.Metric)</span>:</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, name=<span class=\"string\">'gradient_norm'</span>, **kwargs)</span>:</span></span><br><span class=\"line\">    super().__init__(name=name, **kwargs)</span><br><span class=\"line\">    self.gd_norm = self.add_weight(name=<span class=\"string\">'gdnorm'</span>, initializer=<span class=\"string\">'zeros'</span>) \t<span class=\"comment\"># variable must be assigned by add_weight method</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_state</span><span class=\"params\">(self, y, gradients, sample_weight=None)</span>:</span></span><br><span class=\"line\">    norm = tf.norm(gradients, ord=<span class=\"string\">'euclidean'</span>) / tf.cast(tf.size(gradients, out_type=tf.int32), tf.float32)</span><br><span class=\"line\">    self.gd_norm.assign(norm) <span class=\"comment\"># variable must be updated by add_weight method</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">result</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self.gd_norm</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset_states</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">    self.gd_norm.assign(<span class=\"number\">0</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># custom what happened during training by overriding train_step()</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomModel</span><span class=\"params\">(keras.Model)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_step</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Unpack the data. Its structure depends on your model and</span></span><br><span class=\"line\">        <span class=\"comment\"># on what you pass to `fit()`.</span></span><br><span class=\"line\">        x, y = data</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">            y_pred = self(x, training=<span class=\"literal\">True</span>)  <span class=\"comment\"># Forward pass</span></span><br><span class=\"line\">            <span class=\"comment\"># Compute the loss value</span></span><br><span class=\"line\">            <span class=\"comment\"># (the loss function is configured in `compile()`)</span></span><br><span class=\"line\">            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Compute gradients</span></span><br><span class=\"line\">        trainable_vars = self.trainable_variables</span><br><span class=\"line\">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update weights</span></span><br><span class=\"line\">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update metrics</span></span><br><span class=\"line\">        self.compiled_metrics.reset_states()</span><br><span class=\"line\">        self.compiled_metrics.update_state(y, gradients)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Return a dict mapping metric names to current value</span></span><br><span class=\"line\">        return_metrics = &#123;m.name: m.result() <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.metrics&#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> return_metrics</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># create a model</span></span><br><span class=\"line\"><span class=\"comment\"># specify model structure...</span></span><br><span class=\"line\">model = CustomModel(inputs=inputs, outputs=outputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the model</span></span><br><span class=\"line\">model.compile(</span><br><span class=\"line\">    optimizer=keras.optimizers.Adam(learning_rate=<span class=\"number\">0.0001</span>),</span><br><span class=\"line\">    loss=LocLoss(inputs,model_size,nobs,batch_size), <span class=\"comment\"># model inputs as extra args</span></span><br><span class=\"line\">    experimental_run_tf_function=<span class=\"literal\">False</span>, <span class=\"comment\"># neccecarry !</span></span><br><span class=\"line\">    metrics=[GradientNorm()], <span class=\"comment\"># custom metrics</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>这种方法是通过compiled_loss, compiled_metrics计算和更新。如果想要添加多个metrics，而且不同的metrics有不同的输入参量（上面的方法compiled_metrics固定输入参量只能是(y, gradients)），则需要下面的方法，这种方法不在<code>model.compile()</code>时指定loss和metrics。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gd_norm = GradientNorm()</span><br><span class=\"line\"><span class=\"comment\"># PSNR is another custom metric, which takes (y_true, y_pred) as input</span></span><br><span class=\"line\">psnr_metric = PSNR() </span><br><span class=\"line\">loss_tracker = keras.metrics.Mean(name=<span class=\"string\">\"loss\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomModel</span><span class=\"params\">(keras.Model)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_step</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Unpack the data. Its structure depends on your model and</span></span><br><span class=\"line\">        <span class=\"comment\"># on what you pass to `fit()`.</span></span><br><span class=\"line\">        x, y = data</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">            y_pred = self(x, training=<span class=\"literal\">True</span>)  <span class=\"comment\"># Forward pass</span></span><br><span class=\"line\">            <span class=\"comment\"># Compute the loss value</span></span><br><span class=\"line\">            loss = keras.losses.mean_squared_error(y, y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Compute gradients</span></span><br><span class=\"line\">        trainable_vars = self.trainable_variables</span><br><span class=\"line\">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update weights</span></span><br><span class=\"line\">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update metrics (includes the metric that tracks the loss)</span></span><br><span class=\"line\">        gd_norm.update_state(y, gradients)</span><br><span class=\"line\">        psnr_metric.update_state(y, y_pred)</span><br><span class=\"line\">        loss_tracker.update_state(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Return a dict mapping metric names to current value</span></span><br><span class=\"line\">        return_metrics = &#123;m.name: m.result() <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.metrics&#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> return_metrics</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">metrics</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># We list our `Metric` objects here so that `reset_states()` can be</span></span><br><span class=\"line\">        <span class=\"comment\"># called automatically at the start of each epoch</span></span><br><span class=\"line\">        <span class=\"comment\"># or at the start of `evaluate()`.</span></span><br><span class=\"line\">        <span class=\"comment\"># If you don't implement this property, you have to call</span></span><br><span class=\"line\">        <span class=\"comment\"># `reset_states()` yourself at the time of your choosing.</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [loss_tracker, gd_norm, psnr_metric]</span><br></pre></td></tr></table></figure>\n<p>不过这种办法只在tensorflow 2.2以上版本支持，否则会报错（check <a href=\"https://github.com/tensorflow/tensorflow/issues/40041\" target=\"_blank\" rel=\"noopener\">this</a> for more info）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ValueError: The model cannot be compiled because it has no loss to optimize.</span><br></pre></td></tr></table></figure>\n<p><br/></p>\n<h3 id=\"自定义训练过程（Training-Loop\"><a href=\"#自定义训练过程（Training-Loop\" class=\"headerlink\" title=\"自定义训练过程（Training Loop)\"></a>自定义训练过程（Training Loop)</h3><p><br/></p>\n<h3 id=\"自定义层（Layer）\"><a href=\"#自定义层（Layer）\" class=\"headerlink\" title=\"自定义层（Layer）\"></a>自定义层（Layer）</h3><p>在tensorflow自定义layer有两种选择：继承Layer类或继承Lambda类。这两种都属于<code>tensorflow.keras.layers</code>类。</p>\n<p>tensorflow官方更推荐继承Layer类而不是Lambda类。原因可参考<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\" target=\"_blank\" rel=\"noopener\">官方文档</a>，和<a href=\"https://github.com/stellargraph/stellargraph/issues/709\" target=\"_blank\" rel=\"noopener\">这篇</a>。通常来说，只有非常简单的操作才推荐用Lambda层，有两个原因，一是因为Lambda层在save model和load model时需要相同的环境配置，这使得模型没那么便携；另一个是用了Lambda层的model在debug的时候很不方便。</p>\n<p>下面是一个继承Layer类编写自定义层的例子。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras <span class=\"keyword\">import</span> layers</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Loc_by_1D</span><span class=\"params\">(layers.Layer)</span>:</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, obs_dens, model_size, nobs, batch_size, **kwargs)</span>:</span></span><br><span class=\"line\">            super(Loc_by_1D, self).__init__(**kwargs)</span><br><span class=\"line\">            self.obs_dens = obs_dens</span><br><span class=\"line\">            self.model_size = model_size</span><br><span class=\"line\">            self.nobs = nobs</span><br><span class=\"line\">            self.batch_size = batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rotate</span><span class=\"params\">(self, matrix, shifts)</span>:</span></span><br><span class=\"line\">            <span class=\"string\">\"\"\"\"here are some codes\"\"\"</span></span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, inputs)</span>:</span></span><br><span class=\"line\">            kg_f = tf.squeeze(inputs[<span class=\"number\">0</span>]) <span class=\"comment\"># multiple inputs as a list</span></span><br><span class=\"line\">            loc_f = tf.squeeze(inputs[<span class=\"number\">1</span>]) <span class=\"comment\"># multiple inputs as a list</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"string\">\"\"\"some operations on inputs\"\"\"</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> kg_pred</span><br><span class=\"line\">          </span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">            shape = list(input_shape)</span><br><span class=\"line\">            <span class=\"keyword\">assert</span> len(shape) == <span class=\"number\">2</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> tuple([shape[<span class=\"number\">0</span>], self.model_size, self.nobs])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_config</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">            config = &#123;<span class=\"string\">'obs_dens'</span>: self.obs_dens, <span class=\"string\">'model_size'</span>: self.model_size,</span><br><span class=\"line\">                      <span class=\"string\">'nobs'</span>: self.nobs, <span class=\"string\">'batch_size'</span>: self.batch_size&#125;</span><br><span class=\"line\">            base_config = super(Loc_by_1D, self).get_config()</span><br><span class=\"line\">            <span class=\"keyword\">return</span> dict(list(base_config.items()) + list(config.items()))</span><br><span class=\"line\">          </span><br><span class=\"line\"><span class=\"comment\"># the last layer of my keras model</span></span><br><span class=\"line\">outputs = Loc_by_1D(obs_dens=obs_dens, model_size=model_size, nobs=nobs, batch_size=kwargs[<span class=\"string\">'batch_size'</span>])([inputs, x5]) <span class=\"comment\"># here the inputs is a list [inputs, x5]</span></span><br></pre></td></tr></table></figure>\n<p>这里我自定义了一个叫Loc_by_1D的层，其中包含几个主要的函数</p>\n<ul>\n<li>__init__用来初始化，指定一些参数；</li>\n<li>call里一般写的是这个layer的核心功能。当输入给call function的inputs不只一个时，最好用list的形式传递参数（如例子中那样），参考<a href=\"https://stackoverflow.com/questions/61891181/how-to-use-multiple-inputs-in-tensorflow-2-x-keras-custom-layer\" target=\"_blank\" rel=\"noopener\">这篇</a>。</li>\n<li>compute_output_shape用来计算该层输出张量的shape，不是必须的，除了输出shape是动态的情况，都会自动计算；</li>\n<li>get_config用来检查model的时候改层有哪些指定参数。</li>\n</ul>\n<p>在自定义的layer里，各种操作也只能用tensorflow的函数库，比如<code>tf.squeeze()</code>对应numpy里的<code>np.squeeze()</code>，一般来说，numpy里一些简单常用的操作都能在tensorflow找到对应，只不过操作的对象不再是<code>np.array</code>而是<code>tensor</code>，后者是tensorflow里最常用的数组变量。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n","site":{"data":{}},"excerpt":"<p>Tensorflow框架下自定义loss，metrics，training_loop，layer的一些实践经验。","more":"</p>\n<h3 id=\"自定义损失函数（Loss）\"><a href=\"#自定义损失函数（Loss）\" class=\"headerlink\" title=\"自定义损失函数（Loss）\"></a>自定义损失函数（Loss）</h3><h4 id=\"1-Custom-loss-function-with-extra-arguments\"><a href=\"#1-Custom-loss-function-with-extra-arguments\" class=\"headerlink\" title=\"1. Custom loss function with extra arguments\"></a>1. Custom loss function with extra arguments</h4><p>Keras库的损失函数标准的输入参数形式是<code>loss(y_true, y_pred)</code>，面向的使用场景是最常见的预测与目标（或标签）的比较。而我希望计算损失函数时用到模型的input作为输入参数，所以需要自定义一个损失函数。</p>\n<p>在tensorflow<a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses\" target=\"_blank\" rel=\"noopener\">官方doc</a>中，给出了两种自定义损失函数的方法。一种是定义一个loss function，另一种是继承<code>tf.keras.losses.Loss</code>类。其中第二种允许输入除<code>(y_true, y_pred)</code>之外的其他参量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># custom loss function with extra arguments</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LocLoss</span><span class=\"params\">(keras.losses.Loss)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, kg_raw, model_size, nobs, batch_size, name=<span class=\"string\">\"custom_loss\"</span>)</span>:</span></span><br><span class=\"line\">        super().__init__(name=name)</span><br><span class=\"line\">        <span class=\"comment\"># these are some extra arguments:</span></span><br><span class=\"line\">        self.kg_raw = kg_raw </span><br><span class=\"line\">        self.model_size = model_size</span><br><span class=\"line\">        self.nobs = nobs</span><br><span class=\"line\">        self.batch_size =  batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, kg_true, loc_f)</span>:</span></span><br><span class=\"line\">        k = tf.constant([<span class=\"number\">1</span>, <span class=\"number\">1</span>, self.nobs, <span class=\"number\">1</span>], tf.int32)</span><br><span class=\"line\">        kg_pred = tf.math.multiply(self.kg_raw, tf.tile(tf.reshape(loc_f, [self.batch_size,self.model_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>]), k))</span><br><span class=\"line\">        mse = tf.math.reduce_mean(tf.square(kg_true - kg_pred))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mse</span><br><span class=\"line\">      </span><br><span class=\"line\"><span class=\"comment\"># create a model</span></span><br><span class=\"line\"><span class=\"comment\"># specify model structure...</span></span><br><span class=\"line\">model = keras.Model(inputs=inputs, outputs=outputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the model</span></span><br><span class=\"line\">model.compile(</span><br><span class=\"line\">    optimizer=keras.optimizers.Adam(learning_rate=<span class=\"number\">0.0001</span>),</span><br><span class=\"line\">    loss=LocLoss(inputs,model_size,nobs,batch_size), <span class=\"comment\"># model inputs as extra args</span></span><br><span class=\"line\">    experimental_run_tf_function=<span class=\"literal\">False</span>, <span class=\"comment\"># neccecarry !</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train the model</span></span><br><span class=\"line\">history = model.fit(train_dataset, epochs=<span class=\"number\">10</span>, validation_data=test_dataset)</span><br></pre></td></tr></table></figure>\n<p>模型的input是kg_raw，output是一个函数loc_f，target是kg_true，loss是$MSE(kg_{raw}*loc_f, kg_{true})$。注意为了能够让tensorflow自动求导，所有loss中的参量都要以tensorflow tensor的形式计算。</p>\n<p>在自定义LocLoss class中，<code>__init__</code>方法接受extra arguments（kg_raw, model_size, nobs, batch_size）；<code>call</code>方法计算loss，它的输入参数仍然是经典的<code>(y_true, y_pred)</code>，只不过这里的y_pred是模型输出的loc_f，计算时需要用到的extra arguments用self.*args调用。</p>\n<p>在compile model时将inputs作为输入参数，就实现了含其他参数的损失函数的自定义。这里会碰到一个问题，如果不设置<strong>experimental_run_tf_function=False</strong>，编译会报错（Check <a href=\"https://github.com/tensorflow/tensorflow/issues/32142\" target=\"_blank\" rel=\"noopener\">this</a> for more information）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor &#39;...&#39; shape&#x3D;(...) dtype&#x3D;float32&gt;]</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-Weighted-loss\"><a href=\"#2-Weighted-loss\" class=\"headerlink\" title=\"2. Weighted loss\"></a>2. Weighted loss</h4><p>待填坑。</p>\n<h4 id=\"3-Load-pre-trained-model-and-predict\"><a href=\"#3-Load-pre-trained-model-and-predict\" class=\"headerlink\" title=\"3. Load pre-trained model and predict\"></a>3. Load pre-trained model and predict</h4><p>上述含model inputs作为参数自定义loss的训练好的模型，想要保存下来并且之后用来预测。若将整个模型保存，之后load时会报错：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.save(<span class=\"string\">'/save_dir'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">trained_model = tf.keras.models.load_model(<span class=\"string\">'/save_dir'</span>,custom_objects=&#123;<span class=\"string\">'loss'</span>: LocLoss(inputs)&#125;)</span><br></pre></td></tr></table></figure>\n<p>解决办法是只保存模型的weights，load时也只load weights：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.save_weights(<span class=\"string\">'/save_dir'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model.load_weights(<span class=\"string\">'/save_dir'</span>)</span><br></pre></td></tr></table></figure>\n<p>在load之前要预先定义好模型并且compile。</p>\n<p>【附】自定义损失函数的一些常见参考问题：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/74009996\" target=\"_blank\" rel=\"noopener\">Tensorflow2.0中复杂损失函数实现(custom layer + add_loss)</a></li>\n<li><a href=\"https://stackoverflow.com/questions/58022713/tensorflow-2-0-custom-loss-function-with-multiple-inputs\" target=\"_blank\" rel=\"noopener\">Tensorflow 2.0 Custom loss function with multiple inputs / mutiple loss</a></li>\n<li><a href=\"https://www.cnblogs.com/qizhou/p/13530440.html\" target=\"_blank\" rel=\"noopener\">Tensorflow2.0与Keras搭建个性化神经网络模型</a></li>\n<li><a href=\"https://stackoverflow.com/questions/59729481/loading-a-keras-model-with-custom-loss-based-on-input\" target=\"_blank\" rel=\"noopener\">Loading a keras model with custom loss based on input</a></li>\n<li><a href=\"https://stackoverflow.com/questions/57897080/load-custom-loss-with-extra-input-in-keras\" target=\"_blank\" rel=\"noopener\">Load custom loss with extra input in keras</a></li>\n</ul>\n<p><br/></p>\n<h3 id=\"自定义评价函数（Metrics）\"><a href=\"#自定义评价函数（Metrics）\" class=\"headerlink\" title=\"自定义评价函数（Metrics）\"></a>自定义评价函数（Metrics）</h3><p>tensorflow官方<a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics\" target=\"_blank\" rel=\"noopener\">doc</a>中也给出了自定义metrics的方法，同自定义loss类似，可以继承<code>keras.metrics.Metric</code>类，写好后放进<code>model.compile()</code>里编译。</p>\n<p>我自定义metrics的动机是想记录模型训练过程中gradient norm的变化，来检查最优化有没有卡住。gradient在tensorflow 2.0之后可以方便的从<code>GradientTape()</code>得到，所以需要go lower level，看看在训练时发生了什么。具体需要改写的是<code>keras.Model</code>类中的<code>train_step</code>方法，该方法决定了<code>model.fit()</code>时计算gradient, loss, metrics以及梯度下降的过程。tensorflow官方<a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\" target=\"_blank\" rel=\"noopener\">doc</a>有详细解释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># custom metrics </span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GradientNorm</span><span class=\"params\">(tf.keras.metrics.Metric)</span>:</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, name=<span class=\"string\">'gradient_norm'</span>, **kwargs)</span>:</span></span><br><span class=\"line\">    super().__init__(name=name, **kwargs)</span><br><span class=\"line\">    self.gd_norm = self.add_weight(name=<span class=\"string\">'gdnorm'</span>, initializer=<span class=\"string\">'zeros'</span>) \t<span class=\"comment\"># variable must be assigned by add_weight method</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_state</span><span class=\"params\">(self, y, gradients, sample_weight=None)</span>:</span></span><br><span class=\"line\">    norm = tf.norm(gradients, ord=<span class=\"string\">'euclidean'</span>) / tf.cast(tf.size(gradients, out_type=tf.int32), tf.float32)</span><br><span class=\"line\">    self.gd_norm.assign(norm) <span class=\"comment\"># variable must be updated by add_weight method</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">result</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self.gd_norm</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset_states</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">    self.gd_norm.assign(<span class=\"number\">0</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># custom what happened during training by overriding train_step()</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomModel</span><span class=\"params\">(keras.Model)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_step</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Unpack the data. Its structure depends on your model and</span></span><br><span class=\"line\">        <span class=\"comment\"># on what you pass to `fit()`.</span></span><br><span class=\"line\">        x, y = data</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">            y_pred = self(x, training=<span class=\"literal\">True</span>)  <span class=\"comment\"># Forward pass</span></span><br><span class=\"line\">            <span class=\"comment\"># Compute the loss value</span></span><br><span class=\"line\">            <span class=\"comment\"># (the loss function is configured in `compile()`)</span></span><br><span class=\"line\">            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Compute gradients</span></span><br><span class=\"line\">        trainable_vars = self.trainable_variables</span><br><span class=\"line\">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update weights</span></span><br><span class=\"line\">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update metrics</span></span><br><span class=\"line\">        self.compiled_metrics.reset_states()</span><br><span class=\"line\">        self.compiled_metrics.update_state(y, gradients)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Return a dict mapping metric names to current value</span></span><br><span class=\"line\">        return_metrics = &#123;m.name: m.result() <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.metrics&#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> return_metrics</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># create a model</span></span><br><span class=\"line\"><span class=\"comment\"># specify model structure...</span></span><br><span class=\"line\">model = CustomModel(inputs=inputs, outputs=outputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the model</span></span><br><span class=\"line\">model.compile(</span><br><span class=\"line\">    optimizer=keras.optimizers.Adam(learning_rate=<span class=\"number\">0.0001</span>),</span><br><span class=\"line\">    loss=LocLoss(inputs,model_size,nobs,batch_size), <span class=\"comment\"># model inputs as extra args</span></span><br><span class=\"line\">    experimental_run_tf_function=<span class=\"literal\">False</span>, <span class=\"comment\"># neccecarry !</span></span><br><span class=\"line\">    metrics=[GradientNorm()], <span class=\"comment\"># custom metrics</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>这种方法是通过compiled_loss, compiled_metrics计算和更新。如果想要添加多个metrics，而且不同的metrics有不同的输入参量（上面的方法compiled_metrics固定输入参量只能是(y, gradients)），则需要下面的方法，这种方法不在<code>model.compile()</code>时指定loss和metrics。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gd_norm = GradientNorm()</span><br><span class=\"line\"><span class=\"comment\"># PSNR is another custom metric, which takes (y_true, y_pred) as input</span></span><br><span class=\"line\">psnr_metric = PSNR() </span><br><span class=\"line\">loss_tracker = keras.metrics.Mean(name=<span class=\"string\">\"loss\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomModel</span><span class=\"params\">(keras.Model)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_step</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Unpack the data. Its structure depends on your model and</span></span><br><span class=\"line\">        <span class=\"comment\"># on what you pass to `fit()`.</span></span><br><span class=\"line\">        x, y = data</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">            y_pred = self(x, training=<span class=\"literal\">True</span>)  <span class=\"comment\"># Forward pass</span></span><br><span class=\"line\">            <span class=\"comment\"># Compute the loss value</span></span><br><span class=\"line\">            loss = keras.losses.mean_squared_error(y, y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Compute gradients</span></span><br><span class=\"line\">        trainable_vars = self.trainable_variables</span><br><span class=\"line\">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update weights</span></span><br><span class=\"line\">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update metrics (includes the metric that tracks the loss)</span></span><br><span class=\"line\">        gd_norm.update_state(y, gradients)</span><br><span class=\"line\">        psnr_metric.update_state(y, y_pred)</span><br><span class=\"line\">        loss_tracker.update_state(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Return a dict mapping metric names to current value</span></span><br><span class=\"line\">        return_metrics = &#123;m.name: m.result() <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.metrics&#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> return_metrics</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">metrics</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># We list our `Metric` objects here so that `reset_states()` can be</span></span><br><span class=\"line\">        <span class=\"comment\"># called automatically at the start of each epoch</span></span><br><span class=\"line\">        <span class=\"comment\"># or at the start of `evaluate()`.</span></span><br><span class=\"line\">        <span class=\"comment\"># If you don't implement this property, you have to call</span></span><br><span class=\"line\">        <span class=\"comment\"># `reset_states()` yourself at the time of your choosing.</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [loss_tracker, gd_norm, psnr_metric]</span><br></pre></td></tr></table></figure>\n<p>不过这种办法只在tensorflow 2.2以上版本支持，否则会报错（check <a href=\"https://github.com/tensorflow/tensorflow/issues/40041\" target=\"_blank\" rel=\"noopener\">this</a> for more info）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ValueError: The model cannot be compiled because it has no loss to optimize.</span><br></pre></td></tr></table></figure>\n<p><br/></p>\n<h3 id=\"自定义训练过程（Training-Loop\"><a href=\"#自定义训练过程（Training-Loop\" class=\"headerlink\" title=\"自定义训练过程（Training Loop)\"></a>自定义训练过程（Training Loop)</h3><p><br/></p>\n<h3 id=\"自定义层（Layer）\"><a href=\"#自定义层（Layer）\" class=\"headerlink\" title=\"自定义层（Layer）\"></a>自定义层（Layer）</h3><p>在tensorflow自定义layer有两种选择：继承Layer类或继承Lambda类。这两种都属于<code>tensorflow.keras.layers</code>类。</p>\n<p>tensorflow官方更推荐继承Layer类而不是Lambda类。原因可参考<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\" target=\"_blank\" rel=\"noopener\">官方文档</a>，和<a href=\"https://github.com/stellargraph/stellargraph/issues/709\" target=\"_blank\" rel=\"noopener\">这篇</a>。通常来说，只有非常简单的操作才推荐用Lambda层，有两个原因，一是因为Lambda层在save model和load model时需要相同的环境配置，这使得模型没那么便携；另一个是用了Lambda层的model在debug的时候很不方便。</p>\n<p>下面是一个继承Layer类编写自定义层的例子。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras <span class=\"keyword\">import</span> layers</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Loc_by_1D</span><span class=\"params\">(layers.Layer)</span>:</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, obs_dens, model_size, nobs, batch_size, **kwargs)</span>:</span></span><br><span class=\"line\">            super(Loc_by_1D, self).__init__(**kwargs)</span><br><span class=\"line\">            self.obs_dens = obs_dens</span><br><span class=\"line\">            self.model_size = model_size</span><br><span class=\"line\">            self.nobs = nobs</span><br><span class=\"line\">            self.batch_size = batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rotate</span><span class=\"params\">(self, matrix, shifts)</span>:</span></span><br><span class=\"line\">            <span class=\"string\">\"\"\"\"here are some codes\"\"\"</span></span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, inputs)</span>:</span></span><br><span class=\"line\">            kg_f = tf.squeeze(inputs[<span class=\"number\">0</span>]) <span class=\"comment\"># multiple inputs as a list</span></span><br><span class=\"line\">            loc_f = tf.squeeze(inputs[<span class=\"number\">1</span>]) <span class=\"comment\"># multiple inputs as a list</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"string\">\"\"\"some operations on inputs\"\"\"</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> kg_pred</span><br><span class=\"line\">          </span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">            shape = list(input_shape)</span><br><span class=\"line\">            <span class=\"keyword\">assert</span> len(shape) == <span class=\"number\">2</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> tuple([shape[<span class=\"number\">0</span>], self.model_size, self.nobs])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_config</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">            config = &#123;<span class=\"string\">'obs_dens'</span>: self.obs_dens, <span class=\"string\">'model_size'</span>: self.model_size,</span><br><span class=\"line\">                      <span class=\"string\">'nobs'</span>: self.nobs, <span class=\"string\">'batch_size'</span>: self.batch_size&#125;</span><br><span class=\"line\">            base_config = super(Loc_by_1D, self).get_config()</span><br><span class=\"line\">            <span class=\"keyword\">return</span> dict(list(base_config.items()) + list(config.items()))</span><br><span class=\"line\">          </span><br><span class=\"line\"><span class=\"comment\"># the last layer of my keras model</span></span><br><span class=\"line\">outputs = Loc_by_1D(obs_dens=obs_dens, model_size=model_size, nobs=nobs, batch_size=kwargs[<span class=\"string\">'batch_size'</span>])([inputs, x5]) <span class=\"comment\"># here the inputs is a list [inputs, x5]</span></span><br></pre></td></tr></table></figure>\n<p>这里我自定义了一个叫Loc_by_1D的层，其中包含几个主要的函数</p>\n<ul>\n<li>__init__用来初始化，指定一些参数；</li>\n<li>call里一般写的是这个layer的核心功能。当输入给call function的inputs不只一个时，最好用list的形式传递参数（如例子中那样），参考<a href=\"https://stackoverflow.com/questions/61891181/how-to-use-multiple-inputs-in-tensorflow-2-x-keras-custom-layer\" target=\"_blank\" rel=\"noopener\">这篇</a>。</li>\n<li>compute_output_shape用来计算该层输出张量的shape，不是必须的，除了输出shape是动态的情况，都会自动计算；</li>\n<li>get_config用来检查model的时候改层有哪些指定参数。</li>\n</ul>\n<p>在自定义的layer里，各种操作也只能用tensorflow的函数库，比如<code>tf.squeeze()</code>对应numpy里的<code>np.squeeze()</code>，一般来说，numpy里一些简单常用的操作都能在tensorflow找到对应，只不过操作的对象不再是<code>np.array</code>而是<code>tensor</code>，后者是tensorflow里最常用的数组变量。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>"},{"title":"AI学习笔记--Numba+GPU加速","date":"2020-11-21T07:18:00.000Z","_content":"\n使用Numba + GPU加速你的python程序。<!--more-->\n\n<br/>\n\n### Numba加速器\n\n我把一段matlab代码重写迁移到python之后，运行速度慢了近100倍。经过代码分析，找到最耗时间的是python里的for循环。这可能与python语言本身的运行效率有关系，python是一门解释型的高级语言，在python被操作系统理解之前，要先转化为pyc字节码给python虚拟机，通过虚拟机与硬件操作系统交互。相比于一些更底层的语言，python的设计使得它更容易被程序员理解和使用，但trade-off是对机器来说更不容易理解，运行效率降低了。\n\nNumba就是为了解决python计算效率低的痛点开发的JIT（just-in-time）编译器。简单来说，Numba把一部分python代码编译成机器语言，只需在第一次调用时编译一次，之后再调用都能以机器语言的速度运行。使用Numba之后，我的同一段的代码运算速度和matlab相近甚至更快了。\n\n#### Numba安装 \n\n用conda直接装\n\n```python\n$ conda install numba\n```\n\n\n\n#### 在CPU上使用Numba\n\n使用Numba只需要把需要加速的那部分代码封装成function，在function前加一个`@jit`装饰器（decorator）就行了。是的，就这么简单。\n\n```python\nfrom numba import jit\n\n@jit\ndef f(x, y):\n    return x + y\n```\n\n\n\n需要注意的是，并不是所有的python对象Numba都支持，**尽量只加速最耗时的那部分代码**，这样既能避免代码中包含Numba不支持的python类型导致报错，又可以给代码瘦身提高编译速度。详见[官网的说明](https://numba.pydata.org/numba-doc/dev/user/troubleshoot.html#numba-troubleshooting)。\n\n编译有两个模式可选：object mode和nopython mode。后者默认你对要加速的代码充分了解，不包含Numba不支持的python类型，直接编译成机器语言。否则会直接报错。而object mode则不会，遇到不能编的就退出JIT，按python原生的来。所以**想要发挥Numba最好的性能，尽量使用nopython mode**。\n\n```python\n@jit(nopython=True)\ndef f(x, y):\n    return x + y\n```\n\n\n\nCpu parallel\n\n\n\nVectorize\n\n### Numba+GPU并行\n\n#### GPU与并行\n\n\n\n#### Numba使用GPU\n\n##### - 简单并行\n\n\n\n##### - 多维并行\n\n并行适合处理互相独立的运算，如果是对同一个数的循环累加，用单个线程代替一次循环可能会出问题。比如，我要并行化的原代码是一个两层嵌套循环，其中外层相互独立，内层是对同一个数的累加\n\n```python\ndef calx(x, ss2, model_size, a, zwrap, smooth_steps):\n    for i in range(ss2, ss2 + model_size):\n        x[i - ss2, 0] = a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n        for j in range(- smooth_steps + 1, smooth_steps):\n            x[i - ss2, 0] = x[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n        x[i - ss2, 0] = x[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n    return x\n```\n\n假如我把两层循环都并行化，这里我令block的维度是2*smooth_steps，用block里的一个thread计算一次内循环；令grid的维度是model_size，用grid里的每个block计算一次外循环。\n\n```python\n@cuda.jit\ndef gpu_calx(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps):\n    a = a_device\n    zwrap = zwrap_device\n    i = cuda.blockIdx.x + ss2\n    j = cuda.threadIdx.x - smooth_steps\n\n    if j == (- smooth_steps):\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n    elif j == smooth_steps:\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n    else:\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n```\n\n输出cpu的calx函数和gpu的gpu_calx函数计算结果的前三个数，发现两者不相同，有1e-2的差异\n\n```python\ncalx_result[0:3]=\n[[0.23222521]\n [0.42714764]\n [0.63248611]] \ngpu_result[0:3]=\n[[0.17172628]\n [0.36771521]\n [0.5903021 ]]\n```\n\n或许可以这样来解释：用并行的线程取代累加循环时，每个线程并不关心其他线程是否计算完成。假设线程a从内存中取出x_device[i - ss2, 0]执行加法运算时，可能同时有线程b也在执行相同的操作，而两者取出的x_device[i - ss2, 0]是同一值。这样两个线程执行完加法返回值的时候，其实相当于只执行了一次加法运算，线程a或b的其中一个运算是无效的。所以造成了最后结果的差异。\n\n于是，我只能放弃并行内层的累加循环\n\n```python\n@cuda.jit(device=True)\ndef gpu_calx(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps):\n    a = a_device\n    zwrap = zwrap_device\n    i = cuda.threadIdx.x + ss2\n\n    x_device[i - ss2, 0] = a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n    for j in range(- smooth_steps + 1, smooth_steps):\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n    x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n```\n\n然鹅，并行后的代码实际运行时间并不比加了`@jit`的cpu函数快。原因可能是计算本身的dimension不够大，gpu并行节省的时间还打不过gpu和cpu之间数据传输花费的时间。并不是gpu并行就一定比cpu快，这里有一个**数据计算量**和**调用线程、数据传输时间**之间的trade-off，cpu虽然不如gpu核数多，但往往优化做的很好，所以小数据量的计算在cpu上可能更好。\n\n##### -复杂函数的并行\n\n刚刚并行化的是最里层的一个简单函数，只涉及数组元素的算术运算。除此之外，我还想要把最外层的一个独立for循环并行化，这个for循环计算的是独立的40个集合预报成员。但问题是，这层for循环是相当外层的循环了，在它内部还有**多层函数嵌套**和一些**numpy方法**。\n\n```python\n@cuda.jit\ndef gpu_step_z(delta_t, zens, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing):\n    iens = cuda.blockIdx.x\n    #i = cuda.threadIdx.x\n    z = zens[iens, :].T\n        \n    z_save = z\n    # gpu_comp_dt_L04 is the 1st layer of gpu_step_z, more layers of functions lies inside gpu_comp_dt_L04\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the first intermediate step\n    z1 = np.multiply(delta_t, dz)\n    z = z_save + z1 / 2.0\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the second intermediate step\n    z2 = np.multiply(delta_t, dz)\n    z = z_save + z2 / 2.0\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the third intermediate step\n    z3 = np.multiply(delta_t, dz)\n    z = z_save + z3\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute fourth intermediate step\n    z4 = np.multiply(delta_t, dz)\n\n    dzt = z1 / 6.0 + z2 / 3.0 + z3 / 3.0 + z4 / 6.0\n    z = z_save + dzt\n    \n    zens[iens, :] = z.T\n```\n\n\n\n为了解决多层函数嵌套的编译问题，我在每个内层函数前加上`@cuda.jit(device=True)`装饰，在[这篇文章](https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1)有提到device function：\n\n> On the other hand, a `device function` can only be invoked from inside a device (by a kernel or another device function). The plus point is, you can return a value from a `device function`. So, you can use this return value of the function to compute something inside a `kernel function` or a `device function`.\n\n但是在`@cuda.jit`装饰的函数内不能使用numpy方法是硬伤，[numba官方](https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html)不支持在cuda并行编译的程序里使用numpy methods\n\n>Unsupported numpy features:\n>\n>- array creation APIs.\n>- array methods.\n>- functions that returns a new array.\n\n所以只能，放弃了！！！Orzzzzz...\n\n\n\n##### - 内存\n\nCuda.to_device\n\nShared memory\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[Java天堂-GPU加速的思想，图解，和经典案例](https://www.javatt.com/p/41851)*</small>\n\n<small>*[github-harrism/numba_examples](https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb)*</small>\n\n<small>*[towards data science-speed up your algorithms-part2 numba](https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1)*</small>\n\n","source":"_posts/2020-11-21-AI-Numba_GPU.md","raw":"---\ntitle: AI学习笔记--Numba+GPU加速\ndate: 2020-11-21 15:18:00\ncategories:\n- 计算机科学\ntags: \n- python\n- 人工智能\n- 机器学习\n- GPU\n---\n\n使用Numba + GPU加速你的python程序。<!--more-->\n\n<br/>\n\n### Numba加速器\n\n我把一段matlab代码重写迁移到python之后，运行速度慢了近100倍。经过代码分析，找到最耗时间的是python里的for循环。这可能与python语言本身的运行效率有关系，python是一门解释型的高级语言，在python被操作系统理解之前，要先转化为pyc字节码给python虚拟机，通过虚拟机与硬件操作系统交互。相比于一些更底层的语言，python的设计使得它更容易被程序员理解和使用，但trade-off是对机器来说更不容易理解，运行效率降低了。\n\nNumba就是为了解决python计算效率低的痛点开发的JIT（just-in-time）编译器。简单来说，Numba把一部分python代码编译成机器语言，只需在第一次调用时编译一次，之后再调用都能以机器语言的速度运行。使用Numba之后，我的同一段的代码运算速度和matlab相近甚至更快了。\n\n#### Numba安装 \n\n用conda直接装\n\n```python\n$ conda install numba\n```\n\n\n\n#### 在CPU上使用Numba\n\n使用Numba只需要把需要加速的那部分代码封装成function，在function前加一个`@jit`装饰器（decorator）就行了。是的，就这么简单。\n\n```python\nfrom numba import jit\n\n@jit\ndef f(x, y):\n    return x + y\n```\n\n\n\n需要注意的是，并不是所有的python对象Numba都支持，**尽量只加速最耗时的那部分代码**，这样既能避免代码中包含Numba不支持的python类型导致报错，又可以给代码瘦身提高编译速度。详见[官网的说明](https://numba.pydata.org/numba-doc/dev/user/troubleshoot.html#numba-troubleshooting)。\n\n编译有两个模式可选：object mode和nopython mode。后者默认你对要加速的代码充分了解，不包含Numba不支持的python类型，直接编译成机器语言。否则会直接报错。而object mode则不会，遇到不能编的就退出JIT，按python原生的来。所以**想要发挥Numba最好的性能，尽量使用nopython mode**。\n\n```python\n@jit(nopython=True)\ndef f(x, y):\n    return x + y\n```\n\n\n\nCpu parallel\n\n\n\nVectorize\n\n### Numba+GPU并行\n\n#### GPU与并行\n\n\n\n#### Numba使用GPU\n\n##### - 简单并行\n\n\n\n##### - 多维并行\n\n并行适合处理互相独立的运算，如果是对同一个数的循环累加，用单个线程代替一次循环可能会出问题。比如，我要并行化的原代码是一个两层嵌套循环，其中外层相互独立，内层是对同一个数的累加\n\n```python\ndef calx(x, ss2, model_size, a, zwrap, smooth_steps):\n    for i in range(ss2, ss2 + model_size):\n        x[i - ss2, 0] = a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n        for j in range(- smooth_steps + 1, smooth_steps):\n            x[i - ss2, 0] = x[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n        x[i - ss2, 0] = x[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n    return x\n```\n\n假如我把两层循环都并行化，这里我令block的维度是2*smooth_steps，用block里的一个thread计算一次内循环；令grid的维度是model_size，用grid里的每个block计算一次外循环。\n\n```python\n@cuda.jit\ndef gpu_calx(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps):\n    a = a_device\n    zwrap = zwrap_device\n    i = cuda.blockIdx.x + ss2\n    j = cuda.threadIdx.x - smooth_steps\n\n    if j == (- smooth_steps):\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n    elif j == smooth_steps:\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n    else:\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n```\n\n输出cpu的calx函数和gpu的gpu_calx函数计算结果的前三个数，发现两者不相同，有1e-2的差异\n\n```python\ncalx_result[0:3]=\n[[0.23222521]\n [0.42714764]\n [0.63248611]] \ngpu_result[0:3]=\n[[0.17172628]\n [0.36771521]\n [0.5903021 ]]\n```\n\n或许可以这样来解释：用并行的线程取代累加循环时，每个线程并不关心其他线程是否计算完成。假设线程a从内存中取出x_device[i - ss2, 0]执行加法运算时，可能同时有线程b也在执行相同的操作，而两者取出的x_device[i - ss2, 0]是同一值。这样两个线程执行完加法返回值的时候，其实相当于只执行了一次加法运算，线程a或b的其中一个运算是无效的。所以造成了最后结果的差异。\n\n于是，我只能放弃并行内层的累加循环\n\n```python\n@cuda.jit(device=True)\ndef gpu_calx(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps):\n    a = a_device\n    zwrap = zwrap_device\n    i = cuda.threadIdx.x + ss2\n\n    x_device[i - ss2, 0] = a[0] * zwrap[i + 1 - (- smooth_steps), 0] / 2.00\n    for j in range(- smooth_steps + 1, smooth_steps):\n        x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[j + smooth_steps] * zwrap[i + 1 - j, 0]\n    x_device[i - ss2, 0] = x_device[i - ss2, 0] + a[2 * smooth_steps] * zwrap[i + 1 - smooth_steps, 0] / 2.00\n```\n\n然鹅，并行后的代码实际运行时间并不比加了`@jit`的cpu函数快。原因可能是计算本身的dimension不够大，gpu并行节省的时间还打不过gpu和cpu之间数据传输花费的时间。并不是gpu并行就一定比cpu快，这里有一个**数据计算量**和**调用线程、数据传输时间**之间的trade-off，cpu虽然不如gpu核数多，但往往优化做的很好，所以小数据量的计算在cpu上可能更好。\n\n##### -复杂函数的并行\n\n刚刚并行化的是最里层的一个简单函数，只涉及数组元素的算术运算。除此之外，我还想要把最外层的一个独立for循环并行化，这个for循环计算的是独立的40个集合预报成员。但问题是，这层for循环是相当外层的循环了，在它内部还有**多层函数嵌套**和一些**numpy方法**。\n\n```python\n@cuda.jit\ndef gpu_step_z(delta_t, zens, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing):\n    iens = cuda.blockIdx.x\n    #i = cuda.threadIdx.x\n    z = zens[iens, :].T\n        \n    z_save = z\n    # gpu_comp_dt_L04 is the 1st layer of gpu_step_z, more layers of functions lies inside gpu_comp_dt_L04\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the first intermediate step\n    z1 = np.multiply(delta_t, dz)\n    z = z_save + z1 / 2.0\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the second intermediate step\n    z2 = np.multiply(delta_t, dz)\n    z = z_save + z2 / 2.0\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute the third intermediate step\n    z3 = np.multiply(delta_t, dz)\n    z = z_save + z3\n\n    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  # Compute fourth intermediate step\n    z4 = np.multiply(delta_t, dz)\n\n    dzt = z1 / 6.0 + z2 / 3.0 + z3 / 3.0 + z4 / 6.0\n    z = z_save + dzt\n    \n    zens[iens, :] = z.T\n```\n\n\n\n为了解决多层函数嵌套的编译问题，我在每个内层函数前加上`@cuda.jit(device=True)`装饰，在[这篇文章](https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1)有提到device function：\n\n> On the other hand, a `device function` can only be invoked from inside a device (by a kernel or another device function). The plus point is, you can return a value from a `device function`. So, you can use this return value of the function to compute something inside a `kernel function` or a `device function`.\n\n但是在`@cuda.jit`装饰的函数内不能使用numpy方法是硬伤，[numba官方](https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html)不支持在cuda并行编译的程序里使用numpy methods\n\n>Unsupported numpy features:\n>\n>- array creation APIs.\n>- array methods.\n>- functions that returns a new array.\n\n所以只能，放弃了！！！Orzzzzz...\n\n\n\n##### - 内存\n\nCuda.to_device\n\nShared memory\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[Java天堂-GPU加速的思想，图解，和经典案例](https://www.javatt.com/p/41851)*</small>\n\n<small>*[github-harrism/numba_examples](https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb)*</small>\n\n<small>*[towards data science-speed up your algorithms-part2 numba](https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1)*</small>\n\n","slug":"2020-11-21-AI-Numba_GPU","published":1,"updated":"2020-12-13T13:53:55.202Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5i000d2sfy3qul9dlp","content":"<p>使用Numba + GPU加速你的python程序。<a id=\"more\"></a></p>\n<p><br/></p>\n<h3 id=\"Numba加速器\"><a href=\"#Numba加速器\" class=\"headerlink\" title=\"Numba加速器\"></a>Numba加速器</h3><p>我把一段matlab代码重写迁移到python之后，运行速度慢了近100倍。经过代码分析，找到最耗时间的是python里的for循环。这可能与python语言本身的运行效率有关系，python是一门解释型的高级语言，在python被操作系统理解之前，要先转化为pyc字节码给python虚拟机，通过虚拟机与硬件操作系统交互。相比于一些更底层的语言，python的设计使得它更容易被程序员理解和使用，但trade-off是对机器来说更不容易理解，运行效率降低了。</p>\n<p>Numba就是为了解决python计算效率低的痛点开发的JIT（just-in-time）编译器。简单来说，Numba把一部分python代码编译成机器语言，只需在第一次调用时编译一次，之后再调用都能以机器语言的速度运行。使用Numba之后，我的同一段的代码运算速度和matlab相近甚至更快了。</p>\n<h4 id=\"Numba安装\"><a href=\"#Numba安装\" class=\"headerlink\" title=\"Numba安装\"></a>Numba安装</h4><p>用conda直接装</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ conda install numba</span><br></pre></td></tr></table></figure>\n<h4 id=\"在CPU上使用Numba\"><a href=\"#在CPU上使用Numba\" class=\"headerlink\" title=\"在CPU上使用Numba\"></a>在CPU上使用Numba</h4><p>使用Numba只需要把需要加速的那部分代码封装成function，在function前加一个<code>@jit</code>装饰器（decorator）就行了。是的，就这么简单。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> numba <span class=\"keyword\">import</span> jit</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x + y</span><br></pre></td></tr></table></figure>\n<p>需要注意的是，并不是所有的python对象Numba都支持，<strong>尽量只加速最耗时的那部分代码</strong>，这样既能避免代码中包含Numba不支持的python类型导致报错，又可以给代码瘦身提高编译速度。详见<a href=\"https://numba.pydata.org/numba-doc/dev/user/troubleshoot.html#numba-troubleshooting\" target=\"_blank\" rel=\"noopener\">官网的说明</a>。</p>\n<p>编译有两个模式可选：object mode和nopython mode。后者默认你对要加速的代码充分了解，不包含Numba不支持的python类型，直接编译成机器语言。否则会直接报错。而object mode则不会，遇到不能编的就退出JIT，按python原生的来。所以<strong>想要发挥Numba最好的性能，尽量使用nopython mode</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@jit(nopython=True)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x + y</span><br></pre></td></tr></table></figure>\n<p>Cpu parallel</p>\n<p>Vectorize</p>\n<h3 id=\"Numba-GPU并行\"><a href=\"#Numba-GPU并行\" class=\"headerlink\" title=\"Numba+GPU并行\"></a>Numba+GPU并行</h3><h4 id=\"GPU与并行\"><a href=\"#GPU与并行\" class=\"headerlink\" title=\"GPU与并行\"></a>GPU与并行</h4><h4 id=\"Numba使用GPU\"><a href=\"#Numba使用GPU\" class=\"headerlink\" title=\"Numba使用GPU\"></a>Numba使用GPU</h4><h5 id=\"简单并行\"><a href=\"#简单并行\" class=\"headerlink\" title=\"- 简单并行\"></a>- 简单并行</h5><h5 id=\"多维并行\"><a href=\"#多维并行\" class=\"headerlink\" title=\"- 多维并行\"></a>- 多维并行</h5><p>并行适合处理互相独立的运算，如果是对同一个数的循环累加，用单个线程代替一次循环可能会出问题。比如，我要并行化的原代码是一个两层嵌套循环，其中外层相互独立，内层是对同一个数的累加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calx</span><span class=\"params\">(x, ss2, model_size, a, zwrap, smooth_steps)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(ss2, ss2 + model_size):</span><br><span class=\"line\">        x[i - ss2, <span class=\"number\">0</span>] = a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(- smooth_steps + <span class=\"number\">1</span>, smooth_steps):</span><br><span class=\"line\">            x[i - ss2, <span class=\"number\">0</span>] = x[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br><span class=\"line\">        x[i - ss2, <span class=\"number\">0</span>] = x[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>假如我把两层循环都并行化，这里我令block的维度是2*smooth_steps，用block里的一个thread计算一次内循环；令grid的维度是model_size，用grid里的每个block计算一次外循环。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_calx</span><span class=\"params\">(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps)</span>:</span></span><br><span class=\"line\">    a = a_device</span><br><span class=\"line\">    zwrap = zwrap_device</span><br><span class=\"line\">    i = cuda.blockIdx.x + ss2</span><br><span class=\"line\">    j = cuda.threadIdx.x - smooth_steps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> j == (- smooth_steps):</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> j == smooth_steps:</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<p>输出cpu的calx函数和gpu的gpu_calx函数计算结果的前三个数，发现两者不相同，有1e-2的差异</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">calx_result[<span class=\"number\">0</span>:<span class=\"number\">3</span>]=</span><br><span class=\"line\">[[<span class=\"number\">0.23222521</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.42714764</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.63248611</span>]] </span><br><span class=\"line\">gpu_result[<span class=\"number\">0</span>:<span class=\"number\">3</span>]=</span><br><span class=\"line\">[[<span class=\"number\">0.17172628</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.36771521</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.5903021</span> ]]</span><br></pre></td></tr></table></figure>\n<p>或许可以这样来解释：用并行的线程取代累加循环时，每个线程并不关心其他线程是否计算完成。假设线程a从内存中取出x_device[i - ss2, 0]执行加法运算时，可能同时有线程b也在执行相同的操作，而两者取出的x_device[i - ss2, 0]是同一值。这样两个线程执行完加法返回值的时候，其实相当于只执行了一次加法运算，线程a或b的其中一个运算是无效的。所以造成了最后结果的差异。</p>\n<p>于是，我只能放弃并行内层的累加循环</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit(device=True)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_calx</span><span class=\"params\">(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps)</span>:</span></span><br><span class=\"line\">    a = a_device</span><br><span class=\"line\">    zwrap = zwrap_device</span><br><span class=\"line\">    i = cuda.threadIdx.x + ss2</span><br><span class=\"line\"></span><br><span class=\"line\">    x_device[i - ss2, <span class=\"number\">0</span>] = a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(- smooth_steps + <span class=\"number\">1</span>, smooth_steps):</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br><span class=\"line\">    x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br></pre></td></tr></table></figure>\n<p>然鹅，并行后的代码实际运行时间并不比加了<code>@jit</code>的cpu函数快。原因可能是计算本身的dimension不够大，gpu并行节省的时间还打不过gpu和cpu之间数据传输花费的时间。并不是gpu并行就一定比cpu快，这里有一个<strong>数据计算量</strong>和<strong>调用线程、数据传输时间</strong>之间的trade-off，cpu虽然不如gpu核数多，但往往优化做的很好，所以小数据量的计算在cpu上可能更好。</p>\n<h5 id=\"复杂函数的并行\"><a href=\"#复杂函数的并行\" class=\"headerlink\" title=\"-复杂函数的并行\"></a>-复杂函数的并行</h5><p>刚刚并行化的是最里层的一个简单函数，只涉及数组元素的算术运算。除此之外，我还想要把最外层的一个独立for循环并行化，这个for循环计算的是独立的40个集合预报成员。但问题是，这层for循环是相当外层的循环了，在它内部还有<strong>多层函数嵌套</strong>和一些<strong>numpy方法</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_step_z</span><span class=\"params\">(delta_t, zens, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)</span>:</span></span><br><span class=\"line\">    iens = cuda.blockIdx.x</span><br><span class=\"line\">    <span class=\"comment\">#i = cuda.threadIdx.x</span></span><br><span class=\"line\">    z = zens[iens, :].T</span><br><span class=\"line\">        </span><br><span class=\"line\">    z_save = z</span><br><span class=\"line\">    <span class=\"comment\"># gpu_comp_dt_L04 is the 1st layer of gpu_step_z, more layers of functions lies inside gpu_comp_dt_L04</span></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the first intermediate step</span></span><br><span class=\"line\">    z1 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z1 / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the second intermediate step</span></span><br><span class=\"line\">    z2 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z2 / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the third intermediate step</span></span><br><span class=\"line\">    z3 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z3</span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute fourth intermediate step</span></span><br><span class=\"line\">    z4 = np.multiply(delta_t, dz)</span><br><span class=\"line\"></span><br><span class=\"line\">    dzt = z1 / <span class=\"number\">6.0</span> + z2 / <span class=\"number\">3.0</span> + z3 / <span class=\"number\">3.0</span> + z4 / <span class=\"number\">6.0</span></span><br><span class=\"line\">    z = z_save + dzt</span><br><span class=\"line\">    </span><br><span class=\"line\">    zens[iens, :] = z.T</span><br></pre></td></tr></table></figure>\n<p>为了解决多层函数嵌套的编译问题，我在每个内层函数前加上<code>@cuda.jit(device=True)</code>装饰，在<a href=\"https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\" target=\"_blank\" rel=\"noopener\">这篇文章</a>有提到device function：</p>\n<blockquote>\n<p>On the other hand, a <code>device function</code> can only be invoked from inside a device (by a kernel or another device function). The plus point is, you can return a value from a <code>device function</code>. So, you can use this return value of the function to compute something inside a <code>kernel function</code> or a <code>device function</code>.</p>\n</blockquote>\n<p>但是在<code>@cuda.jit</code>装饰的函数内不能使用numpy方法是硬伤，<a href=\"https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html\" target=\"_blank\" rel=\"noopener\">numba官方</a>不支持在cuda并行编译的程序里使用numpy methods</p>\n<blockquote>\n<p>Unsupported numpy features:</p>\n<ul>\n<li>array creation APIs.</li>\n<li>array methods.</li>\n<li>functions that returns a new array.</li>\n</ul>\n</blockquote>\n<p>所以只能，放弃了！！！Orzzzzz…</p>\n<h5 id=\"内存\"><a href=\"#内存\" class=\"headerlink\" title=\"- 内存\"></a>- 内存</h5><p>Cuda.to_device</p>\n<p>Shared memory</p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.javatt.com/p/41851\" target=\"_blank\" rel=\"noopener\">Java天堂-GPU加速的思想，图解，和经典案例</a></em></small></p>\n<p><small><em><a href=\"https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb\" target=\"_blank\" rel=\"noopener\">github-harrism/numba_examples</a></em></small></p>\n<p><small><em><a href=\"https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\" target=\"_blank\" rel=\"noopener\">towards data science-speed up your algorithms-part2 numba</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>使用Numba + GPU加速你的python程序。","more":"</p>\n<p><br/></p>\n<h3 id=\"Numba加速器\"><a href=\"#Numba加速器\" class=\"headerlink\" title=\"Numba加速器\"></a>Numba加速器</h3><p>我把一段matlab代码重写迁移到python之后，运行速度慢了近100倍。经过代码分析，找到最耗时间的是python里的for循环。这可能与python语言本身的运行效率有关系，python是一门解释型的高级语言，在python被操作系统理解之前，要先转化为pyc字节码给python虚拟机，通过虚拟机与硬件操作系统交互。相比于一些更底层的语言，python的设计使得它更容易被程序员理解和使用，但trade-off是对机器来说更不容易理解，运行效率降低了。</p>\n<p>Numba就是为了解决python计算效率低的痛点开发的JIT（just-in-time）编译器。简单来说，Numba把一部分python代码编译成机器语言，只需在第一次调用时编译一次，之后再调用都能以机器语言的速度运行。使用Numba之后，我的同一段的代码运算速度和matlab相近甚至更快了。</p>\n<h4 id=\"Numba安装\"><a href=\"#Numba安装\" class=\"headerlink\" title=\"Numba安装\"></a>Numba安装</h4><p>用conda直接装</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ conda install numba</span><br></pre></td></tr></table></figure>\n<h4 id=\"在CPU上使用Numba\"><a href=\"#在CPU上使用Numba\" class=\"headerlink\" title=\"在CPU上使用Numba\"></a>在CPU上使用Numba</h4><p>使用Numba只需要把需要加速的那部分代码封装成function，在function前加一个<code>@jit</code>装饰器（decorator）就行了。是的，就这么简单。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> numba <span class=\"keyword\">import</span> jit</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x + y</span><br></pre></td></tr></table></figure>\n<p>需要注意的是，并不是所有的python对象Numba都支持，<strong>尽量只加速最耗时的那部分代码</strong>，这样既能避免代码中包含Numba不支持的python类型导致报错，又可以给代码瘦身提高编译速度。详见<a href=\"https://numba.pydata.org/numba-doc/dev/user/troubleshoot.html#numba-troubleshooting\" target=\"_blank\" rel=\"noopener\">官网的说明</a>。</p>\n<p>编译有两个模式可选：object mode和nopython mode。后者默认你对要加速的代码充分了解，不包含Numba不支持的python类型，直接编译成机器语言。否则会直接报错。而object mode则不会，遇到不能编的就退出JIT，按python原生的来。所以<strong>想要发挥Numba最好的性能，尽量使用nopython mode</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@jit(nopython=True)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x + y</span><br></pre></td></tr></table></figure>\n<p>Cpu parallel</p>\n<p>Vectorize</p>\n<h3 id=\"Numba-GPU并行\"><a href=\"#Numba-GPU并行\" class=\"headerlink\" title=\"Numba+GPU并行\"></a>Numba+GPU并行</h3><h4 id=\"GPU与并行\"><a href=\"#GPU与并行\" class=\"headerlink\" title=\"GPU与并行\"></a>GPU与并行</h4><h4 id=\"Numba使用GPU\"><a href=\"#Numba使用GPU\" class=\"headerlink\" title=\"Numba使用GPU\"></a>Numba使用GPU</h4><h5 id=\"简单并行\"><a href=\"#简单并行\" class=\"headerlink\" title=\"- 简单并行\"></a>- 简单并行</h5><h5 id=\"多维并行\"><a href=\"#多维并行\" class=\"headerlink\" title=\"- 多维并行\"></a>- 多维并行</h5><p>并行适合处理互相独立的运算，如果是对同一个数的循环累加，用单个线程代替一次循环可能会出问题。比如，我要并行化的原代码是一个两层嵌套循环，其中外层相互独立，内层是对同一个数的累加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calx</span><span class=\"params\">(x, ss2, model_size, a, zwrap, smooth_steps)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(ss2, ss2 + model_size):</span><br><span class=\"line\">        x[i - ss2, <span class=\"number\">0</span>] = a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(- smooth_steps + <span class=\"number\">1</span>, smooth_steps):</span><br><span class=\"line\">            x[i - ss2, <span class=\"number\">0</span>] = x[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br><span class=\"line\">        x[i - ss2, <span class=\"number\">0</span>] = x[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>假如我把两层循环都并行化，这里我令block的维度是2*smooth_steps，用block里的一个thread计算一次内循环；令grid的维度是model_size，用grid里的每个block计算一次外循环。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_calx</span><span class=\"params\">(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps)</span>:</span></span><br><span class=\"line\">    a = a_device</span><br><span class=\"line\">    zwrap = zwrap_device</span><br><span class=\"line\">    i = cuda.blockIdx.x + ss2</span><br><span class=\"line\">    j = cuda.threadIdx.x - smooth_steps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> j == (- smooth_steps):</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> j == smooth_steps:</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<p>输出cpu的calx函数和gpu的gpu_calx函数计算结果的前三个数，发现两者不相同，有1e-2的差异</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">calx_result[<span class=\"number\">0</span>:<span class=\"number\">3</span>]=</span><br><span class=\"line\">[[<span class=\"number\">0.23222521</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.42714764</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.63248611</span>]] </span><br><span class=\"line\">gpu_result[<span class=\"number\">0</span>:<span class=\"number\">3</span>]=</span><br><span class=\"line\">[[<span class=\"number\">0.17172628</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.36771521</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.5903021</span> ]]</span><br></pre></td></tr></table></figure>\n<p>或许可以这样来解释：用并行的线程取代累加循环时，每个线程并不关心其他线程是否计算完成。假设线程a从内存中取出x_device[i - ss2, 0]执行加法运算时，可能同时有线程b也在执行相同的操作，而两者取出的x_device[i - ss2, 0]是同一值。这样两个线程执行完加法返回值的时候，其实相当于只执行了一次加法运算，线程a或b的其中一个运算是无效的。所以造成了最后结果的差异。</p>\n<p>于是，我只能放弃并行内层的累加循环</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit(device=True)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_calx</span><span class=\"params\">(x_device, ss2, model_size, a_device, zwrap_device, smooth_steps)</span>:</span></span><br><span class=\"line\">    a = a_device</span><br><span class=\"line\">    zwrap = zwrap_device</span><br><span class=\"line\">    i = cuda.threadIdx.x + ss2</span><br><span class=\"line\"></span><br><span class=\"line\">    x_device[i - ss2, <span class=\"number\">0</span>] = a[<span class=\"number\">0</span>] * zwrap[i + <span class=\"number\">1</span> - (- smooth_steps), <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(- smooth_steps + <span class=\"number\">1</span>, smooth_steps):</span><br><span class=\"line\">        x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[j + smooth_steps] * zwrap[i + <span class=\"number\">1</span> - j, <span class=\"number\">0</span>]</span><br><span class=\"line\">    x_device[i - ss2, <span class=\"number\">0</span>] = x_device[i - ss2, <span class=\"number\">0</span>] + a[<span class=\"number\">2</span> * smooth_steps] * zwrap[i + <span class=\"number\">1</span> - smooth_steps, <span class=\"number\">0</span>] / <span class=\"number\">2.00</span></span><br></pre></td></tr></table></figure>\n<p>然鹅，并行后的代码实际运行时间并不比加了<code>@jit</code>的cpu函数快。原因可能是计算本身的dimension不够大，gpu并行节省的时间还打不过gpu和cpu之间数据传输花费的时间。并不是gpu并行就一定比cpu快，这里有一个<strong>数据计算量</strong>和<strong>调用线程、数据传输时间</strong>之间的trade-off，cpu虽然不如gpu核数多，但往往优化做的很好，所以小数据量的计算在cpu上可能更好。</p>\n<h5 id=\"复杂函数的并行\"><a href=\"#复杂函数的并行\" class=\"headerlink\" title=\"-复杂函数的并行\"></a>-复杂函数的并行</h5><p>刚刚并行化的是最里层的一个简单函数，只涉及数组元素的算术运算。除此之外，我还想要把最外层的一个独立for循环并行化，这个for循环计算的是独立的40个集合预报成员。但问题是，这层for循环是相当外层的循环了，在它内部还有<strong>多层函数嵌套</strong>和一些<strong>numpy方法</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@cuda.jit</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gpu_step_z</span><span class=\"params\">(delta_t, zens, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)</span>:</span></span><br><span class=\"line\">    iens = cuda.blockIdx.x</span><br><span class=\"line\">    <span class=\"comment\">#i = cuda.threadIdx.x</span></span><br><span class=\"line\">    z = zens[iens, :].T</span><br><span class=\"line\">        </span><br><span class=\"line\">    z_save = z</span><br><span class=\"line\">    <span class=\"comment\"># gpu_comp_dt_L04 is the 1st layer of gpu_step_z, more layers of functions lies inside gpu_comp_dt_L04</span></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the first intermediate step</span></span><br><span class=\"line\">    z1 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z1 / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the second intermediate step</span></span><br><span class=\"line\">    z2 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z2 / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute the third intermediate step</span></span><br><span class=\"line\">    z3 = np.multiply(delta_t, dz)</span><br><span class=\"line\">    z = z_save + z3</span><br><span class=\"line\"></span><br><span class=\"line\">    dz = gpu_comp_dt_L04(z, model_size, ss2, smooth_steps, a, model_number, K4, H, K, K2, sts2, coupling, space_time_scale, forcing)  <span class=\"comment\"># Compute fourth intermediate step</span></span><br><span class=\"line\">    z4 = np.multiply(delta_t, dz)</span><br><span class=\"line\"></span><br><span class=\"line\">    dzt = z1 / <span class=\"number\">6.0</span> + z2 / <span class=\"number\">3.0</span> + z3 / <span class=\"number\">3.0</span> + z4 / <span class=\"number\">6.0</span></span><br><span class=\"line\">    z = z_save + dzt</span><br><span class=\"line\">    </span><br><span class=\"line\">    zens[iens, :] = z.T</span><br></pre></td></tr></table></figure>\n<p>为了解决多层函数嵌套的编译问题，我在每个内层函数前加上<code>@cuda.jit(device=True)</code>装饰，在<a href=\"https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\" target=\"_blank\" rel=\"noopener\">这篇文章</a>有提到device function：</p>\n<blockquote>\n<p>On the other hand, a <code>device function</code> can only be invoked from inside a device (by a kernel or another device function). The plus point is, you can return a value from a <code>device function</code>. So, you can use this return value of the function to compute something inside a <code>kernel function</code> or a <code>device function</code>.</p>\n</blockquote>\n<p>但是在<code>@cuda.jit</code>装饰的函数内不能使用numpy方法是硬伤，<a href=\"https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html\" target=\"_blank\" rel=\"noopener\">numba官方</a>不支持在cuda并行编译的程序里使用numpy methods</p>\n<blockquote>\n<p>Unsupported numpy features:</p>\n<ul>\n<li>array creation APIs.</li>\n<li>array methods.</li>\n<li>functions that returns a new array.</li>\n</ul>\n</blockquote>\n<p>所以只能，放弃了！！！Orzzzzz…</p>\n<h5 id=\"内存\"><a href=\"#内存\" class=\"headerlink\" title=\"- 内存\"></a>- 内存</h5><p>Cuda.to_device</p>\n<p>Shared memory</p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.javatt.com/p/41851\" target=\"_blank\" rel=\"noopener\">Java天堂-GPU加速的思想，图解，和经典案例</a></em></small></p>\n<p><small><em><a href=\"https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb\" target=\"_blank\" rel=\"noopener\">github-harrism/numba_examples</a></em></small></p>\n<p><small><em><a href=\"https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\" target=\"_blank\" rel=\"noopener\">towards data science-speed up your algorithms-part2 numba</a></em></small></p>"},{"title":"AI学习笔记--神经网络","date":"2020-02-26T15:45:00.000Z","mathjax":true,"_content":"\n人工智能$\\supset$机器学习$\\supset$深度学习\n\n*专家系统*：依靠人编写足够多的规则给计算机执行，让计算机成为“专家”，计算机不具备学习能力（20世纪80年代）。属于符号主义人工智能。\n\n*机器学习*：机器可以学习人告诉它的规则之外的规则。<!--more-->\n\n<br/>\n\n机器学习的技术定义：在预先设定好的可能性空间（hypothesis space）中，利用反馈信号的指引找到输入数据的最有用**表示**（representation）。\n\n<br/>\n\n什么是深度学习？ 深度学习的深度（depth）不是指理解深度，而是指一些列的**表示层**（layer）的层数很多，这些分层几乎都是通过叫“神经网络”（neural network）的模型学习到的。（神经网络虽然灵感是来自人脑，但其实和真实的人脑学习机制并不相同，只是一种数学框架）\n\n**深度学习的基本原理**：每个表示层有一个权重，预测结果与真实结果的差距用损失函数（loss function）或目标函数（objective function）衡量，一遍遍的训练循环，以损失函数的反馈调整表示层的权重，最后找到最优权重组。(BP误差反向传播算法)\n\n1. 将批量数据输入神经网络\n2. 计算神经网络预测值$y_{pred}$和真值$y_t$的差距（loss function）\n3. 运用最优化方法，如随机梯度下降法，沿梯度反方向调整权重参数\n\n相比于SVM、决策树等浅层算法，深度学习有更多的表示层，并且可以同时学习所有表示层（贪婪学习）。现在流行的两种方法：梯度提升机和深度学习。梯度提升机用于浅层学习，处理结构化数据；深度学习则用于图像处理等感知问题。\n\n传统机器学习和深度学习的区别：传统机器学习需要人工提取特征（特征工程），再利用分类算法（SVM、决策树等）输出结果；深度学习直接端到端学习，特征提取和分类全部由神经网络完成。\n\n<img src=\"/images/Machine-vs-deep-learning-1.png\" alt=\"Machine-vs-deep-learning-1\" style=\"zoom:50%;\" />\n\n<center><small>图片来源:trantorinc.com</small></center>\n\n<br/>\n\n## 神经网络基本构成\n\n---\n\n### 数据\n\n深度学习中的数据张量可以有几个轴，其中0轴一般都是**样本轴/批量轴**，下面说明常见的数据张量可以有哪些轴。\n\n* 2D张量：（samples，features）向量数据\n* 3D张量：（samples，timesteps，features）时间序列数据或序列数据\n* 4D张量：（samples，height，width，channels）图像\n* 5D张量：（samples，frames，height，width，channels）视频\n\n### 层\n\n不同维度的数据张量对应输入不同的层。2D张量通常用密集连接层（dense connected layer）/全连接层（fully connected layer）；3D张量通常用循环层（recurrent layer）；4D张量通常用二维卷积层（如Keras的Conc2D）。\n\n在Keras中，根据层兼容性（该层输入输出的特定张量形状），层与层可以像搭积木一样组合在一起。\n\n### 模型\n\n层构成的网络称为模型。不同的模型其实就是具有不同拓扑结构的层网络。一些常见的网络拓扑结构有：\n\n* 线性结构：将单一输入映射为单一输出\n* 双分支机构（two-branch）网络\n* 多头网络（multihead）\n* Inception模块\n\n在Keras中有两种定义模型的方法：\n\n1. Sequential类（仅用于层的线性堆叠）\n2. 函数式API（用于层组成的有向无环图，可以构建任何形式的架构）\n\n### 损失函数\n\n选择最合适的损失函数。比如二分类问题可以选择二元交叉熵（（binary cross entropy）；多分类问题可以用分类交叉熵（categorical cross entropy）；回归问题可以用均方误差（mean-square error）等。\n\n### 优化器\n\n优化方法的选择。\n\n\n\n<br/>\n\n## 神经网络的学习算法\n\n---\n\n### 人工神经网络（ANN）\n\n* #### 误差反向传播（BP）算法\n\n  利用网络的输出与期望的误差，反向调整前面各层的权重，最终可以得到一个逼近期望的神经网络（Rumelhart, McCliland 1985）。\n\n* #### Hopfield算法\n\n* #### 自适应共振理论（ART）算法\n\n* #### 自组织映射（SOM）算法\n\n  两层拓扑结构，输入和输出层。输入可以是任意形式的，输出层（也称竞争层）一般是一维或二维的。\n\n  <img src=\"/images/IMG_0660.jpg\" alt=\"IMG_0660\" style=\"zoom:25%;\" />\n\n  <img src=\"/images/IMG_0661.jpg\" alt=\"IMG_0661\" style=\"zoom:50%;\" />\n\n  \n\n### 卷积神经网络（CNN）\n\n卷积神经网络与普通人工神经网络相比增加了卷积层和池化层。[How Convolutional Neural Networks work-Brandon Rohrer](https://m.youtube.com/watch?v=FmpDIaiMIeA)\n\n* #### 卷积（convolution）\n\n  [什么是卷积](https://www.matongxue.com/madocs/32.html) \n\n* #### 池化（pooling）\n\n  [什么是池化](https://www.jiqizhixin.com/graph/technologies/0a4cedf0-0ee0-4406-946e-2877950da91d)\n\n* #### 完全连接层（fully connected layer）\n\n  完全连接层的理想权重可以由Back Propagation方法得到。\n\n\n\n<br/>\n\n## Keras实践\n\n---\n\n### 准备数据\n\n对数据预处理，有时需要标准化。\n\n* #### 训练集、验证集和测试集\n\n  数据需要分成三份：训练集、验证集和测试集。单独留出一份测试集，而不使用验证集做最后测试的原因是：除了机器最优化过程对权重参数的“学习”，人往往还要调整模型配置（成为**超参数**），比如选择层数、每层的大小，这本质上也是“学习”，特别是当多次重复“以验证集评估，修改模型配置”的过程，验证集的信息会泄露到模型中。\n\n### 选择模型\n\n包括层的配置，损失函数的选择，优化器选择\n\n### 验证模型\n\n* #### 简单留出验证\n\n* #### K折验证\n\n* #### 重复K折验证\n\n当数据集比较小时，可以考虑用K折验证的方法评估你的网络。\n\n<br/>\n\n##### *「过拟合的概念」*\n\n<img src=\"/images/IMG_0287.PNG\" alt=\"IMG_0287\" style=\"zoom:24.3%;\" /> <img src=\"/images/IMG_0288.PNG\" alt=\"IMG_0288\" style=\"zoom:25%;\" />\n\n训练精度和验证精度刚开始回随着迭代次数增加而增加，但会到达一个临界点，训练精度虽然继续增加，但验证精度反而会下降，这叫做过拟合。\n\n\n\n\n\n应用深度学习需要同时理解：\n\n1. 问题的动机和特点；\n2. 将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理；\n3. 在原始数据上拟合极复杂的深层模型的优化算法；\n4. 有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能；\n5. 为解决方案挑选合适的变量（超参数）组合的经验。\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*弗朗索瓦·肖莱《Python深度学习》人民邮电出版社*</small>\n\n","source":"_posts/AI_Neural_Network.md","raw":"---\ntitle: AI学习笔记--神经网络 \ndate: 2020-02-26 23:45:00\ncategories:\n- 计算机科学\ntags: \n- 人工智能\n- 机器学习\n- python\n- 深度学习\nmathjax: true\n\n---\n\n人工智能$\\supset$机器学习$\\supset$深度学习\n\n*专家系统*：依靠人编写足够多的规则给计算机执行，让计算机成为“专家”，计算机不具备学习能力（20世纪80年代）。属于符号主义人工智能。\n\n*机器学习*：机器可以学习人告诉它的规则之外的规则。<!--more-->\n\n<br/>\n\n机器学习的技术定义：在预先设定好的可能性空间（hypothesis space）中，利用反馈信号的指引找到输入数据的最有用**表示**（representation）。\n\n<br/>\n\n什么是深度学习？ 深度学习的深度（depth）不是指理解深度，而是指一些列的**表示层**（layer）的层数很多，这些分层几乎都是通过叫“神经网络”（neural network）的模型学习到的。（神经网络虽然灵感是来自人脑，但其实和真实的人脑学习机制并不相同，只是一种数学框架）\n\n**深度学习的基本原理**：每个表示层有一个权重，预测结果与真实结果的差距用损失函数（loss function）或目标函数（objective function）衡量，一遍遍的训练循环，以损失函数的反馈调整表示层的权重，最后找到最优权重组。(BP误差反向传播算法)\n\n1. 将批量数据输入神经网络\n2. 计算神经网络预测值$y_{pred}$和真值$y_t$的差距（loss function）\n3. 运用最优化方法，如随机梯度下降法，沿梯度反方向调整权重参数\n\n相比于SVM、决策树等浅层算法，深度学习有更多的表示层，并且可以同时学习所有表示层（贪婪学习）。现在流行的两种方法：梯度提升机和深度学习。梯度提升机用于浅层学习，处理结构化数据；深度学习则用于图像处理等感知问题。\n\n传统机器学习和深度学习的区别：传统机器学习需要人工提取特征（特征工程），再利用分类算法（SVM、决策树等）输出结果；深度学习直接端到端学习，特征提取和分类全部由神经网络完成。\n\n<img src=\"/images/Machine-vs-deep-learning-1.png\" alt=\"Machine-vs-deep-learning-1\" style=\"zoom:50%;\" />\n\n<center><small>图片来源:trantorinc.com</small></center>\n\n<br/>\n\n## 神经网络基本构成\n\n---\n\n### 数据\n\n深度学习中的数据张量可以有几个轴，其中0轴一般都是**样本轴/批量轴**，下面说明常见的数据张量可以有哪些轴。\n\n* 2D张量：（samples，features）向量数据\n* 3D张量：（samples，timesteps，features）时间序列数据或序列数据\n* 4D张量：（samples，height，width，channels）图像\n* 5D张量：（samples，frames，height，width，channels）视频\n\n### 层\n\n不同维度的数据张量对应输入不同的层。2D张量通常用密集连接层（dense connected layer）/全连接层（fully connected layer）；3D张量通常用循环层（recurrent layer）；4D张量通常用二维卷积层（如Keras的Conc2D）。\n\n在Keras中，根据层兼容性（该层输入输出的特定张量形状），层与层可以像搭积木一样组合在一起。\n\n### 模型\n\n层构成的网络称为模型。不同的模型其实就是具有不同拓扑结构的层网络。一些常见的网络拓扑结构有：\n\n* 线性结构：将单一输入映射为单一输出\n* 双分支机构（two-branch）网络\n* 多头网络（multihead）\n* Inception模块\n\n在Keras中有两种定义模型的方法：\n\n1. Sequential类（仅用于层的线性堆叠）\n2. 函数式API（用于层组成的有向无环图，可以构建任何形式的架构）\n\n### 损失函数\n\n选择最合适的损失函数。比如二分类问题可以选择二元交叉熵（（binary cross entropy）；多分类问题可以用分类交叉熵（categorical cross entropy）；回归问题可以用均方误差（mean-square error）等。\n\n### 优化器\n\n优化方法的选择。\n\n\n\n<br/>\n\n## 神经网络的学习算法\n\n---\n\n### 人工神经网络（ANN）\n\n* #### 误差反向传播（BP）算法\n\n  利用网络的输出与期望的误差，反向调整前面各层的权重，最终可以得到一个逼近期望的神经网络（Rumelhart, McCliland 1985）。\n\n* #### Hopfield算法\n\n* #### 自适应共振理论（ART）算法\n\n* #### 自组织映射（SOM）算法\n\n  两层拓扑结构，输入和输出层。输入可以是任意形式的，输出层（也称竞争层）一般是一维或二维的。\n\n  <img src=\"/images/IMG_0660.jpg\" alt=\"IMG_0660\" style=\"zoom:25%;\" />\n\n  <img src=\"/images/IMG_0661.jpg\" alt=\"IMG_0661\" style=\"zoom:50%;\" />\n\n  \n\n### 卷积神经网络（CNN）\n\n卷积神经网络与普通人工神经网络相比增加了卷积层和池化层。[How Convolutional Neural Networks work-Brandon Rohrer](https://m.youtube.com/watch?v=FmpDIaiMIeA)\n\n* #### 卷积（convolution）\n\n  [什么是卷积](https://www.matongxue.com/madocs/32.html) \n\n* #### 池化（pooling）\n\n  [什么是池化](https://www.jiqizhixin.com/graph/technologies/0a4cedf0-0ee0-4406-946e-2877950da91d)\n\n* #### 完全连接层（fully connected layer）\n\n  完全连接层的理想权重可以由Back Propagation方法得到。\n\n\n\n<br/>\n\n## Keras实践\n\n---\n\n### 准备数据\n\n对数据预处理，有时需要标准化。\n\n* #### 训练集、验证集和测试集\n\n  数据需要分成三份：训练集、验证集和测试集。单独留出一份测试集，而不使用验证集做最后测试的原因是：除了机器最优化过程对权重参数的“学习”，人往往还要调整模型配置（成为**超参数**），比如选择层数、每层的大小，这本质上也是“学习”，特别是当多次重复“以验证集评估，修改模型配置”的过程，验证集的信息会泄露到模型中。\n\n### 选择模型\n\n包括层的配置，损失函数的选择，优化器选择\n\n### 验证模型\n\n* #### 简单留出验证\n\n* #### K折验证\n\n* #### 重复K折验证\n\n当数据集比较小时，可以考虑用K折验证的方法评估你的网络。\n\n<br/>\n\n##### *「过拟合的概念」*\n\n<img src=\"/images/IMG_0287.PNG\" alt=\"IMG_0287\" style=\"zoom:24.3%;\" /> <img src=\"/images/IMG_0288.PNG\" alt=\"IMG_0288\" style=\"zoom:25%;\" />\n\n训练精度和验证精度刚开始回随着迭代次数增加而增加，但会到达一个临界点，训练精度虽然继续增加，但验证精度反而会下降，这叫做过拟合。\n\n\n\n\n\n应用深度学习需要同时理解：\n\n1. 问题的动机和特点；\n2. 将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理；\n3. 在原始数据上拟合极复杂的深层模型的优化算法；\n4. 有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能；\n5. 为解决方案挑选合适的变量（超参数）组合的经验。\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*弗朗索瓦·肖莱《Python深度学习》人民邮电出版社*</small>\n\n","slug":"AI_Neural_Network","published":1,"updated":"2020-10-23T17:19:59.740Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5o000h2sfyh02rh1og","content":"<p>人工智能$\\supset$机器学习$\\supset$深度学习</p>\n<p><em>专家系统</em>：依靠人编写足够多的规则给计算机执行，让计算机成为“专家”，计算机不具备学习能力（20世纪80年代）。属于符号主义人工智能。</p>\n<p><em>机器学习</em>：机器可以学习人告诉它的规则之外的规则。<a id=\"more\"></a></p>\n<p><br/></p>\n<p>机器学习的技术定义：在预先设定好的可能性空间（hypothesis space）中，利用反馈信号的指引找到输入数据的最有用<strong>表示</strong>（representation）。</p>\n<p><br/></p>\n<p>什么是深度学习？ 深度学习的深度（depth）不是指理解深度，而是指一些列的<strong>表示层</strong>（layer）的层数很多，这些分层几乎都是通过叫“神经网络”（neural network）的模型学习到的。（神经网络虽然灵感是来自人脑，但其实和真实的人脑学习机制并不相同，只是一种数学框架）</p>\n<p><strong>深度学习的基本原理</strong>：每个表示层有一个权重，预测结果与真实结果的差距用损失函数（loss function）或目标函数（objective function）衡量，一遍遍的训练循环，以损失函数的反馈调整表示层的权重，最后找到最优权重组。(BP误差反向传播算法)</p>\n<ol>\n<li>将批量数据输入神经网络</li>\n<li>计算神经网络预测值$y_{pred}$和真值$y_t$的差距（loss function）</li>\n<li>运用最优化方法，如随机梯度下降法，沿梯度反方向调整权重参数</li>\n</ol>\n<p>相比于SVM、决策树等浅层算法，深度学习有更多的表示层，并且可以同时学习所有表示层（贪婪学习）。现在流行的两种方法：梯度提升机和深度学习。梯度提升机用于浅层学习，处理结构化数据；深度学习则用于图像处理等感知问题。</p>\n<p>传统机器学习和深度学习的区别：传统机器学习需要人工提取特征（特征工程），再利用分类算法（SVM、决策树等）输出结果；深度学习直接端到端学习，特征提取和分类全部由神经网络完成。</p>\n<p><img src=\"/images/Machine-vs-deep-learning-1.png\" alt=\"Machine-vs-deep-learning-1\" style=\"zoom:50%;\" /></p>\n<center><small>图片来源:trantorinc.com</small></center>\n\n<p><br/></p>\n<h2 id=\"神经网络基本构成\"><a href=\"#神经网络基本构成\" class=\"headerlink\" title=\"神经网络基本构成\"></a>神经网络基本构成</h2><hr>\n<h3 id=\"数据\"><a href=\"#数据\" class=\"headerlink\" title=\"数据\"></a>数据</h3><p>深度学习中的数据张量可以有几个轴，其中0轴一般都是<strong>样本轴/批量轴</strong>，下面说明常见的数据张量可以有哪些轴。</p>\n<ul>\n<li>2D张量：（samples，features）向量数据</li>\n<li>3D张量：（samples，timesteps，features）时间序列数据或序列数据</li>\n<li>4D张量：（samples，height，width，channels）图像</li>\n<li>5D张量：（samples，frames，height，width，channels）视频</li>\n</ul>\n<h3 id=\"层\"><a href=\"#层\" class=\"headerlink\" title=\"层\"></a>层</h3><p>不同维度的数据张量对应输入不同的层。2D张量通常用密集连接层（dense connected layer）/全连接层（fully connected layer）；3D张量通常用循环层（recurrent layer）；4D张量通常用二维卷积层（如Keras的Conc2D）。</p>\n<p>在Keras中，根据层兼容性（该层输入输出的特定张量形状），层与层可以像搭积木一样组合在一起。</p>\n<h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><p>层构成的网络称为模型。不同的模型其实就是具有不同拓扑结构的层网络。一些常见的网络拓扑结构有：</p>\n<ul>\n<li>线性结构：将单一输入映射为单一输出</li>\n<li>双分支机构（two-branch）网络</li>\n<li>多头网络（multihead）</li>\n<li>Inception模块</li>\n</ul>\n<p>在Keras中有两种定义模型的方法：</p>\n<ol>\n<li>Sequential类（仅用于层的线性堆叠）</li>\n<li>函数式API（用于层组成的有向无环图，可以构建任何形式的架构）</li>\n</ol>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>选择最合适的损失函数。比如二分类问题可以选择二元交叉熵（（binary cross entropy）；多分类问题可以用分类交叉熵（categorical cross entropy）；回归问题可以用均方误差（mean-square error）等。</p>\n<h3 id=\"优化器\"><a href=\"#优化器\" class=\"headerlink\" title=\"优化器\"></a>优化器</h3><p>优化方法的选择。</p>\n<p><br/></p>\n<h2 id=\"神经网络的学习算法\"><a href=\"#神经网络的学习算法\" class=\"headerlink\" title=\"神经网络的学习算法\"></a>神经网络的学习算法</h2><hr>\n<h3 id=\"人工神经网络（ANN）\"><a href=\"#人工神经网络（ANN）\" class=\"headerlink\" title=\"人工神经网络（ANN）\"></a>人工神经网络（ANN）</h3><ul>\n<li><h4 id=\"误差反向传播（BP）算法\"><a href=\"#误差反向传播（BP）算法\" class=\"headerlink\" title=\"误差反向传播（BP）算法\"></a>误差反向传播（BP）算法</h4><p>利用网络的输出与期望的误差，反向调整前面各层的权重，最终可以得到一个逼近期望的神经网络（Rumelhart, McCliland 1985）。</p>\n</li>\n<li><h4 id=\"Hopfield算法\"><a href=\"#Hopfield算法\" class=\"headerlink\" title=\"Hopfield算法\"></a>Hopfield算法</h4></li>\n<li><h4 id=\"自适应共振理论（ART）算法\"><a href=\"#自适应共振理论（ART）算法\" class=\"headerlink\" title=\"自适应共振理论（ART）算法\"></a>自适应共振理论（ART）算法</h4></li>\n<li><h4 id=\"自组织映射（SOM）算法\"><a href=\"#自组织映射（SOM）算法\" class=\"headerlink\" title=\"自组织映射（SOM）算法\"></a>自组织映射（SOM）算法</h4><p>两层拓扑结构，输入和输出层。输入可以是任意形式的，输出层（也称竞争层）一般是一维或二维的。</p>\n<p><img src=\"/images/IMG_0660.jpg\" alt=\"IMG_0660\" style=\"zoom:25%;\" /></p>\n<p><img src=\"/images/IMG_0661.jpg\" alt=\"IMG_0661\" style=\"zoom:50%;\" /></p>\n</li>\n</ul>\n<h3 id=\"卷积神经网络（CNN）\"><a href=\"#卷积神经网络（CNN）\" class=\"headerlink\" title=\"卷积神经网络（CNN）\"></a>卷积神经网络（CNN）</h3><p>卷积神经网络与普通人工神经网络相比增加了卷积层和池化层。<a href=\"https://m.youtube.com/watch?v=FmpDIaiMIeA\" target=\"_blank\" rel=\"noopener\">How Convolutional Neural Networks work-Brandon Rohrer</a></p>\n<ul>\n<li><h4 id=\"卷积（convolution）\"><a href=\"#卷积（convolution）\" class=\"headerlink\" title=\"卷积（convolution）\"></a>卷积（convolution）</h4><p><a href=\"https://www.matongxue.com/madocs/32.html\" target=\"_blank\" rel=\"noopener\">什么是卷积</a> </p>\n</li>\n<li><h4 id=\"池化（pooling）\"><a href=\"#池化（pooling）\" class=\"headerlink\" title=\"池化（pooling）\"></a>池化（pooling）</h4><p><a href=\"https://www.jiqizhixin.com/graph/technologies/0a4cedf0-0ee0-4406-946e-2877950da91d\" target=\"_blank\" rel=\"noopener\">什么是池化</a></p>\n</li>\n<li><h4 id=\"完全连接层（fully-connected-layer）\"><a href=\"#完全连接层（fully-connected-layer）\" class=\"headerlink\" title=\"完全连接层（fully connected layer）\"></a>完全连接层（fully connected layer）</h4><p>完全连接层的理想权重可以由Back Propagation方法得到。</p>\n</li>\n</ul>\n<p><br/></p>\n<h2 id=\"Keras实践\"><a href=\"#Keras实践\" class=\"headerlink\" title=\"Keras实践\"></a>Keras实践</h2><hr>\n<h3 id=\"准备数据\"><a href=\"#准备数据\" class=\"headerlink\" title=\"准备数据\"></a>准备数据</h3><p>对数据预处理，有时需要标准化。</p>\n<ul>\n<li><h4 id=\"训练集、验证集和测试集\"><a href=\"#训练集、验证集和测试集\" class=\"headerlink\" title=\"训练集、验证集和测试集\"></a>训练集、验证集和测试集</h4><p>数据需要分成三份：训练集、验证集和测试集。单独留出一份测试集，而不使用验证集做最后测试的原因是：除了机器最优化过程对权重参数的“学习”，人往往还要调整模型配置（成为<strong>超参数</strong>），比如选择层数、每层的大小，这本质上也是“学习”，特别是当多次重复“以验证集评估，修改模型配置”的过程，验证集的信息会泄露到模型中。</p>\n</li>\n</ul>\n<h3 id=\"选择模型\"><a href=\"#选择模型\" class=\"headerlink\" title=\"选择模型\"></a>选择模型</h3><p>包括层的配置，损失函数的选择，优化器选择</p>\n<h3 id=\"验证模型\"><a href=\"#验证模型\" class=\"headerlink\" title=\"验证模型\"></a>验证模型</h3><ul>\n<li><h4 id=\"简单留出验证\"><a href=\"#简单留出验证\" class=\"headerlink\" title=\"简单留出验证\"></a>简单留出验证</h4></li>\n<li><h4 id=\"K折验证\"><a href=\"#K折验证\" class=\"headerlink\" title=\"K折验证\"></a>K折验证</h4></li>\n<li><h4 id=\"重复K折验证\"><a href=\"#重复K折验证\" class=\"headerlink\" title=\"重复K折验证\"></a>重复K折验证</h4></li>\n</ul>\n<p>当数据集比较小时，可以考虑用K折验证的方法评估你的网络。</p>\n<p><br/></p>\n<h5 id=\"「过拟合的概念」\"><a href=\"#「过拟合的概念」\" class=\"headerlink\" title=\"「过拟合的概念」\"></a><em>「过拟合的概念」</em></h5><p><img src=\"/images/IMG_0287.PNG\" alt=\"IMG_0287\" style=\"zoom:24.3%;\" /> <img src=\"/images/IMG_0288.PNG\" alt=\"IMG_0288\" style=\"zoom:25%;\" /></p>\n<p>训练精度和验证精度刚开始回随着迭代次数增加而增加，但会到达一个临界点，训练精度虽然继续增加，但验证精度反而会下降，这叫做过拟合。</p>\n<p>应用深度学习需要同时理解：</p>\n<ol>\n<li>问题的动机和特点；</li>\n<li>将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理；</li>\n<li>在原始数据上拟合极复杂的深层模型的优化算法；</li>\n<li>有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能；</li>\n<li>为解决方案挑选合适的变量（超参数）组合的经验。</li>\n</ol>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>弗朗索瓦·肖莱《Python深度学习》人民邮电出版社</em></small></p>\n","site":{"data":{}},"excerpt":"<p>人工智能$\\supset$机器学习$\\supset$深度学习</p>\n<p><em>专家系统</em>：依靠人编写足够多的规则给计算机执行，让计算机成为“专家”，计算机不具备学习能力（20世纪80年代）。属于符号主义人工智能。</p>\n<p><em>机器学习</em>：机器可以学习人告诉它的规则之外的规则。","more":"</p>\n<p><br/></p>\n<p>机器学习的技术定义：在预先设定好的可能性空间（hypothesis space）中，利用反馈信号的指引找到输入数据的最有用<strong>表示</strong>（representation）。</p>\n<p><br/></p>\n<p>什么是深度学习？ 深度学习的深度（depth）不是指理解深度，而是指一些列的<strong>表示层</strong>（layer）的层数很多，这些分层几乎都是通过叫“神经网络”（neural network）的模型学习到的。（神经网络虽然灵感是来自人脑，但其实和真实的人脑学习机制并不相同，只是一种数学框架）</p>\n<p><strong>深度学习的基本原理</strong>：每个表示层有一个权重，预测结果与真实结果的差距用损失函数（loss function）或目标函数（objective function）衡量，一遍遍的训练循环，以损失函数的反馈调整表示层的权重，最后找到最优权重组。(BP误差反向传播算法)</p>\n<ol>\n<li>将批量数据输入神经网络</li>\n<li>计算神经网络预测值$y_{pred}$和真值$y_t$的差距（loss function）</li>\n<li>运用最优化方法，如随机梯度下降法，沿梯度反方向调整权重参数</li>\n</ol>\n<p>相比于SVM、决策树等浅层算法，深度学习有更多的表示层，并且可以同时学习所有表示层（贪婪学习）。现在流行的两种方法：梯度提升机和深度学习。梯度提升机用于浅层学习，处理结构化数据；深度学习则用于图像处理等感知问题。</p>\n<p>传统机器学习和深度学习的区别：传统机器学习需要人工提取特征（特征工程），再利用分类算法（SVM、决策树等）输出结果；深度学习直接端到端学习，特征提取和分类全部由神经网络完成。</p>\n<p><img src=\"/images/Machine-vs-deep-learning-1.png\" alt=\"Machine-vs-deep-learning-1\" style=\"zoom:50%;\" /></p>\n<center><small>图片来源:trantorinc.com</small></center>\n\n<p><br/></p>\n<h2 id=\"神经网络基本构成\"><a href=\"#神经网络基本构成\" class=\"headerlink\" title=\"神经网络基本构成\"></a>神经网络基本构成</h2><hr>\n<h3 id=\"数据\"><a href=\"#数据\" class=\"headerlink\" title=\"数据\"></a>数据</h3><p>深度学习中的数据张量可以有几个轴，其中0轴一般都是<strong>样本轴/批量轴</strong>，下面说明常见的数据张量可以有哪些轴。</p>\n<ul>\n<li>2D张量：（samples，features）向量数据</li>\n<li>3D张量：（samples，timesteps，features）时间序列数据或序列数据</li>\n<li>4D张量：（samples，height，width，channels）图像</li>\n<li>5D张量：（samples，frames，height，width，channels）视频</li>\n</ul>\n<h3 id=\"层\"><a href=\"#层\" class=\"headerlink\" title=\"层\"></a>层</h3><p>不同维度的数据张量对应输入不同的层。2D张量通常用密集连接层（dense connected layer）/全连接层（fully connected layer）；3D张量通常用循环层（recurrent layer）；4D张量通常用二维卷积层（如Keras的Conc2D）。</p>\n<p>在Keras中，根据层兼容性（该层输入输出的特定张量形状），层与层可以像搭积木一样组合在一起。</p>\n<h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><p>层构成的网络称为模型。不同的模型其实就是具有不同拓扑结构的层网络。一些常见的网络拓扑结构有：</p>\n<ul>\n<li>线性结构：将单一输入映射为单一输出</li>\n<li>双分支机构（two-branch）网络</li>\n<li>多头网络（multihead）</li>\n<li>Inception模块</li>\n</ul>\n<p>在Keras中有两种定义模型的方法：</p>\n<ol>\n<li>Sequential类（仅用于层的线性堆叠）</li>\n<li>函数式API（用于层组成的有向无环图，可以构建任何形式的架构）</li>\n</ol>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>选择最合适的损失函数。比如二分类问题可以选择二元交叉熵（（binary cross entropy）；多分类问题可以用分类交叉熵（categorical cross entropy）；回归问题可以用均方误差（mean-square error）等。</p>\n<h3 id=\"优化器\"><a href=\"#优化器\" class=\"headerlink\" title=\"优化器\"></a>优化器</h3><p>优化方法的选择。</p>\n<p><br/></p>\n<h2 id=\"神经网络的学习算法\"><a href=\"#神经网络的学习算法\" class=\"headerlink\" title=\"神经网络的学习算法\"></a>神经网络的学习算法</h2><hr>\n<h3 id=\"人工神经网络（ANN）\"><a href=\"#人工神经网络（ANN）\" class=\"headerlink\" title=\"人工神经网络（ANN）\"></a>人工神经网络（ANN）</h3><ul>\n<li><h4 id=\"误差反向传播（BP）算法\"><a href=\"#误差反向传播（BP）算法\" class=\"headerlink\" title=\"误差反向传播（BP）算法\"></a>误差反向传播（BP）算法</h4><p>利用网络的输出与期望的误差，反向调整前面各层的权重，最终可以得到一个逼近期望的神经网络（Rumelhart, McCliland 1985）。</p>\n</li>\n<li><h4 id=\"Hopfield算法\"><a href=\"#Hopfield算法\" class=\"headerlink\" title=\"Hopfield算法\"></a>Hopfield算法</h4></li>\n<li><h4 id=\"自适应共振理论（ART）算法\"><a href=\"#自适应共振理论（ART）算法\" class=\"headerlink\" title=\"自适应共振理论（ART）算法\"></a>自适应共振理论（ART）算法</h4></li>\n<li><h4 id=\"自组织映射（SOM）算法\"><a href=\"#自组织映射（SOM）算法\" class=\"headerlink\" title=\"自组织映射（SOM）算法\"></a>自组织映射（SOM）算法</h4><p>两层拓扑结构，输入和输出层。输入可以是任意形式的，输出层（也称竞争层）一般是一维或二维的。</p>\n<p><img src=\"/images/IMG_0660.jpg\" alt=\"IMG_0660\" style=\"zoom:25%;\" /></p>\n<p><img src=\"/images/IMG_0661.jpg\" alt=\"IMG_0661\" style=\"zoom:50%;\" /></p>\n</li>\n</ul>\n<h3 id=\"卷积神经网络（CNN）\"><a href=\"#卷积神经网络（CNN）\" class=\"headerlink\" title=\"卷积神经网络（CNN）\"></a>卷积神经网络（CNN）</h3><p>卷积神经网络与普通人工神经网络相比增加了卷积层和池化层。<a href=\"https://m.youtube.com/watch?v=FmpDIaiMIeA\" target=\"_blank\" rel=\"noopener\">How Convolutional Neural Networks work-Brandon Rohrer</a></p>\n<ul>\n<li><h4 id=\"卷积（convolution）\"><a href=\"#卷积（convolution）\" class=\"headerlink\" title=\"卷积（convolution）\"></a>卷积（convolution）</h4><p><a href=\"https://www.matongxue.com/madocs/32.html\" target=\"_blank\" rel=\"noopener\">什么是卷积</a> </p>\n</li>\n<li><h4 id=\"池化（pooling）\"><a href=\"#池化（pooling）\" class=\"headerlink\" title=\"池化（pooling）\"></a>池化（pooling）</h4><p><a href=\"https://www.jiqizhixin.com/graph/technologies/0a4cedf0-0ee0-4406-946e-2877950da91d\" target=\"_blank\" rel=\"noopener\">什么是池化</a></p>\n</li>\n<li><h4 id=\"完全连接层（fully-connected-layer）\"><a href=\"#完全连接层（fully-connected-layer）\" class=\"headerlink\" title=\"完全连接层（fully connected layer）\"></a>完全连接层（fully connected layer）</h4><p>完全连接层的理想权重可以由Back Propagation方法得到。</p>\n</li>\n</ul>\n<p><br/></p>\n<h2 id=\"Keras实践\"><a href=\"#Keras实践\" class=\"headerlink\" title=\"Keras实践\"></a>Keras实践</h2><hr>\n<h3 id=\"准备数据\"><a href=\"#准备数据\" class=\"headerlink\" title=\"准备数据\"></a>准备数据</h3><p>对数据预处理，有时需要标准化。</p>\n<ul>\n<li><h4 id=\"训练集、验证集和测试集\"><a href=\"#训练集、验证集和测试集\" class=\"headerlink\" title=\"训练集、验证集和测试集\"></a>训练集、验证集和测试集</h4><p>数据需要分成三份：训练集、验证集和测试集。单独留出一份测试集，而不使用验证集做最后测试的原因是：除了机器最优化过程对权重参数的“学习”，人往往还要调整模型配置（成为<strong>超参数</strong>），比如选择层数、每层的大小，这本质上也是“学习”，特别是当多次重复“以验证集评估，修改模型配置”的过程，验证集的信息会泄露到模型中。</p>\n</li>\n</ul>\n<h3 id=\"选择模型\"><a href=\"#选择模型\" class=\"headerlink\" title=\"选择模型\"></a>选择模型</h3><p>包括层的配置，损失函数的选择，优化器选择</p>\n<h3 id=\"验证模型\"><a href=\"#验证模型\" class=\"headerlink\" title=\"验证模型\"></a>验证模型</h3><ul>\n<li><h4 id=\"简单留出验证\"><a href=\"#简单留出验证\" class=\"headerlink\" title=\"简单留出验证\"></a>简单留出验证</h4></li>\n<li><h4 id=\"K折验证\"><a href=\"#K折验证\" class=\"headerlink\" title=\"K折验证\"></a>K折验证</h4></li>\n<li><h4 id=\"重复K折验证\"><a href=\"#重复K折验证\" class=\"headerlink\" title=\"重复K折验证\"></a>重复K折验证</h4></li>\n</ul>\n<p>当数据集比较小时，可以考虑用K折验证的方法评估你的网络。</p>\n<p><br/></p>\n<h5 id=\"「过拟合的概念」\"><a href=\"#「过拟合的概念」\" class=\"headerlink\" title=\"「过拟合的概念」\"></a><em>「过拟合的概念」</em></h5><p><img src=\"/images/IMG_0287.PNG\" alt=\"IMG_0287\" style=\"zoom:24.3%;\" /> <img src=\"/images/IMG_0288.PNG\" alt=\"IMG_0288\" style=\"zoom:25%;\" /></p>\n<p>训练精度和验证精度刚开始回随着迭代次数增加而增加，但会到达一个临界点，训练精度虽然继续增加，但验证精度反而会下降，这叫做过拟合。</p>\n<p>应用深度学习需要同时理解：</p>\n<ol>\n<li>问题的动机和特点；</li>\n<li>将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理；</li>\n<li>在原始数据上拟合极复杂的深层模型的优化算法；</li>\n<li>有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能；</li>\n<li>为解决方案挑选合适的变量（超参数）组合的经验。</li>\n</ol>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>弗朗索瓦·肖莱《Python深度学习》人民邮电出版社</em></small></p>"},{"title":"AI学习笔记--安装Tensorflow框架","date":"2020-09-10T13:27:00.000Z","_content":"\nTensorflow是Google开发的开源机器学习框架，基于Tensorflow的Keras是一个更高级的API，Keras的易用性使得它对新手非常友好。<!--more-->另一个流行的深度学习框架是Facebook开发的Pytorch。关于深度学习框架的选择可参考这篇[Keras vs PyTorch：谁是「第一」深度学习框架？](https://www.jiqizhixin.com/articles/keras-or-pytorch)\n\n<br/>\n\n### Anaconda安装Tensorflow\n\n[Tensorflow官网](https://www.tensorflow.org/install?hl=zh-cn)给出的安装方式为pip安装，但其实用Anaconda也可以安装，conda强大的虚拟环境管理功能允许不同的项目、不同的框架独立区隔，便于管理。Anaconda的安装指导见[官网文档](https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html)。\n\n#### CPU版本\n\nCPU版本是在我的macOS系统上安装的。首先，为Tensorflow新建一个专门的python环境，取名为”tf“\n\n`conda create -n tf python=3.6`\n\n注意这里指定的python版本为3.6，这是因为**目前Tensorflow仅支持python 3.5-3.7的版本**（不装3.7的原因是anaconda官网源安装python3.7太太太慢了，但清华的镜像源最新只有python3.6的版本）。\n\n听说conda已经支持Tensorflow2.0了，但试了一下好像还是不行，直接conda install的话安装的是Tensorflow1.1.0。\n\n`conda install tensorflow`\n\n[Stack overflow上给出了一种解决办法](https://stackoverflow.com/questions/55392100/install-tensorflow-2-0-in-conda-enviroment)，先用conda安装，再用pip更新Tensorflow至2.0。\n\n`pip install --upgrade tensorflow==2.0.0`\n\n装好后可以`import tensorflow as tf`试一下，用`tf.__version__`查看版本。\n\n<br/>\n\n#### GPU版本\n\nGPU版本是在服务器上尝试安装的，直接conda安装的2.1.0版本，竟然异常的顺利。\n\n`conda create -n tf python=3.7`\n\n`conda install tensorflow-gpu=2.1.0`\n\nTensorflow在GPU上使用还需要一些支持的驱动程序和加速库，包括CUDA 和 cuDNN等。Tensorflow官网建议使用[Docker容器](https://www.tensorflow.org/install/docker?hl=zh-cn)为Tensorflow提供GPU支持和虚拟环境。一些论坛上还有建议[手动为Tensorflow-GPU配置支持](https://zhuanlan.zhihu.com/p/60924644)\n\n的。但是，conda其实已经为我们做了这些工作，conda install时已经包含了cudatoolkit、cudnn这两个包。也是异常的顺利。\n\n `tf.__version__ ` 检查Tensorflow版本\n\n `tf.test.is_gpu_available()`  检查Tensorflow-GPU是否可用\n\n检查的结果是可以的，但用MXnet官网的方法pip安装了MXnet之后gpu就都不能用了：\n\n![image-20200923171733370](/images/image-20200923171733370.png)\n\n报错显示nvidia的gpu driver不在运行。\n\n`nvidia-smi`检查gpu driver的状态\n\n```\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n似乎服务器上没有装gpu driver，不知道是环境变量被修改了还是什么。关于这个问题Stack overflow有[解答](https://stackoverflow.com/questions/56470424/nvcc-missing-when-installing-cudatoolkit)，conda安装的cuda是不包含gpu driver的，下面要自己装一下gpu driver，参考的是知乎的[这篇](https://zhuanlan.zhihu.com/p/59618999)。\n\n检查GPU及推荐的服务器：` ubuntu-drivers devices`，显示\n\n```\n== /sys/devices/pci0000:53/0000:53:00.0/0000:54:00.0/0000:55:08.0/0000:5a:00.0 ==\nmodalias : pci:v000010DEd00001E04sv000010DEsd000012FAbc03sc00i00\nvendor  : NVIDIA Corporation\nmodel  : TU102 [GeForce RTX 2080 Ti]\ndriver  : nvidia-driver-435 - distro non-free\ndriver  : nvidia-driver-440-server - distro non-free\ndriver  : nvidia-driver-450 - distro non-free recommended\ndriver  : nvidia-driver-450-server - distro non-free\ndriver  : nvidia-driver-418-server - distro non-free\ndriver  : xserver-xorg-video-nouveau - distro free builtin\n```\n\n看到这里推荐的是nvidia-driver-450，使用下面的指令自动安装\n\n```\nsudo ubuntu-drivers autoinstall\n```\n\n安装后重启系统（`sudo shutdown -r now `），输入`nvidia-smi`检查是否安装成功，若成功会显示GPU的信息。`dpkg -l | grep nvidia`显示相关nvidia相关包的信息。\n\n再次测试tensorflow是否可用GPU，这次显示\n\n<img src=\"/images/image-20201012154534615.png\" alt=\"image-20201012154534615\" style=\"zoom:80%;\" />\n\n若系统上曾经装过gpu driver，没有卸载干净之前的就装新的会很mess，nvida官方也建议卸载干净旧版再装新版，相关问题看[这篇](https://forums.developer.nvidia.com/t/nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver-ubuntu-16-04/48635)。因为之前安装的nvidia driver导致无法安装新版本的情况，参考[这篇](https://unix.stackexchange.com/questions/620141/cant-install-nvidia-driver-455-upgrade-from-450-version)。\n\n---\n\n重装过nivdia gpu driver之后，今天发现又`nvidia-smi`又找不到driver了，找了半天原因，发现不知道什么时候ubuntu的内核偷偷更新过了……以前是5.4.0-48-generic，现在是5.4.0-53-generic。内核更新后就和原来的driver不匹配了，参考[这篇](https://codeleading.com/article/2604429676/)，和[这篇](https://www.pianshen.com/article/7487258025/)。\n\n`uname -r`查看当前内核版本\n\n`dkms status`    查看匹配的ubuntu内核和nvidia驱动版本\n\n`grep menuentry /boot/grub/grub.cfg`查看系统有哪些内核可用\n\n![ubuntu_core](/images/ubuntu_core.png)\n\n找到旧版的5.4.0-48-generic内核是'Advanced options for Ubuntu'下的第3个menuentry，所以`sudo vi /etc/default/grub`，找到\n\n```\nGRUB_DEFAULT=0\n```\n\n改为\n\n```\nGRUB_DEFAULT=\"1 >2\"\n```\n\n2是第3个menuentry的编号。\n\n更新一下\n\n```\nsudo update-grub\n```\n\n之后`sudo reboot`重启，`nvidia-smi`就可以用啦。\n\n<br/>\n\n<br/>\n\n<br/>","source":"_posts/Anaconda_install_Tensorflow.md","raw":"---\ntitle: AI学习笔记--安装Tensorflow框架\ndate: 2020-09-10 21:27:00\ncategories:\n- 计算机科学\ntags: \n- python\n- 人工智能\n- 机器学习\n- 深度学习\n---\n\nTensorflow是Google开发的开源机器学习框架，基于Tensorflow的Keras是一个更高级的API，Keras的易用性使得它对新手非常友好。<!--more-->另一个流行的深度学习框架是Facebook开发的Pytorch。关于深度学习框架的选择可参考这篇[Keras vs PyTorch：谁是「第一」深度学习框架？](https://www.jiqizhixin.com/articles/keras-or-pytorch)\n\n<br/>\n\n### Anaconda安装Tensorflow\n\n[Tensorflow官网](https://www.tensorflow.org/install?hl=zh-cn)给出的安装方式为pip安装，但其实用Anaconda也可以安装，conda强大的虚拟环境管理功能允许不同的项目、不同的框架独立区隔，便于管理。Anaconda的安装指导见[官网文档](https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html)。\n\n#### CPU版本\n\nCPU版本是在我的macOS系统上安装的。首先，为Tensorflow新建一个专门的python环境，取名为”tf“\n\n`conda create -n tf python=3.6`\n\n注意这里指定的python版本为3.6，这是因为**目前Tensorflow仅支持python 3.5-3.7的版本**（不装3.7的原因是anaconda官网源安装python3.7太太太慢了，但清华的镜像源最新只有python3.6的版本）。\n\n听说conda已经支持Tensorflow2.0了，但试了一下好像还是不行，直接conda install的话安装的是Tensorflow1.1.0。\n\n`conda install tensorflow`\n\n[Stack overflow上给出了一种解决办法](https://stackoverflow.com/questions/55392100/install-tensorflow-2-0-in-conda-enviroment)，先用conda安装，再用pip更新Tensorflow至2.0。\n\n`pip install --upgrade tensorflow==2.0.0`\n\n装好后可以`import tensorflow as tf`试一下，用`tf.__version__`查看版本。\n\n<br/>\n\n#### GPU版本\n\nGPU版本是在服务器上尝试安装的，直接conda安装的2.1.0版本，竟然异常的顺利。\n\n`conda create -n tf python=3.7`\n\n`conda install tensorflow-gpu=2.1.0`\n\nTensorflow在GPU上使用还需要一些支持的驱动程序和加速库，包括CUDA 和 cuDNN等。Tensorflow官网建议使用[Docker容器](https://www.tensorflow.org/install/docker?hl=zh-cn)为Tensorflow提供GPU支持和虚拟环境。一些论坛上还有建议[手动为Tensorflow-GPU配置支持](https://zhuanlan.zhihu.com/p/60924644)\n\n的。但是，conda其实已经为我们做了这些工作，conda install时已经包含了cudatoolkit、cudnn这两个包。也是异常的顺利。\n\n `tf.__version__ ` 检查Tensorflow版本\n\n `tf.test.is_gpu_available()`  检查Tensorflow-GPU是否可用\n\n检查的结果是可以的，但用MXnet官网的方法pip安装了MXnet之后gpu就都不能用了：\n\n![image-20200923171733370](/images/image-20200923171733370.png)\n\n报错显示nvidia的gpu driver不在运行。\n\n`nvidia-smi`检查gpu driver的状态\n\n```\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n似乎服务器上没有装gpu driver，不知道是环境变量被修改了还是什么。关于这个问题Stack overflow有[解答](https://stackoverflow.com/questions/56470424/nvcc-missing-when-installing-cudatoolkit)，conda安装的cuda是不包含gpu driver的，下面要自己装一下gpu driver，参考的是知乎的[这篇](https://zhuanlan.zhihu.com/p/59618999)。\n\n检查GPU及推荐的服务器：` ubuntu-drivers devices`，显示\n\n```\n== /sys/devices/pci0000:53/0000:53:00.0/0000:54:00.0/0000:55:08.0/0000:5a:00.0 ==\nmodalias : pci:v000010DEd00001E04sv000010DEsd000012FAbc03sc00i00\nvendor  : NVIDIA Corporation\nmodel  : TU102 [GeForce RTX 2080 Ti]\ndriver  : nvidia-driver-435 - distro non-free\ndriver  : nvidia-driver-440-server - distro non-free\ndriver  : nvidia-driver-450 - distro non-free recommended\ndriver  : nvidia-driver-450-server - distro non-free\ndriver  : nvidia-driver-418-server - distro non-free\ndriver  : xserver-xorg-video-nouveau - distro free builtin\n```\n\n看到这里推荐的是nvidia-driver-450，使用下面的指令自动安装\n\n```\nsudo ubuntu-drivers autoinstall\n```\n\n安装后重启系统（`sudo shutdown -r now `），输入`nvidia-smi`检查是否安装成功，若成功会显示GPU的信息。`dpkg -l | grep nvidia`显示相关nvidia相关包的信息。\n\n再次测试tensorflow是否可用GPU，这次显示\n\n<img src=\"/images/image-20201012154534615.png\" alt=\"image-20201012154534615\" style=\"zoom:80%;\" />\n\n若系统上曾经装过gpu driver，没有卸载干净之前的就装新的会很mess，nvida官方也建议卸载干净旧版再装新版，相关问题看[这篇](https://forums.developer.nvidia.com/t/nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver-ubuntu-16-04/48635)。因为之前安装的nvidia driver导致无法安装新版本的情况，参考[这篇](https://unix.stackexchange.com/questions/620141/cant-install-nvidia-driver-455-upgrade-from-450-version)。\n\n---\n\n重装过nivdia gpu driver之后，今天发现又`nvidia-smi`又找不到driver了，找了半天原因，发现不知道什么时候ubuntu的内核偷偷更新过了……以前是5.4.0-48-generic，现在是5.4.0-53-generic。内核更新后就和原来的driver不匹配了，参考[这篇](https://codeleading.com/article/2604429676/)，和[这篇](https://www.pianshen.com/article/7487258025/)。\n\n`uname -r`查看当前内核版本\n\n`dkms status`    查看匹配的ubuntu内核和nvidia驱动版本\n\n`grep menuentry /boot/grub/grub.cfg`查看系统有哪些内核可用\n\n![ubuntu_core](/images/ubuntu_core.png)\n\n找到旧版的5.4.0-48-generic内核是'Advanced options for Ubuntu'下的第3个menuentry，所以`sudo vi /etc/default/grub`，找到\n\n```\nGRUB_DEFAULT=0\n```\n\n改为\n\n```\nGRUB_DEFAULT=\"1 >2\"\n```\n\n2是第3个menuentry的编号。\n\n更新一下\n\n```\nsudo update-grub\n```\n\n之后`sudo reboot`重启，`nvidia-smi`就可以用啦。\n\n<br/>\n\n<br/>\n\n<br/>","slug":"Anaconda_install_Tensorflow","published":1,"updated":"2022-05-07T07:22:15.953Z","_id":"ckyoogd5r000i2sfyazuaafml","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Tensorflow是Google开发的开源机器学习框架，基于Tensorflow的Keras是一个更高级的API，Keras的易用性使得它对新手非常友好。<a id=\"more\"></a>另一个流行的深度学习框架是Facebook开发的Pytorch。关于深度学习框架的选择可参考这篇<a href=\"https://www.jiqizhixin.com/articles/keras-or-pytorch\" target=\"_blank\" rel=\"noopener\">Keras vs PyTorch：谁是「第一」深度学习框架？</a></p>\n<p><br/></p>\n<h3 id=\"Anaconda安装Tensorflow\"><a href=\"#Anaconda安装Tensorflow\" class=\"headerlink\" title=\"Anaconda安装Tensorflow\"></a>Anaconda安装Tensorflow</h3><p><a href=\"https://www.tensorflow.org/install?hl=zh-cn\" target=\"_blank\" rel=\"noopener\">Tensorflow官网</a>给出的安装方式为pip安装，但其实用Anaconda也可以安装，conda强大的虚拟环境管理功能允许不同的项目、不同的框架独立区隔，便于管理。Anaconda的安装指导见<a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html\" target=\"_blank\" rel=\"noopener\">官网文档</a>。</p>\n<h4 id=\"CPU版本\"><a href=\"#CPU版本\" class=\"headerlink\" title=\"CPU版本\"></a>CPU版本</h4><p>CPU版本是在我的macOS系统上安装的。首先，为Tensorflow新建一个专门的python环境，取名为”tf“</p>\n<p><code>conda create -n tf python=3.6</code></p>\n<p>注意这里指定的python版本为3.6，这是因为<strong>目前Tensorflow仅支持python 3.5-3.7的版本</strong>（不装3.7的原因是anaconda官网源安装python3.7太太太慢了，但清华的镜像源最新只有python3.6的版本）。</p>\n<p>听说conda已经支持Tensorflow2.0了，但试了一下好像还是不行，直接conda install的话安装的是Tensorflow1.1.0。</p>\n<p><code>conda install tensorflow</code></p>\n<p><a href=\"https://stackoverflow.com/questions/55392100/install-tensorflow-2-0-in-conda-enviroment\" target=\"_blank\" rel=\"noopener\">Stack overflow上给出了一种解决办法</a>，先用conda安装，再用pip更新Tensorflow至2.0。</p>\n<p><code>pip install --upgrade tensorflow==2.0.0</code></p>\n<p>装好后可以<code>import tensorflow as tf</code>试一下，用<code>tf.__version__</code>查看版本。</p>\n<p><br/></p>\n<h4 id=\"GPU版本\"><a href=\"#GPU版本\" class=\"headerlink\" title=\"GPU版本\"></a>GPU版本</h4><p>GPU版本是在服务器上尝试安装的，直接conda安装的2.1.0版本，竟然异常的顺利。</p>\n<p><code>conda create -n tf python=3.7</code></p>\n<p><code>conda install tensorflow-gpu=2.1.0</code></p>\n<p>Tensorflow在GPU上使用还需要一些支持的驱动程序和加速库，包括CUDA 和 cuDNN等。Tensorflow官网建议使用<a href=\"https://www.tensorflow.org/install/docker?hl=zh-cn\" target=\"_blank\" rel=\"noopener\">Docker容器</a>为Tensorflow提供GPU支持和虚拟环境。一些论坛上还有建议<a href=\"https://zhuanlan.zhihu.com/p/60924644\" target=\"_blank\" rel=\"noopener\">手动为Tensorflow-GPU配置支持</a></p>\n<p>的。但是，conda其实已经为我们做了这些工作，conda install时已经包含了cudatoolkit、cudnn这两个包。也是异常的顺利。</p>\n<p> <code>tf.__version__</code> 检查Tensorflow版本</p>\n<p> <code>tf.test.is_gpu_available()</code>  检查Tensorflow-GPU是否可用</p>\n<p>检查的结果是可以的，但用MXnet官网的方法pip安装了MXnet之后gpu就都不能用了：</p>\n<p><img src=\"/images/image-20200923171733370.png\" alt=\"image-20200923171733370\"></p>\n<p>报错显示nvidia的gpu driver不在运行。</p>\n<p><code>nvidia-smi</code>检查gpu driver的状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</span><br></pre></td></tr></table></figure>\n<p>似乎服务器上没有装gpu driver，不知道是环境变量被修改了还是什么。关于这个问题Stack overflow有<a href=\"https://stackoverflow.com/questions/56470424/nvcc-missing-when-installing-cudatoolkit\" target=\"_blank\" rel=\"noopener\">解答</a>，conda安装的cuda是不包含gpu driver的，下面要自己装一下gpu driver，参考的是知乎的<a href=\"https://zhuanlan.zhihu.com/p/59618999\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<p>检查GPU及推荐的服务器：<code>ubuntu-drivers devices</code>，显示</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D; &#x2F;sys&#x2F;devices&#x2F;pci0000:53&#x2F;0000:53:00.0&#x2F;0000:54:00.0&#x2F;0000:55:08.0&#x2F;0000:5a:00.0 &#x3D;&#x3D;</span><br><span class=\"line\">modalias : pci:v000010DEd00001E04sv000010DEsd000012FAbc03sc00i00</span><br><span class=\"line\">vendor  : NVIDIA Corporation</span><br><span class=\"line\">model  : TU102 [GeForce RTX 2080 Ti]</span><br><span class=\"line\">driver  : nvidia-driver-435 - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-440-server - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-450 - distro non-free recommended</span><br><span class=\"line\">driver  : nvidia-driver-450-server - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-418-server - distro non-free</span><br><span class=\"line\">driver  : xserver-xorg-video-nouveau - distro free builtin</span><br></pre></td></tr></table></figure>\n<p>看到这里推荐的是nvidia-driver-450，使用下面的指令自动安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ubuntu-drivers autoinstall</span><br></pre></td></tr></table></figure>\n<p>安装后重启系统（<code>sudo shutdown -r now</code>），输入<code>nvidia-smi</code>检查是否安装成功，若成功会显示GPU的信息。<code>dpkg -l | grep nvidia</code>显示相关nvidia相关包的信息。</p>\n<p>再次测试tensorflow是否可用GPU，这次显示</p>\n<p><img src=\"/images/image-20201012154534615.png\" alt=\"image-20201012154534615\" style=\"zoom:80%;\" /></p>\n<p>若系统上曾经装过gpu driver，没有卸载干净之前的就装新的会很mess，nvida官方也建议卸载干净旧版再装新版，相关问题看<a href=\"https://forums.developer.nvidia.com/t/nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver-ubuntu-16-04/48635\" target=\"_blank\" rel=\"noopener\">这篇</a>。因为之前安装的nvidia driver导致无法安装新版本的情况，参考<a href=\"https://unix.stackexchange.com/questions/620141/cant-install-nvidia-driver-455-upgrade-from-450-version\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<hr>\n<p>重装过nivdia gpu driver之后，今天发现又<code>nvidia-smi</code>又找不到driver了，找了半天原因，发现不知道什么时候ubuntu的内核偷偷更新过了……以前是5.4.0-48-generic，现在是5.4.0-53-generic。内核更新后就和原来的driver不匹配了，参考<a href=\"https://codeleading.com/article/2604429676/\" target=\"_blank\" rel=\"noopener\">这篇</a>，和<a href=\"https://www.pianshen.com/article/7487258025/\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<p><code>uname -r</code>查看当前内核版本</p>\n<p><code>dkms status</code>    查看匹配的ubuntu内核和nvidia驱动版本</p>\n<p><code>grep menuentry /boot/grub/grub.cfg</code>查看系统有哪些内核可用</p>\n<p><img src=\"/images/ubuntu_core.png\" alt=\"ubuntu_core\"></p>\n<p>找到旧版的5.4.0-48-generic内核是’Advanced options for Ubuntu’下的第3个menuentry，所以<code>sudo vi /etc/default/grub</code>，找到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_DEFAULT&#x3D;0</span><br></pre></td></tr></table></figure>\n<p>改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_DEFAULT&#x3D;&quot;1 &gt;2&quot;</span><br></pre></td></tr></table></figure>\n<p>2是第3个menuentry的编号。</p>\n<p>更新一下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo update-grub</span><br></pre></td></tr></table></figure>\n<p>之后<code>sudo reboot</code>重启，<code>nvidia-smi</code>就可以用啦。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n","site":{"data":{}},"excerpt":"<p>Tensorflow是Google开发的开源机器学习框架，基于Tensorflow的Keras是一个更高级的API，Keras的易用性使得它对新手非常友好。","more":"另一个流行的深度学习框架是Facebook开发的Pytorch。关于深度学习框架的选择可参考这篇<a href=\"https://www.jiqizhixin.com/articles/keras-or-pytorch\" target=\"_blank\" rel=\"noopener\">Keras vs PyTorch：谁是「第一」深度学习框架？</a></p>\n<p><br/></p>\n<h3 id=\"Anaconda安装Tensorflow\"><a href=\"#Anaconda安装Tensorflow\" class=\"headerlink\" title=\"Anaconda安装Tensorflow\"></a>Anaconda安装Tensorflow</h3><p><a href=\"https://www.tensorflow.org/install?hl=zh-cn\" target=\"_blank\" rel=\"noopener\">Tensorflow官网</a>给出的安装方式为pip安装，但其实用Anaconda也可以安装，conda强大的虚拟环境管理功能允许不同的项目、不同的框架独立区隔，便于管理。Anaconda的安装指导见<a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html\" target=\"_blank\" rel=\"noopener\">官网文档</a>。</p>\n<h4 id=\"CPU版本\"><a href=\"#CPU版本\" class=\"headerlink\" title=\"CPU版本\"></a>CPU版本</h4><p>CPU版本是在我的macOS系统上安装的。首先，为Tensorflow新建一个专门的python环境，取名为”tf“</p>\n<p><code>conda create -n tf python=3.6</code></p>\n<p>注意这里指定的python版本为3.6，这是因为<strong>目前Tensorflow仅支持python 3.5-3.7的版本</strong>（不装3.7的原因是anaconda官网源安装python3.7太太太慢了，但清华的镜像源最新只有python3.6的版本）。</p>\n<p>听说conda已经支持Tensorflow2.0了，但试了一下好像还是不行，直接conda install的话安装的是Tensorflow1.1.0。</p>\n<p><code>conda install tensorflow</code></p>\n<p><a href=\"https://stackoverflow.com/questions/55392100/install-tensorflow-2-0-in-conda-enviroment\" target=\"_blank\" rel=\"noopener\">Stack overflow上给出了一种解决办法</a>，先用conda安装，再用pip更新Tensorflow至2.0。</p>\n<p><code>pip install --upgrade tensorflow==2.0.0</code></p>\n<p>装好后可以<code>import tensorflow as tf</code>试一下，用<code>tf.__version__</code>查看版本。</p>\n<p><br/></p>\n<h4 id=\"GPU版本\"><a href=\"#GPU版本\" class=\"headerlink\" title=\"GPU版本\"></a>GPU版本</h4><p>GPU版本是在服务器上尝试安装的，直接conda安装的2.1.0版本，竟然异常的顺利。</p>\n<p><code>conda create -n tf python=3.7</code></p>\n<p><code>conda install tensorflow-gpu=2.1.0</code></p>\n<p>Tensorflow在GPU上使用还需要一些支持的驱动程序和加速库，包括CUDA 和 cuDNN等。Tensorflow官网建议使用<a href=\"https://www.tensorflow.org/install/docker?hl=zh-cn\" target=\"_blank\" rel=\"noopener\">Docker容器</a>为Tensorflow提供GPU支持和虚拟环境。一些论坛上还有建议<a href=\"https://zhuanlan.zhihu.com/p/60924644\" target=\"_blank\" rel=\"noopener\">手动为Tensorflow-GPU配置支持</a></p>\n<p>的。但是，conda其实已经为我们做了这些工作，conda install时已经包含了cudatoolkit、cudnn这两个包。也是异常的顺利。</p>\n<p> <code>tf.__version__</code> 检查Tensorflow版本</p>\n<p> <code>tf.test.is_gpu_available()</code>  检查Tensorflow-GPU是否可用</p>\n<p>检查的结果是可以的，但用MXnet官网的方法pip安装了MXnet之后gpu就都不能用了：</p>\n<p><img src=\"/images/image-20200923171733370.png\" alt=\"image-20200923171733370\"></p>\n<p>报错显示nvidia的gpu driver不在运行。</p>\n<p><code>nvidia-smi</code>检查gpu driver的状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</span><br></pre></td></tr></table></figure>\n<p>似乎服务器上没有装gpu driver，不知道是环境变量被修改了还是什么。关于这个问题Stack overflow有<a href=\"https://stackoverflow.com/questions/56470424/nvcc-missing-when-installing-cudatoolkit\" target=\"_blank\" rel=\"noopener\">解答</a>，conda安装的cuda是不包含gpu driver的，下面要自己装一下gpu driver，参考的是知乎的<a href=\"https://zhuanlan.zhihu.com/p/59618999\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<p>检查GPU及推荐的服务器：<code>ubuntu-drivers devices</code>，显示</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D; &#x2F;sys&#x2F;devices&#x2F;pci0000:53&#x2F;0000:53:00.0&#x2F;0000:54:00.0&#x2F;0000:55:08.0&#x2F;0000:5a:00.0 &#x3D;&#x3D;</span><br><span class=\"line\">modalias : pci:v000010DEd00001E04sv000010DEsd000012FAbc03sc00i00</span><br><span class=\"line\">vendor  : NVIDIA Corporation</span><br><span class=\"line\">model  : TU102 [GeForce RTX 2080 Ti]</span><br><span class=\"line\">driver  : nvidia-driver-435 - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-440-server - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-450 - distro non-free recommended</span><br><span class=\"line\">driver  : nvidia-driver-450-server - distro non-free</span><br><span class=\"line\">driver  : nvidia-driver-418-server - distro non-free</span><br><span class=\"line\">driver  : xserver-xorg-video-nouveau - distro free builtin</span><br></pre></td></tr></table></figure>\n<p>看到这里推荐的是nvidia-driver-450，使用下面的指令自动安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ubuntu-drivers autoinstall</span><br></pre></td></tr></table></figure>\n<p>安装后重启系统（<code>sudo shutdown -r now</code>），输入<code>nvidia-smi</code>检查是否安装成功，若成功会显示GPU的信息。<code>dpkg -l | grep nvidia</code>显示相关nvidia相关包的信息。</p>\n<p>再次测试tensorflow是否可用GPU，这次显示</p>\n<p><img src=\"/images/image-20201012154534615.png\" alt=\"image-20201012154534615\" style=\"zoom:80%;\" /></p>\n<p>若系统上曾经装过gpu driver，没有卸载干净之前的就装新的会很mess，nvida官方也建议卸载干净旧版再装新版，相关问题看<a href=\"https://forums.developer.nvidia.com/t/nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver-ubuntu-16-04/48635\" target=\"_blank\" rel=\"noopener\">这篇</a>。因为之前安装的nvidia driver导致无法安装新版本的情况，参考<a href=\"https://unix.stackexchange.com/questions/620141/cant-install-nvidia-driver-455-upgrade-from-450-version\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<hr>\n<p>重装过nivdia gpu driver之后，今天发现又<code>nvidia-smi</code>又找不到driver了，找了半天原因，发现不知道什么时候ubuntu的内核偷偷更新过了……以前是5.4.0-48-generic，现在是5.4.0-53-generic。内核更新后就和原来的driver不匹配了，参考<a href=\"https://codeleading.com/article/2604429676/\" target=\"_blank\" rel=\"noopener\">这篇</a>，和<a href=\"https://www.pianshen.com/article/7487258025/\" target=\"_blank\" rel=\"noopener\">这篇</a>。</p>\n<p><code>uname -r</code>查看当前内核版本</p>\n<p><code>dkms status</code>    查看匹配的ubuntu内核和nvidia驱动版本</p>\n<p><code>grep menuentry /boot/grub/grub.cfg</code>查看系统有哪些内核可用</p>\n<p><img src=\"/images/ubuntu_core.png\" alt=\"ubuntu_core\"></p>\n<p>找到旧版的5.4.0-48-generic内核是’Advanced options for Ubuntu’下的第3个menuentry，所以<code>sudo vi /etc/default/grub</code>，找到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_DEFAULT&#x3D;0</span><br></pre></td></tr></table></figure>\n<p>改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_DEFAULT&#x3D;&quot;1 &gt;2&quot;</span><br></pre></td></tr></table></figure>\n<p>2是第3个menuentry的编号。</p>\n<p>更新一下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo update-grub</span><br></pre></td></tr></table></figure>\n<p>之后<code>sudo reboot</code>重启，<code>nvidia-smi</code>就可以用啦。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>"},{"title":"Linear and Nonlinear","date":"2021-11-28T14:07:00.000Z","mathjax":true,"_content":"\nIf we know it's a linear relationship: given correlation, var of x, var of y, we can deduce that $y=\\beta x+\\epsilon$.<!--more-->\n\nNow, if we know it's a nonlinear relationship: given correlation information (say, mutual information), can we conduct similar deductions in the context of nonlinear? \n\nUnfortunately, no. We can't deduce y based on x and multual information. \n\nIf linear is a male elephant, given the knowledge that this creature has two huge ears, we can safely deduce that it also has a long nose. However, nonlinear is not a female elephant, instead it is the whole animal world. It can be a pink pig with two big ears but a short nose, or 大耳朵图图 who has only a hat between his eyes.\n\n<br/><br/><br/>","source":"_posts/Linear and nolinear.md","raw":"---\ntitle: Linear and Nonlinear\ndate: 2021-11-28 22:07:00\ncategories:\n- 杂想\ntags: \n- 数学\nmathjax: true\n---\n\nIf we know it's a linear relationship: given correlation, var of x, var of y, we can deduce that $y=\\beta x+\\epsilon$.<!--more-->\n\nNow, if we know it's a nonlinear relationship: given correlation information (say, mutual information), can we conduct similar deductions in the context of nonlinear? \n\nUnfortunately, no. We can't deduce y based on x and multual information. \n\nIf linear is a male elephant, given the knowledge that this creature has two huge ears, we can safely deduce that it also has a long nose. However, nonlinear is not a female elephant, instead it is the whole animal world. It can be a pink pig with two big ears but a short nose, or 大耳朵图图 who has only a hat between his eyes.\n\n<br/><br/><br/>","slug":"Linear and nolinear","published":1,"updated":"2021-12-01T11:40:54.852Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5v000m2sfy9iwb1wrv","content":"<p>If we know it’s a linear relationship: given correlation, var of x, var of y, we can deduce that $y=\\beta x+\\epsilon$.<a id=\"more\"></a></p>\n<p>Now, if we know it’s a nonlinear relationship: given correlation information (say, mutual information), can we conduct similar deductions in the context of nonlinear? </p>\n<p>Unfortunately, no. We can’t deduce y based on x and multual information. </p>\n<p>If linear is a male elephant, given the knowledge that this creature has two huge ears, we can safely deduce that it also has a long nose. However, nonlinear is not a female elephant, instead it is the whole animal world. It can be a pink pig with two big ears but a short nose, or 大耳朵图图 who has only a hat between his eyes.</p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>If we know it’s a linear relationship: given correlation, var of x, var of y, we can deduce that $y=\\beta x+\\epsilon$.","more":"</p>\n<p>Now, if we know it’s a nonlinear relationship: given correlation information (say, mutual information), can we conduct similar deductions in the context of nonlinear? </p>\n<p>Unfortunately, no. We can’t deduce y based on x and multual information. </p>\n<p>If linear is a male elephant, given the knowledge that this creature has two huge ears, we can safely deduce that it also has a long nose. However, nonlinear is not a female elephant, instead it is the whole animal world. It can be a pink pig with two big ears but a short nose, or 大耳朵图图 who has only a hat between his eyes.</p>\n<p><br/><br/><br/></p>"},{"title":"Python 基础 (一)","date":"2020-02-11T16:39:00.000Z","mathjax":true,"_content":"\npython 解释风格、python 变量和数据类型、python 基本运算、python 模块<!--more-->\n\n\n\n---\n\n### Python 解释风格\n\n1. #### 注释\n\n   ```python\n   # this is annotation\n   ```\n\n2. #### 缩进\n\n   ```python\n   if i > 0:\n       x = 1 # 有4个空格的缩进\n   elif i == 0:\n       x = 2\n   else i < 0:\n       x = 3\n   ```\n\n3. #### 续行\n\n   * 续行符\\\n\n   ```python\n   if i > 0 and\\\n       j < 0:\n       x = 1\n   ```\n\n   * 无需续行符\n\n   括号内部、三引号内部'''无需续行符直接换行\n   \n   \n\n---\n\n### Python 变量和数据类型\n\n1. #### 动态的强类型语言\n\n   ```python\n   # python是动态强类型语言，变量无需声明，自动确定数据类型\n   >>>pi = 3.1415926\n   >>>PI = 'apple pie'\n   >>>print (type(pi))\n   >>>print (type(PI))\n   \n   <type 'float'>\n   <type 'str'>\n   ```\n\n2. #### 基本数据类型\n\n   * ##### 整型（integer）\n\n   * ##### 浮点型（float）\n\n   * ##### 布尔型（boolean）True，False\n\n   * ##### 复数型（complex）\n\n     ```python\n     2+3j #虚数部分必须加j\n     ```\n\n   * ##### 序列（sequence）\n\n     * 字符串（string）--不可变类型\n\n       ```python\n       >>>str = 'this is a string'\n       >>>str = \"this is also a string\"\n       >>>str = ''' this is a string, too'''\n       \n       # 当要输出含双引号的字符串时，可用单引号包裹，反之亦然\n       >>> astr = 'a \"blue\" apple'\n       >>> print(astr)\n       a \"blue\" apple\n       \n       >>> bstr = \"a 'black' orange\"\n       >>> print(bstr)\n       a 'black' orange\n       \n       # 三引号内可以实现方便的换行，不需要转义符\n       >>> cstr = '''Who is he?\n       ... James Bond'''\n       >>> print(cstr)\n       Who is he?\n       James Bond\n       ```\n   \n     * 列表（list）--可变类型\n   \n       ```python\n       >>>list = [123, 'sun', True]\n       >>>print (type(list))\n       \n       <type 'list'>\n       \n       >>>''.join(list)        # 列表转字符串\n       ```\n   \n     * 元组（tuple）--不可变类型\n   \n       ```python\n       >>>tuple = (213, 'sun', True)\n       >>>print (type(tuple))\n            \n       <type 'tuple'>\n       ```\n   \n     * 引用方式 [下限:上限:步长]\n   \n       ```python\n       # 列表下标从 0 开始\n       >>>print (list[0])\n       123\n       \n       # 若写明上限，则上限不包括在内\n       >>>s1[:5]     # 从开始到下标4 （下标5的元素不包括在内）\n       >>>s1[2:]     # 从下标2到最后\n       >>>s1[0:5:2]  # 从下标0到下标4 (5不包括在内)，每隔2取一个元素\n       >>>s1[::-1]   # 倒序 相当于s1[-1:-len(s1)-1:-1]（缺省默认填值）\n       \n       # 尾部元素引用\n       >>>s1[-1]     # 序列最后一个元素\n       >>>s1[-3]     # 序列倒数第三个元素\n       ```\n       \n     * 重复 sequence*copies\n   \n       ```python\n       >>>'apple'*3\n       'appleappleapple'\n       \n       >>>[0]*3\n       '[0,0,0]'\n       ```\n   \n     * 拼接 sequence+sequence\n   \n       ```python\n       >>>print('pine'+'apple')\n       'pineapple'\n       \n       >>>print([1,2,3]+[4,5,6])\n       [1, 2, 3, 4, 5, 6]\n       \n       >>>print((1,2,3)+(4,5,6))\n       (1, 2, 3, 4, 5, 6)\n       ```\n   \n     * 判断成员 in，not in\n   \n       ```python\n       >>>namelist = ['Jack','Russell','Mary']\n       >>>'Russell' in namelist\n       True\n       ```\n   \n     * 排序sorted，倒序reversed\n   \n       ```python\n       >>>a = [3,6,1,9]\n       >>>print(sorted(a))       \n       [1, 3, 6, 9]\n       >>>print(a)\n       [3, 6, 1, 9]                # sorted()函数不改变原对象\n       \n       >>>a.sort()                  \n       >>>print(a)\n       [1, 3, 6, 9]                # .sort()方法对原对象进行改动\n       \n       # reversed()和.reverse()同理\n       ```\n   \n   * #####  字典（dictionary）\n   \n      字典和序列一样，是可以储存多个元素的类（称为容器），不同的是字典包含映射关系：字典的元素有两部分——“键”和“值”；且字典没有顺序。\n   \n      ```python\n      dic = {'name':'orange', 'shape':'sphere', 'price':2.6}\n      print (dic['price'])         # 字典通过“键”引用\n      \n      dic['color'] = 'orange'      # 可在字典中加入新的元素\n      \n      for key in dic:              # 在循环中，也是通过“键”引用值\n          print (dic[key])\n      ```\n      \n      创建字典的方法\n      \n      ```python\n      # dict()函数\n      dict([('Ola',23),('Kary','24'),('Mike','22')])\n      dict([['Ola',23],['Kary','24'],['Mike','22']])\n      dict((('Ola',23),('Kary','24'),('Mike','22')))\n      dict(Ola = 23), Kary = 24, Mike = 22)\n      dict(zip(name,age))\n      \n      # fromkeys(seq,value) 批量赋值\n      a = {}.fromkeys(('Ola','Kary','Mike'),23)\n      ```\n      \n      字典的常见用法\n      \n      ```python\n      dic.keys()           # 返回dic所有的键\n      dic.values()         # 返回dic所有的值\n      dic.items()          # 返回dic所有的元素（键值对）\n      dic.clear()          # 清空dic\n      del dic['xxx']       # 删除dic中的‘xxx’元素\n      dic.update(newdic)   # 添加新的字典键值对\n      ```\n      \n   * ##### 集合（set）\n   \n      集合是一个无序的容器，用{ }表示，且其中不包含重复的元素。\n      \n      ```python\n      a={\"Tom.py\", \"Mike.py\",\"Anne.py\",\"Denny.py\",\"Jack.py\",\"Fan.py\"}\n      b={\"Tom.py\", \"Lily.py\", \"Anne.py\", \"Richard.py\",\"Jack.py\"}\n      print('(2)',a&b)      # & 交集\n      print('(4)',a|b)      # | 并集\n      print('(1)',a-b)      # - 补集，a-b为差集\n      print('(3)',a^b)      # ^ 对称差集\n      \n      {'Tom.py', 'Anne.py', 'Jack.py'}\n      {'Tom.py', 'Denny.py', 'Lily.py', 'Fan.py', 'Anne.py', 'Mike.py', \n      'Richard.py', 'Jack.py'}\n      {'Fan.py', 'Denny.py', 'Mike.py'}\n      {'Denny.py', 'Mike.py', 'Lily.py', 'Fan.py', 'Richard.py'}\n      ```\n      \n      集合的常见方法\n      \n      ```python\n      set.add()             # 添加元素\n      set.remove()          # 删除元素\n      x in set              # 判断x是否在集合中\n      ```\n   \n3. #### 赋值\n\n   ```python\n   >>>a = 10            # 普通赋值\n   >>>a /= 5            # 增量赋值 a = a/5\n   >>>b = a = a + 1     # 链式赋值 a = a+1 , b = a\n   >>>a,b = 10,'orange' # 多重赋值 a = 10 , b = 'orange'\n   >>>a,b,c = [1,2,3]   # 解包\n   ```\n   \n   python中，数值、字符串、元祖等是**值类型**的对象，本身不可变；列表、字典是**引用类型**的对象，可变。\n   \n   ```python\n   # 值类型对象是不可变的，对值类型变量的修改是使变量指向了一个新的对象\n   >>>a = 10\n   >>>print id(a)\n   33521053L\n   \n   >>>a = 20\n   >>>print id(a)\n   27629312L\n   \n   # 引用类型对象是可变的，对引用类型的修改则是修改的对象本身\n   >>>l = [1,2,3]\n   >>>print id(l)\n   39774280L\n   \n   >>>l[0] = 0\n   >>>print id(l)\n   39774280L\n   ```\n\n---\n\n### Python 基本运算\n\n优先级：算术运算>位运算>关系运算>逻辑运算\n\n1. #### 算术运算\n\n   按优先级：\n\n   ** （乘方）\n\n   +,-（正负）\n\n   *（乘）, //（整除）, /（除）, %（取余） \n\n   +,-（加减）\n\n2. #### 位运算（二进制运算）\n\n   ```python\n   >>>~1           # 取反\n   -2\n   >>> 16 << 2     # 左移\n   64\n   >>> 16 >> 2     # 右移\n   4\n   >>> 64 & 15     # 与\n   0\n   >>> 64 | 15     # 或\n   79\n   >>> 64 ^ 14     # 异或\n   78\n   ```\n\n3. #### 关系运算\n\n   !=（不等于）\n\n4. #### 逻辑运算\n\n   优先级：not > and > or\n\n   \n\n---\n\n### Python 模块\n\n1. #### 单个模块\n\n   一个.py文件就是一个模块，用 import 函数导入模块，可以使用其中的函数、类等\n\n\t```python\n\t>>> import math          # 导入模块 math\n\t>>> math.pi              # 使用“模块.对象”格式调用模块中的对象\n\n\t3.141592653589793\n\t```\n\t\n\t还有其它导入模块的方式：\n\t\n\t```python\n\timport a as b            # 导入a，重命名为b\n\t\n\tfrom a import func1      # 从模块a中引入func1对象，调用时直接使用func1，不用再写a.function1\n\t\n\tfrom a import *          # 从模块a中引入所有对象，调用时直接使用对象，不用再写\ta.对象\n\t```\n\t\n2. #### 模块包\n   \n\t将功能相似的模块放在同一个**文件夹**，构成一个模块包\n\t\n\t```python\n\timport AFolder.module    # 导入AFolder文件夹中的模块\n\t```\n\n  \t该文件夹中必须包含一个\\_\\_init\\_\\_.py的文件，提醒Python，该文件夹为一个模块包。\\_\\_init\\_\\_.py可以是一个空文件。\n\n \n\n\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[Vamei 博客园-Python快速教程](https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html)*</small>\n\n<small>*[菜鸟教程 Python内置函数](https://www.runoob.com/python/python-built-in-functions.html)*</small>\n\n<small>*延伸*</small>\n\n<small>*[w3school python string methods](https://www.w3schools.com/python/python_ref_string.asp)*</small>\n\n<small>*[菜鸟教程 Python 直接赋值、浅拷贝和深度拷贝解析](https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html)*</small>\n\n","source":"_posts/Python 基础（一）.md","raw":"---\ntitle: Python 基础 (一)\ndate: 2020-02-12 00:39:00\ncategories:\n- 计算机科学\ntags: \n- python\nmathjax: true\n---\n\npython 解释风格、python 变量和数据类型、python 基本运算、python 模块<!--more-->\n\n\n\n---\n\n### Python 解释风格\n\n1. #### 注释\n\n   ```python\n   # this is annotation\n   ```\n\n2. #### 缩进\n\n   ```python\n   if i > 0:\n       x = 1 # 有4个空格的缩进\n   elif i == 0:\n       x = 2\n   else i < 0:\n       x = 3\n   ```\n\n3. #### 续行\n\n   * 续行符\\\n\n   ```python\n   if i > 0 and\\\n       j < 0:\n       x = 1\n   ```\n\n   * 无需续行符\n\n   括号内部、三引号内部'''无需续行符直接换行\n   \n   \n\n---\n\n### Python 变量和数据类型\n\n1. #### 动态的强类型语言\n\n   ```python\n   # python是动态强类型语言，变量无需声明，自动确定数据类型\n   >>>pi = 3.1415926\n   >>>PI = 'apple pie'\n   >>>print (type(pi))\n   >>>print (type(PI))\n   \n   <type 'float'>\n   <type 'str'>\n   ```\n\n2. #### 基本数据类型\n\n   * ##### 整型（integer）\n\n   * ##### 浮点型（float）\n\n   * ##### 布尔型（boolean）True，False\n\n   * ##### 复数型（complex）\n\n     ```python\n     2+3j #虚数部分必须加j\n     ```\n\n   * ##### 序列（sequence）\n\n     * 字符串（string）--不可变类型\n\n       ```python\n       >>>str = 'this is a string'\n       >>>str = \"this is also a string\"\n       >>>str = ''' this is a string, too'''\n       \n       # 当要输出含双引号的字符串时，可用单引号包裹，反之亦然\n       >>> astr = 'a \"blue\" apple'\n       >>> print(astr)\n       a \"blue\" apple\n       \n       >>> bstr = \"a 'black' orange\"\n       >>> print(bstr)\n       a 'black' orange\n       \n       # 三引号内可以实现方便的换行，不需要转义符\n       >>> cstr = '''Who is he?\n       ... James Bond'''\n       >>> print(cstr)\n       Who is he?\n       James Bond\n       ```\n   \n     * 列表（list）--可变类型\n   \n       ```python\n       >>>list = [123, 'sun', True]\n       >>>print (type(list))\n       \n       <type 'list'>\n       \n       >>>''.join(list)        # 列表转字符串\n       ```\n   \n     * 元组（tuple）--不可变类型\n   \n       ```python\n       >>>tuple = (213, 'sun', True)\n       >>>print (type(tuple))\n            \n       <type 'tuple'>\n       ```\n   \n     * 引用方式 [下限:上限:步长]\n   \n       ```python\n       # 列表下标从 0 开始\n       >>>print (list[0])\n       123\n       \n       # 若写明上限，则上限不包括在内\n       >>>s1[:5]     # 从开始到下标4 （下标5的元素不包括在内）\n       >>>s1[2:]     # 从下标2到最后\n       >>>s1[0:5:2]  # 从下标0到下标4 (5不包括在内)，每隔2取一个元素\n       >>>s1[::-1]   # 倒序 相当于s1[-1:-len(s1)-1:-1]（缺省默认填值）\n       \n       # 尾部元素引用\n       >>>s1[-1]     # 序列最后一个元素\n       >>>s1[-3]     # 序列倒数第三个元素\n       ```\n       \n     * 重复 sequence*copies\n   \n       ```python\n       >>>'apple'*3\n       'appleappleapple'\n       \n       >>>[0]*3\n       '[0,0,0]'\n       ```\n   \n     * 拼接 sequence+sequence\n   \n       ```python\n       >>>print('pine'+'apple')\n       'pineapple'\n       \n       >>>print([1,2,3]+[4,5,6])\n       [1, 2, 3, 4, 5, 6]\n       \n       >>>print((1,2,3)+(4,5,6))\n       (1, 2, 3, 4, 5, 6)\n       ```\n   \n     * 判断成员 in，not in\n   \n       ```python\n       >>>namelist = ['Jack','Russell','Mary']\n       >>>'Russell' in namelist\n       True\n       ```\n   \n     * 排序sorted，倒序reversed\n   \n       ```python\n       >>>a = [3,6,1,9]\n       >>>print(sorted(a))       \n       [1, 3, 6, 9]\n       >>>print(a)\n       [3, 6, 1, 9]                # sorted()函数不改变原对象\n       \n       >>>a.sort()                  \n       >>>print(a)\n       [1, 3, 6, 9]                # .sort()方法对原对象进行改动\n       \n       # reversed()和.reverse()同理\n       ```\n   \n   * #####  字典（dictionary）\n   \n      字典和序列一样，是可以储存多个元素的类（称为容器），不同的是字典包含映射关系：字典的元素有两部分——“键”和“值”；且字典没有顺序。\n   \n      ```python\n      dic = {'name':'orange', 'shape':'sphere', 'price':2.6}\n      print (dic['price'])         # 字典通过“键”引用\n      \n      dic['color'] = 'orange'      # 可在字典中加入新的元素\n      \n      for key in dic:              # 在循环中，也是通过“键”引用值\n          print (dic[key])\n      ```\n      \n      创建字典的方法\n      \n      ```python\n      # dict()函数\n      dict([('Ola',23),('Kary','24'),('Mike','22')])\n      dict([['Ola',23],['Kary','24'],['Mike','22']])\n      dict((('Ola',23),('Kary','24'),('Mike','22')))\n      dict(Ola = 23), Kary = 24, Mike = 22)\n      dict(zip(name,age))\n      \n      # fromkeys(seq,value) 批量赋值\n      a = {}.fromkeys(('Ola','Kary','Mike'),23)\n      ```\n      \n      字典的常见用法\n      \n      ```python\n      dic.keys()           # 返回dic所有的键\n      dic.values()         # 返回dic所有的值\n      dic.items()          # 返回dic所有的元素（键值对）\n      dic.clear()          # 清空dic\n      del dic['xxx']       # 删除dic中的‘xxx’元素\n      dic.update(newdic)   # 添加新的字典键值对\n      ```\n      \n   * ##### 集合（set）\n   \n      集合是一个无序的容器，用{ }表示，且其中不包含重复的元素。\n      \n      ```python\n      a={\"Tom.py\", \"Mike.py\",\"Anne.py\",\"Denny.py\",\"Jack.py\",\"Fan.py\"}\n      b={\"Tom.py\", \"Lily.py\", \"Anne.py\", \"Richard.py\",\"Jack.py\"}\n      print('(2)',a&b)      # & 交集\n      print('(4)',a|b)      # | 并集\n      print('(1)',a-b)      # - 补集，a-b为差集\n      print('(3)',a^b)      # ^ 对称差集\n      \n      {'Tom.py', 'Anne.py', 'Jack.py'}\n      {'Tom.py', 'Denny.py', 'Lily.py', 'Fan.py', 'Anne.py', 'Mike.py', \n      'Richard.py', 'Jack.py'}\n      {'Fan.py', 'Denny.py', 'Mike.py'}\n      {'Denny.py', 'Mike.py', 'Lily.py', 'Fan.py', 'Richard.py'}\n      ```\n      \n      集合的常见方法\n      \n      ```python\n      set.add()             # 添加元素\n      set.remove()          # 删除元素\n      x in set              # 判断x是否在集合中\n      ```\n   \n3. #### 赋值\n\n   ```python\n   >>>a = 10            # 普通赋值\n   >>>a /= 5            # 增量赋值 a = a/5\n   >>>b = a = a + 1     # 链式赋值 a = a+1 , b = a\n   >>>a,b = 10,'orange' # 多重赋值 a = 10 , b = 'orange'\n   >>>a,b,c = [1,2,3]   # 解包\n   ```\n   \n   python中，数值、字符串、元祖等是**值类型**的对象，本身不可变；列表、字典是**引用类型**的对象，可变。\n   \n   ```python\n   # 值类型对象是不可变的，对值类型变量的修改是使变量指向了一个新的对象\n   >>>a = 10\n   >>>print id(a)\n   33521053L\n   \n   >>>a = 20\n   >>>print id(a)\n   27629312L\n   \n   # 引用类型对象是可变的，对引用类型的修改则是修改的对象本身\n   >>>l = [1,2,3]\n   >>>print id(l)\n   39774280L\n   \n   >>>l[0] = 0\n   >>>print id(l)\n   39774280L\n   ```\n\n---\n\n### Python 基本运算\n\n优先级：算术运算>位运算>关系运算>逻辑运算\n\n1. #### 算术运算\n\n   按优先级：\n\n   ** （乘方）\n\n   +,-（正负）\n\n   *（乘）, //（整除）, /（除）, %（取余） \n\n   +,-（加减）\n\n2. #### 位运算（二进制运算）\n\n   ```python\n   >>>~1           # 取反\n   -2\n   >>> 16 << 2     # 左移\n   64\n   >>> 16 >> 2     # 右移\n   4\n   >>> 64 & 15     # 与\n   0\n   >>> 64 | 15     # 或\n   79\n   >>> 64 ^ 14     # 异或\n   78\n   ```\n\n3. #### 关系运算\n\n   !=（不等于）\n\n4. #### 逻辑运算\n\n   优先级：not > and > or\n\n   \n\n---\n\n### Python 模块\n\n1. #### 单个模块\n\n   一个.py文件就是一个模块，用 import 函数导入模块，可以使用其中的函数、类等\n\n\t```python\n\t>>> import math          # 导入模块 math\n\t>>> math.pi              # 使用“模块.对象”格式调用模块中的对象\n\n\t3.141592653589793\n\t```\n\t\n\t还有其它导入模块的方式：\n\t\n\t```python\n\timport a as b            # 导入a，重命名为b\n\t\n\tfrom a import func1      # 从模块a中引入func1对象，调用时直接使用func1，不用再写a.function1\n\t\n\tfrom a import *          # 从模块a中引入所有对象，调用时直接使用对象，不用再写\ta.对象\n\t```\n\t\n2. #### 模块包\n   \n\t将功能相似的模块放在同一个**文件夹**，构成一个模块包\n\t\n\t```python\n\timport AFolder.module    # 导入AFolder文件夹中的模块\n\t```\n\n  \t该文件夹中必须包含一个\\_\\_init\\_\\_.py的文件，提醒Python，该文件夹为一个模块包。\\_\\_init\\_\\_.py可以是一个空文件。\n\n \n\n\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[Vamei 博客园-Python快速教程](https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html)*</small>\n\n<small>*[菜鸟教程 Python内置函数](https://www.runoob.com/python/python-built-in-functions.html)*</small>\n\n<small>*延伸*</small>\n\n<small>*[w3school python string methods](https://www.w3schools.com/python/python_ref_string.asp)*</small>\n\n<small>*[菜鸟教程 Python 直接赋值、浅拷贝和深度拷贝解析](https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html)*</small>\n\n","slug":"Python 基础（一）","published":1,"updated":"2020-04-10T17:15:56.755Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5x000p2sfy4j4n543i","content":"<p>python 解释风格、python 变量和数据类型、python 基本运算、python 模块<a id=\"more\"></a></p>\n<hr>\n<h3 id=\"Python-解释风格\"><a href=\"#Python-解释风格\" class=\"headerlink\" title=\"Python 解释风格\"></a>Python 解释风格</h3><ol>\n<li><h4 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># this is annotation</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> i &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">1</span> <span class=\"comment\"># 有4个空格的缩进</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> i == <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> i &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">3</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"续行\"><a href=\"#续行\" class=\"headerlink\" title=\"续行\"></a>续行</h4><ul>\n<li>续行符\\</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> i &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span>\\</span><br><span class=\"line\">    j &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>无需续行符</li>\n</ul>\n<p>括号内部、三引号内部’’’无需续行符直接换行</p>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-变量和数据类型\"><a href=\"#Python-变量和数据类型\" class=\"headerlink\" title=\"Python 变量和数据类型\"></a>Python 变量和数据类型</h3><ol>\n<li><h4 id=\"动态的强类型语言\"><a href=\"#动态的强类型语言\" class=\"headerlink\" title=\"动态的强类型语言\"></a>动态的强类型语言</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># python是动态强类型语言，变量无需声明，自动确定数据类型</span></span><br><span class=\"line\">&gt;&gt;&gt;pi = <span class=\"number\">3.1415926</span></span><br><span class=\"line\">&gt;&gt;&gt;PI = <span class=\"string\">'apple pie'</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(pi))</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(PI))</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;type <span class=\"string\">'float'</span>&gt;</span><br><span class=\"line\">&lt;type <span class=\"string\">'str'</span>&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"基本数据类型\"><a href=\"#基本数据类型\" class=\"headerlink\" title=\"基本数据类型\"></a>基本数据类型</h4><ul>\n<li><h5 id=\"整型（integer）\"><a href=\"#整型（integer）\" class=\"headerlink\" title=\"整型（integer）\"></a>整型（integer）</h5></li>\n<li><h5 id=\"浮点型（float）\"><a href=\"#浮点型（float）\" class=\"headerlink\" title=\"浮点型（float）\"></a>浮点型（float）</h5></li>\n<li><h5 id=\"布尔型（boolean）True，False\"><a href=\"#布尔型（boolean）True，False\" class=\"headerlink\" title=\"布尔型（boolean）True，False\"></a>布尔型（boolean）True，False</h5></li>\n<li><h5 id=\"复数型（complex）\"><a href=\"#复数型（complex）\" class=\"headerlink\" title=\"复数型（complex）\"></a>复数型（complex）</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2</span>+<span class=\"number\">3j</span> <span class=\"comment\">#虚数部分必须加j</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"序列（sequence）\"><a href=\"#序列（sequence）\" class=\"headerlink\" title=\"序列（sequence）\"></a>序列（sequence）</h5><ul>\n<li><p>字符串（string）—不可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">'this is a string'</span></span><br><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">\"this is also a string\"</span></span><br><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">''' this is a string, too'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 当要输出含双引号的字符串时，可用单引号包裹，反之亦然</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>astr = <span class=\"string\">'a \"blue\" apple'</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(astr)</span><br><span class=\"line\">a <span class=\"string\">\"blue\"</span> apple</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bstr = <span class=\"string\">\"a 'black' orange\"</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(bstr)</span><br><span class=\"line\">a <span class=\"string\">'black'</span> orange</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 三引号内可以实现方便的换行，不需要转义符</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>cstr = <span class=\"string\">'''Who is he?</span></span><br><span class=\"line\"><span class=\"string\"><span class=\"meta\">... </span>James Bond'''</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(cstr)</span><br><span class=\"line\">Who is he?</span><br><span class=\"line\">James Bond</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>列表（list）—可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;list = [<span class=\"number\">123</span>, <span class=\"string\">'sun'</span>, <span class=\"literal\">True</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(list))</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;type <span class=\"string\">'list'</span>&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">''</span>.join(list)        <span class=\"comment\"># 列表转字符串</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>元组（tuple）—不可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;tuple = (<span class=\"number\">213</span>, <span class=\"string\">'sun'</span>, <span class=\"literal\">True</span>)</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(tuple))</span><br><span class=\"line\">     </span><br><span class=\"line\">&lt;type <span class=\"string\">'tuple'</span>&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>引用方式 [下限:上限:步长]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 列表下标从 0 开始</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (list[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"number\">123</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 若写明上限，则上限不包括在内</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[:<span class=\"number\">5</span>]     <span class=\"comment\"># 从开始到下标4 （下标5的元素不包括在内）</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">2</span>:]     <span class=\"comment\"># 从下标2到最后</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">0</span>:<span class=\"number\">5</span>:<span class=\"number\">2</span>]  <span class=\"comment\"># 从下标0到下标4 (5不包括在内)，每隔2取一个元素</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[::<span class=\"number\">-1</span>]   <span class=\"comment\"># 倒序 相当于s1[-1:-len(s1)-1:-1]（缺省默认填值）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 尾部元素引用</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">-1</span>]     <span class=\"comment\"># 序列最后一个元素</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">-3</span>]     <span class=\"comment\"># 序列倒数第三个元素</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重复 sequence*copies</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">'apple'</span>*<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"string\">'appleappleapple'</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;[<span class=\"number\">0</span>]*<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"string\">'[0,0,0]'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>拼接 sequence+sequence</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;print(<span class=\"string\">'pine'</span>+<span class=\"string\">'apple'</span>)</span><br><span class=\"line\"><span class=\"string\">'pineapple'</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]+[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print((<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)+(<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\">(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>判断成员 in，not in</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;namelist = [<span class=\"string\">'Jack'</span>,<span class=\"string\">'Russell'</span>,<span class=\"string\">'Mary'</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">'Russell'</span> <span class=\"keyword\">in</span> namelist</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>排序sorted，倒序reversed</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = [<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\">&gt;&gt;&gt;print(sorted(a))       </span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>]</span><br><span class=\"line\">&gt;&gt;&gt;print(a)</span><br><span class=\"line\">[<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">9</span>]                <span class=\"comment\"># sorted()函数不改变原对象</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;a.sort()                  </span><br><span class=\"line\">&gt;&gt;&gt;print(a)</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>]                <span class=\"comment\"># .sort()方法对原对象进行改动</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># reversed()和.reverse()同理</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h5 id=\"字典（dictionary）\"><a href=\"#字典（dictionary）\" class=\"headerlink\" title=\"字典（dictionary）\"></a>字典（dictionary）</h5><p> 字典和序列一样，是可以储存多个元素的类（称为容器），不同的是字典包含映射关系：字典的元素有两部分——“键”和“值”；且字典没有顺序。</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dic = &#123;<span class=\"string\">'name'</span>:<span class=\"string\">'orange'</span>, <span class=\"string\">'shape'</span>:<span class=\"string\">'sphere'</span>, <span class=\"string\">'price'</span>:<span class=\"number\">2.6</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">print</span> (dic[<span class=\"string\">'price'</span>])         <span class=\"comment\"># 字典通过“键”引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">dic[<span class=\"string\">'color'</span>] = <span class=\"string\">'orange'</span>      <span class=\"comment\"># 可在字典中加入新的元素</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> dic:              <span class=\"comment\"># 在循环中，也是通过“键”引用值</span></span><br><span class=\"line\">    <span class=\"keyword\">print</span> (dic[key])</span><br></pre></td></tr></table></figure>\n<p> 创建字典的方法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dict()函数</span></span><br><span class=\"line\">dict([(<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>),(<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>),(<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>)])</span><br><span class=\"line\">dict([[<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>],[<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>],[<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>]])</span><br><span class=\"line\">dict(((<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>),(<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>),(<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>)))</span><br><span class=\"line\">dict(Ola = <span class=\"number\">23</span>), Kary = <span class=\"number\">24</span>, Mike = <span class=\"number\">22</span>)</span><br><span class=\"line\">dict(zip(name,age))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># fromkeys(seq,value) 批量赋值</span></span><br><span class=\"line\">a = &#123;&#125;.fromkeys((<span class=\"string\">'Ola'</span>,<span class=\"string\">'Kary'</span>,<span class=\"string\">'Mike'</span>),<span class=\"number\">23</span>)</span><br></pre></td></tr></table></figure>\n<p> 字典的常见用法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dic.keys()           <span class=\"comment\"># 返回dic所有的键</span></span><br><span class=\"line\">dic.values()         <span class=\"comment\"># 返回dic所有的值</span></span><br><span class=\"line\">dic.items()          <span class=\"comment\"># 返回dic所有的元素（键值对）</span></span><br><span class=\"line\">dic.clear()          <span class=\"comment\"># 清空dic</span></span><br><span class=\"line\"><span class=\"keyword\">del</span> dic[<span class=\"string\">'xxx'</span>]       <span class=\"comment\"># 删除dic中的‘xxx’元素</span></span><br><span class=\"line\">dic.update(newdic)   <span class=\"comment\"># 添加新的字典键值对</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"集合（set）\"><a href=\"#集合（set）\" class=\"headerlink\" title=\"集合（set）\"></a>集合（set）</h5><p> 集合是一个无序的容器，用{ }表示，且其中不包含重复的元素。</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=&#123;<span class=\"string\">\"Tom.py\"</span>, <span class=\"string\">\"Mike.py\"</span>,<span class=\"string\">\"Anne.py\"</span>,<span class=\"string\">\"Denny.py\"</span>,<span class=\"string\">\"Jack.py\"</span>,<span class=\"string\">\"Fan.py\"</span>&#125;</span><br><span class=\"line\">b=&#123;<span class=\"string\">\"Tom.py\"</span>, <span class=\"string\">\"Lily.py\"</span>, <span class=\"string\">\"Anne.py\"</span>, <span class=\"string\">\"Richard.py\"</span>,<span class=\"string\">\"Jack.py\"</span>&#125;</span><br><span class=\"line\">print(<span class=\"string\">'(2)'</span>,a&amp;b)      <span class=\"comment\"># &amp; 交集</span></span><br><span class=\"line\">print(<span class=\"string\">'(4)'</span>,a|b)      <span class=\"comment\"># | 并集</span></span><br><span class=\"line\">print(<span class=\"string\">'(1)'</span>,a-b)      <span class=\"comment\"># - 补集，a-b为差集</span></span><br><span class=\"line\">print(<span class=\"string\">'(3)'</span>,a^b)      <span class=\"comment\"># ^ 对称差集</span></span><br><span class=\"line\"></span><br><span class=\"line\">&#123;<span class=\"string\">'Tom.py'</span>, <span class=\"string\">'Anne.py'</span>, <span class=\"string\">'Jack.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Tom.py'</span>, <span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Lily.py'</span>, <span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Anne.py'</span>, <span class=\"string\">'Mike.py'</span>, </span><br><span class=\"line\"><span class=\"string\">'Richard.py'</span>, <span class=\"string\">'Jack.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Mike.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Mike.py'</span>, <span class=\"string\">'Lily.py'</span>, <span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Richard.py'</span>&#125;</span><br></pre></td></tr></table></figure>\n<p> 集合的常见方法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set.add()             <span class=\"comment\"># 添加元素</span></span><br><span class=\"line\">set.remove()          <span class=\"comment\"># 删除元素</span></span><br><span class=\"line\">x <span class=\"keyword\">in</span> set              <span class=\"comment\"># 判断x是否在集合中</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"赋值\"><a href=\"#赋值\" class=\"headerlink\" title=\"赋值\"></a>赋值</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">10</span>            <span class=\"comment\"># 普通赋值</span></span><br><span class=\"line\">&gt;&gt;&gt;a /= <span class=\"number\">5</span>            <span class=\"comment\"># 增量赋值 a = a/5</span></span><br><span class=\"line\">&gt;&gt;&gt;b = a = a + <span class=\"number\">1</span>     <span class=\"comment\"># 链式赋值 a = a+1 , b = a</span></span><br><span class=\"line\">&gt;&gt;&gt;a,b = <span class=\"number\">10</span>,<span class=\"string\">'orange'</span> <span class=\"comment\"># 多重赋值 a = 10 , b = 'orange'</span></span><br><span class=\"line\">&gt;&gt;&gt;a,b,c = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]   <span class=\"comment\"># 解包</span></span><br></pre></td></tr></table></figure>\n<p>python中，数值、字符串、元祖等是<strong>值类型</strong>的对象，本身不可变；列表、字典是<strong>引用类型</strong>的对象，可变。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 值类型对象是不可变的，对值类型变量的修改是使变量指向了一个新的对象</span></span><br><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">10</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(a)</span><br><span class=\"line\"><span class=\"number\">33521053L</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">20</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(a)</span><br><span class=\"line\"><span class=\"number\">27629312L</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 引用类型对象是可变的，对引用类型的修改则是修改的对象本身</span></span><br><span class=\"line\">&gt;&gt;&gt;l = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(l)</span><br><span class=\"line\"><span class=\"number\">39774280L</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;l[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(l)</span><br><span class=\"line\"><span class=\"number\">39774280L</span></span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-基本运算\"><a href=\"#Python-基本运算\" class=\"headerlink\" title=\"Python 基本运算\"></a>Python 基本运算</h3><p>优先级：算术运算&gt;位运算&gt;关系运算&gt;逻辑运算</p>\n<ol>\n<li><h4 id=\"算术运算\"><a href=\"#算术运算\" class=\"headerlink\" title=\"算术运算\"></a>算术运算</h4><p>按优先级：</p>\n<p>** （乘方）</p>\n<p>+,-（正负）</p>\n<p>*（乘）, //（整除）, /（除）, %（取余） </p>\n<p>+,-（加减）</p>\n</li>\n<li><h4 id=\"位运算（二进制运算）\"><a href=\"#位运算（二进制运算）\" class=\"headerlink\" title=\"位运算（二进制运算）\"></a>位运算（二进制运算）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;~<span class=\"number\">1</span>           <span class=\"comment\"># 取反</span></span><br><span class=\"line\"><span class=\"number\">-2</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">16</span> &lt;&lt; <span class=\"number\">2</span>     <span class=\"comment\"># 左移</span></span><br><span class=\"line\"><span class=\"number\">64</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">16</span> &gt;&gt; <span class=\"number\">2</span>     <span class=\"comment\"># 右移</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> &amp; <span class=\"number\">15</span>     <span class=\"comment\"># 与</span></span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> | <span class=\"number\">15</span>     <span class=\"comment\"># 或</span></span><br><span class=\"line\"><span class=\"number\">79</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> ^ <span class=\"number\">14</span>     <span class=\"comment\"># 异或</span></span><br><span class=\"line\"><span class=\"number\">78</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"关系运算\"><a href=\"#关系运算\" class=\"headerlink\" title=\"关系运算\"></a>关系运算</h4><p>!=（不等于）</p>\n</li>\n<li><h4 id=\"逻辑运算\"><a href=\"#逻辑运算\" class=\"headerlink\" title=\"逻辑运算\"></a>逻辑运算</h4><p>优先级：not &gt; and &gt; or</p>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-模块\"><a href=\"#Python-模块\" class=\"headerlink\" title=\"Python 模块\"></a>Python 模块</h3><ol>\n<li><h4 id=\"单个模块\"><a href=\"#单个模块\" class=\"headerlink\" title=\"单个模块\"></a>单个模块</h4><p>一个.py文件就是一个模块，用 import 函数导入模块，可以使用其中的函数、类等</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> math          <span class=\"comment\"># 导入模块 math</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>math.pi              <span class=\"comment\"># 使用“模块.对象”格式调用模块中的对象</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">3.141592653589793</span></span><br></pre></td></tr></table></figure>\n<p> 还有其它导入模块的方式：</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> a <span class=\"keyword\">as</span> b            <span class=\"comment\"># 导入a，重命名为b</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> a <span class=\"keyword\">import</span> func1      <span class=\"comment\"># 从模块a中引入func1对象，调用时直接使用func1，不用再写a.function1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> a <span class=\"keyword\">import</span> *          <span class=\"comment\"># 从模块a中引入所有对象，调用时直接使用对象，不用再写\ta.对象</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"模块包\"><a href=\"#模块包\" class=\"headerlink\" title=\"模块包\"></a>模块包</h4><p> 将功能相似的模块放在同一个<strong>文件夹</strong>，构成一个模块包</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> AFolder.module    <span class=\"comment\"># 导入AFolder文件夹中的模块</span></span><br></pre></td></tr></table></figure>\n<p>   该文件夹中必须包含一个__init__.py的文件，提醒Python，该文件夹为一个模块包。__init__.py可以是一个空文件。</p>\n</li>\n</ol>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python快速教程</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/python/python-built-in-functions.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程 Python内置函数</a></em></small></p>\n<p><small><em>延伸</em></small></p>\n<p><small><em><a href=\"https://www.w3schools.com/python/python_ref_string.asp\" target=\"_blank\" rel=\"noopener\">w3school python string methods</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程 Python 直接赋值、浅拷贝和深度拷贝解析</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>python 解释风格、python 变量和数据类型、python 基本运算、python 模块","more":"</p>\n<hr>\n<h3 id=\"Python-解释风格\"><a href=\"#Python-解释风格\" class=\"headerlink\" title=\"Python 解释风格\"></a>Python 解释风格</h3><ol>\n<li><h4 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># this is annotation</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> i &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">1</span> <span class=\"comment\"># 有4个空格的缩进</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> i == <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> i &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">3</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"续行\"><a href=\"#续行\" class=\"headerlink\" title=\"续行\"></a>续行</h4><ul>\n<li>续行符\\</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> i &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span>\\</span><br><span class=\"line\">    j &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    x = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>无需续行符</li>\n</ul>\n<p>括号内部、三引号内部’’’无需续行符直接换行</p>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-变量和数据类型\"><a href=\"#Python-变量和数据类型\" class=\"headerlink\" title=\"Python 变量和数据类型\"></a>Python 变量和数据类型</h3><ol>\n<li><h4 id=\"动态的强类型语言\"><a href=\"#动态的强类型语言\" class=\"headerlink\" title=\"动态的强类型语言\"></a>动态的强类型语言</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># python是动态强类型语言，变量无需声明，自动确定数据类型</span></span><br><span class=\"line\">&gt;&gt;&gt;pi = <span class=\"number\">3.1415926</span></span><br><span class=\"line\">&gt;&gt;&gt;PI = <span class=\"string\">'apple pie'</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(pi))</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(PI))</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;type <span class=\"string\">'float'</span>&gt;</span><br><span class=\"line\">&lt;type <span class=\"string\">'str'</span>&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"基本数据类型\"><a href=\"#基本数据类型\" class=\"headerlink\" title=\"基本数据类型\"></a>基本数据类型</h4><ul>\n<li><h5 id=\"整型（integer）\"><a href=\"#整型（integer）\" class=\"headerlink\" title=\"整型（integer）\"></a>整型（integer）</h5></li>\n<li><h5 id=\"浮点型（float）\"><a href=\"#浮点型（float）\" class=\"headerlink\" title=\"浮点型（float）\"></a>浮点型（float）</h5></li>\n<li><h5 id=\"布尔型（boolean）True，False\"><a href=\"#布尔型（boolean）True，False\" class=\"headerlink\" title=\"布尔型（boolean）True，False\"></a>布尔型（boolean）True，False</h5></li>\n<li><h5 id=\"复数型（complex）\"><a href=\"#复数型（complex）\" class=\"headerlink\" title=\"复数型（complex）\"></a>复数型（complex）</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2</span>+<span class=\"number\">3j</span> <span class=\"comment\">#虚数部分必须加j</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"序列（sequence）\"><a href=\"#序列（sequence）\" class=\"headerlink\" title=\"序列（sequence）\"></a>序列（sequence）</h5><ul>\n<li><p>字符串（string）—不可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">'this is a string'</span></span><br><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">\"this is also a string\"</span></span><br><span class=\"line\">&gt;&gt;&gt;str = <span class=\"string\">''' this is a string, too'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 当要输出含双引号的字符串时，可用单引号包裹，反之亦然</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>astr = <span class=\"string\">'a \"blue\" apple'</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(astr)</span><br><span class=\"line\">a <span class=\"string\">\"blue\"</span> apple</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bstr = <span class=\"string\">\"a 'black' orange\"</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(bstr)</span><br><span class=\"line\">a <span class=\"string\">'black'</span> orange</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 三引号内可以实现方便的换行，不需要转义符</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>cstr = <span class=\"string\">'''Who is he?</span></span><br><span class=\"line\"><span class=\"string\"><span class=\"meta\">... </span>James Bond'''</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(cstr)</span><br><span class=\"line\">Who is he?</span><br><span class=\"line\">James Bond</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>列表（list）—可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;list = [<span class=\"number\">123</span>, <span class=\"string\">'sun'</span>, <span class=\"literal\">True</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(list))</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;type <span class=\"string\">'list'</span>&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">''</span>.join(list)        <span class=\"comment\"># 列表转字符串</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>元组（tuple）—不可变类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;tuple = (<span class=\"number\">213</span>, <span class=\"string\">'sun'</span>, <span class=\"literal\">True</span>)</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (type(tuple))</span><br><span class=\"line\">     </span><br><span class=\"line\">&lt;type <span class=\"string\">'tuple'</span>&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>引用方式 [下限:上限:步长]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 列表下标从 0 开始</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> (list[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"number\">123</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 若写明上限，则上限不包括在内</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[:<span class=\"number\">5</span>]     <span class=\"comment\"># 从开始到下标4 （下标5的元素不包括在内）</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">2</span>:]     <span class=\"comment\"># 从下标2到最后</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">0</span>:<span class=\"number\">5</span>:<span class=\"number\">2</span>]  <span class=\"comment\"># 从下标0到下标4 (5不包括在内)，每隔2取一个元素</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[::<span class=\"number\">-1</span>]   <span class=\"comment\"># 倒序 相当于s1[-1:-len(s1)-1:-1]（缺省默认填值）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 尾部元素引用</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">-1</span>]     <span class=\"comment\"># 序列最后一个元素</span></span><br><span class=\"line\">&gt;&gt;&gt;s1[<span class=\"number\">-3</span>]     <span class=\"comment\"># 序列倒数第三个元素</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重复 sequence*copies</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">'apple'</span>*<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"string\">'appleappleapple'</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;[<span class=\"number\">0</span>]*<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"string\">'[0,0,0]'</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>拼接 sequence+sequence</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;print(<span class=\"string\">'pine'</span>+<span class=\"string\">'apple'</span>)</span><br><span class=\"line\"><span class=\"string\">'pineapple'</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]+[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;print((<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)+(<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\">(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>判断成员 in，not in</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;namelist = [<span class=\"string\">'Jack'</span>,<span class=\"string\">'Russell'</span>,<span class=\"string\">'Mary'</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">'Russell'</span> <span class=\"keyword\">in</span> namelist</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>排序sorted，倒序reversed</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = [<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\">&gt;&gt;&gt;print(sorted(a))       </span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>]</span><br><span class=\"line\">&gt;&gt;&gt;print(a)</span><br><span class=\"line\">[<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">9</span>]                <span class=\"comment\"># sorted()函数不改变原对象</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;a.sort()                  </span><br><span class=\"line\">&gt;&gt;&gt;print(a)</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>]                <span class=\"comment\"># .sort()方法对原对象进行改动</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># reversed()和.reverse()同理</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h5 id=\"字典（dictionary）\"><a href=\"#字典（dictionary）\" class=\"headerlink\" title=\"字典（dictionary）\"></a>字典（dictionary）</h5><p> 字典和序列一样，是可以储存多个元素的类（称为容器），不同的是字典包含映射关系：字典的元素有两部分——“键”和“值”；且字典没有顺序。</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dic = &#123;<span class=\"string\">'name'</span>:<span class=\"string\">'orange'</span>, <span class=\"string\">'shape'</span>:<span class=\"string\">'sphere'</span>, <span class=\"string\">'price'</span>:<span class=\"number\">2.6</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">print</span> (dic[<span class=\"string\">'price'</span>])         <span class=\"comment\"># 字典通过“键”引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">dic[<span class=\"string\">'color'</span>] = <span class=\"string\">'orange'</span>      <span class=\"comment\"># 可在字典中加入新的元素</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> dic:              <span class=\"comment\"># 在循环中，也是通过“键”引用值</span></span><br><span class=\"line\">    <span class=\"keyword\">print</span> (dic[key])</span><br></pre></td></tr></table></figure>\n<p> 创建字典的方法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dict()函数</span></span><br><span class=\"line\">dict([(<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>),(<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>),(<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>)])</span><br><span class=\"line\">dict([[<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>],[<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>],[<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>]])</span><br><span class=\"line\">dict(((<span class=\"string\">'Ola'</span>,<span class=\"number\">23</span>),(<span class=\"string\">'Kary'</span>,<span class=\"string\">'24'</span>),(<span class=\"string\">'Mike'</span>,<span class=\"string\">'22'</span>)))</span><br><span class=\"line\">dict(Ola = <span class=\"number\">23</span>), Kary = <span class=\"number\">24</span>, Mike = <span class=\"number\">22</span>)</span><br><span class=\"line\">dict(zip(name,age))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># fromkeys(seq,value) 批量赋值</span></span><br><span class=\"line\">a = &#123;&#125;.fromkeys((<span class=\"string\">'Ola'</span>,<span class=\"string\">'Kary'</span>,<span class=\"string\">'Mike'</span>),<span class=\"number\">23</span>)</span><br></pre></td></tr></table></figure>\n<p> 字典的常见用法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dic.keys()           <span class=\"comment\"># 返回dic所有的键</span></span><br><span class=\"line\">dic.values()         <span class=\"comment\"># 返回dic所有的值</span></span><br><span class=\"line\">dic.items()          <span class=\"comment\"># 返回dic所有的元素（键值对）</span></span><br><span class=\"line\">dic.clear()          <span class=\"comment\"># 清空dic</span></span><br><span class=\"line\"><span class=\"keyword\">del</span> dic[<span class=\"string\">'xxx'</span>]       <span class=\"comment\"># 删除dic中的‘xxx’元素</span></span><br><span class=\"line\">dic.update(newdic)   <span class=\"comment\"># 添加新的字典键值对</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h5 id=\"集合（set）\"><a href=\"#集合（set）\" class=\"headerlink\" title=\"集合（set）\"></a>集合（set）</h5><p> 集合是一个无序的容器，用{ }表示，且其中不包含重复的元素。</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=&#123;<span class=\"string\">\"Tom.py\"</span>, <span class=\"string\">\"Mike.py\"</span>,<span class=\"string\">\"Anne.py\"</span>,<span class=\"string\">\"Denny.py\"</span>,<span class=\"string\">\"Jack.py\"</span>,<span class=\"string\">\"Fan.py\"</span>&#125;</span><br><span class=\"line\">b=&#123;<span class=\"string\">\"Tom.py\"</span>, <span class=\"string\">\"Lily.py\"</span>, <span class=\"string\">\"Anne.py\"</span>, <span class=\"string\">\"Richard.py\"</span>,<span class=\"string\">\"Jack.py\"</span>&#125;</span><br><span class=\"line\">print(<span class=\"string\">'(2)'</span>,a&amp;b)      <span class=\"comment\"># &amp; 交集</span></span><br><span class=\"line\">print(<span class=\"string\">'(4)'</span>,a|b)      <span class=\"comment\"># | 并集</span></span><br><span class=\"line\">print(<span class=\"string\">'(1)'</span>,a-b)      <span class=\"comment\"># - 补集，a-b为差集</span></span><br><span class=\"line\">print(<span class=\"string\">'(3)'</span>,a^b)      <span class=\"comment\"># ^ 对称差集</span></span><br><span class=\"line\"></span><br><span class=\"line\">&#123;<span class=\"string\">'Tom.py'</span>, <span class=\"string\">'Anne.py'</span>, <span class=\"string\">'Jack.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Tom.py'</span>, <span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Lily.py'</span>, <span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Anne.py'</span>, <span class=\"string\">'Mike.py'</span>, </span><br><span class=\"line\"><span class=\"string\">'Richard.py'</span>, <span class=\"string\">'Jack.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Mike.py'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">'Denny.py'</span>, <span class=\"string\">'Mike.py'</span>, <span class=\"string\">'Lily.py'</span>, <span class=\"string\">'Fan.py'</span>, <span class=\"string\">'Richard.py'</span>&#125;</span><br></pre></td></tr></table></figure>\n<p> 集合的常见方法</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set.add()             <span class=\"comment\"># 添加元素</span></span><br><span class=\"line\">set.remove()          <span class=\"comment\"># 删除元素</span></span><br><span class=\"line\">x <span class=\"keyword\">in</span> set              <span class=\"comment\"># 判断x是否在集合中</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><h4 id=\"赋值\"><a href=\"#赋值\" class=\"headerlink\" title=\"赋值\"></a>赋值</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">10</span>            <span class=\"comment\"># 普通赋值</span></span><br><span class=\"line\">&gt;&gt;&gt;a /= <span class=\"number\">5</span>            <span class=\"comment\"># 增量赋值 a = a/5</span></span><br><span class=\"line\">&gt;&gt;&gt;b = a = a + <span class=\"number\">1</span>     <span class=\"comment\"># 链式赋值 a = a+1 , b = a</span></span><br><span class=\"line\">&gt;&gt;&gt;a,b = <span class=\"number\">10</span>,<span class=\"string\">'orange'</span> <span class=\"comment\"># 多重赋值 a = 10 , b = 'orange'</span></span><br><span class=\"line\">&gt;&gt;&gt;a,b,c = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]   <span class=\"comment\"># 解包</span></span><br></pre></td></tr></table></figure>\n<p>python中，数值、字符串、元祖等是<strong>值类型</strong>的对象，本身不可变；列表、字典是<strong>引用类型</strong>的对象，可变。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 值类型对象是不可变的，对值类型变量的修改是使变量指向了一个新的对象</span></span><br><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">10</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(a)</span><br><span class=\"line\"><span class=\"number\">33521053L</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;a = <span class=\"number\">20</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(a)</span><br><span class=\"line\"><span class=\"number\">27629312L</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 引用类型对象是可变的，对引用类型的修改则是修改的对象本身</span></span><br><span class=\"line\">&gt;&gt;&gt;l = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(l)</span><br><span class=\"line\"><span class=\"number\">39774280L</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;l[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">print</span> id(l)</span><br><span class=\"line\"><span class=\"number\">39774280L</span></span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-基本运算\"><a href=\"#Python-基本运算\" class=\"headerlink\" title=\"Python 基本运算\"></a>Python 基本运算</h3><p>优先级：算术运算&gt;位运算&gt;关系运算&gt;逻辑运算</p>\n<ol>\n<li><h4 id=\"算术运算\"><a href=\"#算术运算\" class=\"headerlink\" title=\"算术运算\"></a>算术运算</h4><p>按优先级：</p>\n<p>** （乘方）</p>\n<p>+,-（正负）</p>\n<p>*（乘）, //（整除）, /（除）, %（取余） </p>\n<p>+,-（加减）</p>\n</li>\n<li><h4 id=\"位运算（二进制运算）\"><a href=\"#位运算（二进制运算）\" class=\"headerlink\" title=\"位运算（二进制运算）\"></a>位运算（二进制运算）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;~<span class=\"number\">1</span>           <span class=\"comment\"># 取反</span></span><br><span class=\"line\"><span class=\"number\">-2</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">16</span> &lt;&lt; <span class=\"number\">2</span>     <span class=\"comment\"># 左移</span></span><br><span class=\"line\"><span class=\"number\">64</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">16</span> &gt;&gt; <span class=\"number\">2</span>     <span class=\"comment\"># 右移</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> &amp; <span class=\"number\">15</span>     <span class=\"comment\"># 与</span></span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> | <span class=\"number\">15</span>     <span class=\"comment\"># 或</span></span><br><span class=\"line\"><span class=\"number\">79</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">64</span> ^ <span class=\"number\">14</span>     <span class=\"comment\"># 异或</span></span><br><span class=\"line\"><span class=\"number\">78</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"关系运算\"><a href=\"#关系运算\" class=\"headerlink\" title=\"关系运算\"></a>关系运算</h4><p>!=（不等于）</p>\n</li>\n<li><h4 id=\"逻辑运算\"><a href=\"#逻辑运算\" class=\"headerlink\" title=\"逻辑运算\"></a>逻辑运算</h4><p>优先级：not &gt; and &gt; or</p>\n</li>\n</ol>\n<hr>\n<h3 id=\"Python-模块\"><a href=\"#Python-模块\" class=\"headerlink\" title=\"Python 模块\"></a>Python 模块</h3><ol>\n<li><h4 id=\"单个模块\"><a href=\"#单个模块\" class=\"headerlink\" title=\"单个模块\"></a>单个模块</h4><p>一个.py文件就是一个模块，用 import 函数导入模块，可以使用其中的函数、类等</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> math          <span class=\"comment\"># 导入模块 math</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>math.pi              <span class=\"comment\"># 使用“模块.对象”格式调用模块中的对象</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">3.141592653589793</span></span><br></pre></td></tr></table></figure>\n<p> 还有其它导入模块的方式：</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> a <span class=\"keyword\">as</span> b            <span class=\"comment\"># 导入a，重命名为b</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> a <span class=\"keyword\">import</span> func1      <span class=\"comment\"># 从模块a中引入func1对象，调用时直接使用func1，不用再写a.function1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> a <span class=\"keyword\">import</span> *          <span class=\"comment\"># 从模块a中引入所有对象，调用时直接使用对象，不用再写\ta.对象</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"模块包\"><a href=\"#模块包\" class=\"headerlink\" title=\"模块包\"></a>模块包</h4><p> 将功能相似的模块放在同一个<strong>文件夹</strong>，构成一个模块包</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> AFolder.module    <span class=\"comment\"># 导入AFolder文件夹中的模块</span></span><br></pre></td></tr></table></figure>\n<p>   该文件夹中必须包含一个__init__.py的文件，提醒Python，该文件夹为一个模块包。__init__.py可以是一个空文件。</p>\n</li>\n</ol>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html\" target=\"_blank\" rel=\"noopener\">Vamei 博客园-Python快速教程</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/python/python-built-in-functions.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程 Python内置函数</a></em></small></p>\n<p><small><em>延伸</em></small></p>\n<p><small><em><a href=\"https://www.w3schools.com/python/python_ref_string.asp\" target=\"_blank\" rel=\"noopener\">w3school python string methods</a></em></small></p>\n<p><small><em><a href=\"https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程 Python 直接赋值、浅拷贝和深度拷贝解析</a></em></small></p>"},{"title":"从社会学经验视角对杜尚《泉》的考察及反思","date":"2019-06-10T16:02:00.000Z","_content":"\n1917年4月，纽约独立艺术家协会计划在中央大厦举办一场艺术展览。美籍法裔艺术家马赛尔·杜尚把一个署有“R.Mutt”名字的陶瓷小便池提交给协会作为参展作品，命名为《泉》。尽管协会理事会拒绝将其展出，但这件充满挑衅性的作品引起了广泛的讨论，日后成为当代艺术的出发点。<!--more-->\n\n“*文化资本*”是布迪厄在艺术消费习性的社会学研究中确立的概念。他借鉴韦伯对社会分层的分析，将其应用到审美价值领域，认为社会结构的不平等不仅存在于经济资本，而且在文化领域还存在一种“文化资本”的不平等。布迪厄认为，拥有较多文化资本的人，对于复杂、抽象、“难懂”的艺术品的美学品质有更强的辨识欣赏技能，因此造成了艺术趣味的分层和区隔。而文化资本又很大程度依赖于经济资本：相比于工人阶级，一个在富裕的中产阶级家庭成长的孩子有更多的机会接触到高雅艺术（对欣赏技能有较高要求），这种机会体现在家庭环境的熏陶、教育背景、额外的金钱以及时间等方面。考察杜尚的家庭背景，不难发现他成长于一个典型的中产阶级家庭。杜尚的父亲是一名公证人，收入可观，受人尊敬，杜尚和他的家人住在一栋乡间别墅里。杜尚15岁开始画画，中学毕业后进入朱莉亚艺术学院学习，他的两个哥哥和一个妹妹也都学习艺术。在1917年杜尚递交《泉》给独立艺术家协会之前，他已经凭借《下楼梯的裸女》二号在美国取得知名度，移民美国后，他很快便与当地的艺术家及艺术赞助人们熟识。这表明杜尚在早年得以积累文化资本，并在创作《泉》时已经具有被当时的艺术家群体认可的文化资本，考虑到文化资本造成的趣味区隔是社会分层的重要依据，这一点对于极具反叛精神的《泉》得以被艺术界接纳至关重要。谈及“*艺术界*”，下面就要涉及到分析哲学的艺术体制理论。\n\n艺术体制理论认为，艺术和非艺术的区别是不能用肉眼直接区分的，仅通过物理属性无法识别艺术品，它们的识别仅仅取决于某种授予它们身份的社会体制。这种体制就是所谓的“*艺术界*”。用该理论代表学者迪基的话说：“在分类意义上，一件艺术品是：（1）一件人工制品；（2）代表某种社会体制的一些人，授予它作为鉴赏候选者的身份。” 透过这一视角，让我们关注当时是哪些力量推动了《泉》被纳入艺术范畴。在杜尚的小便池被拒之门外一个月后，达达主义艺术杂志《盲人》刊登了美国艺术家贝翠丝·伍德和沃尔特·康拉德·阿伦斯伯格共同撰写的题为《理察·马特一案》的文章。这篇文章为《泉》的艺术合法性地位做出辩护，后来的事实证明，它对当代艺术的发展产生了深远影响。与文章一起收录的，还有被称为“现代摄影之父“的阿尔弗雷德·斯蒂格利茨为《泉》拍摄的展览照片。另一位美国作家露易丝·瓦雷兹的文章《厕所神像》，指出《泉》除了戏谑外还有严肃的面向。这些艺术体制内具有话语权的权威们做出声明，称《泉》有资格作为一件艺术品参与展出，由此引发了界内对于艺术的反思和大讨论。其结果是：2004年，《泉》被500位英国艺术界的艺术家、艺术史学者票选为20世纪最具影响力的艺术作品，获得评审团64%的票数，比西班牙画家毕加索的著名画作《[亚维农的少女](https://zh.wikipedia.org/wiki/亞維農的少女)》还要多。\n\n经过上面的讨论，似乎可以得到一个论点：涉及艺术的审美评价仅仅是社会特定群体特定消费行为的经验事实，它与文化权力和视角有关。这种说法暗含一种指控，即艺术的审美趣味仅仅是艺术圈内人士的自娱自乐和自我感动，以及对于身份区分的表达，不具有任何普遍性意义。可笑的是，这恰恰是杜尚的小便池所要嘲弄的。那么，艺术的价值到底在哪里呢？它的本质又是什么？\n\n需要警惕，完全按照经验社会学的观点来看，把价值问题简化为权力和视角问题，就会落入相对主义的陷阱。这时，需要我们回到规范性的视角，从康德《判断力批判》确立的美学思想传统中，重新发现艺术的价值。在康德关于审美判断的预设中，有一个普遍性假设：**当一个人做出审美判断后，他应当有充分的理由让别人也接受这个判断**。这意味着，一个人的审美判断超越了个人的主观状态，可推广于他人。这种可推广性意味着对审美判断有效性的主张，而艺术的价值其实就寓含于这种有效性当中——因为是真实存在的价值支撑着审美主张的有效。因此，一件艺术品应该具有这样的品质：它不仅可以成为个人喜好的对象，还应是蕴含巨大的智力与情感价值的对象，能让每个熟悉它的人，都能领略它的价值和优秀品质。也只有从这个角度讲，一个工厂生产的普通陶瓷小便池才得以从它的孪生兄弟们中分离出来，成为能带给人“反思快感”的艺术品。","source":"_posts/Marcel Duchamp's Urinal.md","raw":"---\ntitle: 从社会学经验视角对杜尚《泉》的考察及反思\ndate: 2019-06-11 00:02:00\ncategories:\n- 杂想\ntags: \n- 社会科学\n- 艺术\n- 哲学\n---\n\n1917年4月，纽约独立艺术家协会计划在中央大厦举办一场艺术展览。美籍法裔艺术家马赛尔·杜尚把一个署有“R.Mutt”名字的陶瓷小便池提交给协会作为参展作品，命名为《泉》。尽管协会理事会拒绝将其展出，但这件充满挑衅性的作品引起了广泛的讨论，日后成为当代艺术的出发点。<!--more-->\n\n“*文化资本*”是布迪厄在艺术消费习性的社会学研究中确立的概念。他借鉴韦伯对社会分层的分析，将其应用到审美价值领域，认为社会结构的不平等不仅存在于经济资本，而且在文化领域还存在一种“文化资本”的不平等。布迪厄认为，拥有较多文化资本的人，对于复杂、抽象、“难懂”的艺术品的美学品质有更强的辨识欣赏技能，因此造成了艺术趣味的分层和区隔。而文化资本又很大程度依赖于经济资本：相比于工人阶级，一个在富裕的中产阶级家庭成长的孩子有更多的机会接触到高雅艺术（对欣赏技能有较高要求），这种机会体现在家庭环境的熏陶、教育背景、额外的金钱以及时间等方面。考察杜尚的家庭背景，不难发现他成长于一个典型的中产阶级家庭。杜尚的父亲是一名公证人，收入可观，受人尊敬，杜尚和他的家人住在一栋乡间别墅里。杜尚15岁开始画画，中学毕业后进入朱莉亚艺术学院学习，他的两个哥哥和一个妹妹也都学习艺术。在1917年杜尚递交《泉》给独立艺术家协会之前，他已经凭借《下楼梯的裸女》二号在美国取得知名度，移民美国后，他很快便与当地的艺术家及艺术赞助人们熟识。这表明杜尚在早年得以积累文化资本，并在创作《泉》时已经具有被当时的艺术家群体认可的文化资本，考虑到文化资本造成的趣味区隔是社会分层的重要依据，这一点对于极具反叛精神的《泉》得以被艺术界接纳至关重要。谈及“*艺术界*”，下面就要涉及到分析哲学的艺术体制理论。\n\n艺术体制理论认为，艺术和非艺术的区别是不能用肉眼直接区分的，仅通过物理属性无法识别艺术品，它们的识别仅仅取决于某种授予它们身份的社会体制。这种体制就是所谓的“*艺术界*”。用该理论代表学者迪基的话说：“在分类意义上，一件艺术品是：（1）一件人工制品；（2）代表某种社会体制的一些人，授予它作为鉴赏候选者的身份。” 透过这一视角，让我们关注当时是哪些力量推动了《泉》被纳入艺术范畴。在杜尚的小便池被拒之门外一个月后，达达主义艺术杂志《盲人》刊登了美国艺术家贝翠丝·伍德和沃尔特·康拉德·阿伦斯伯格共同撰写的题为《理察·马特一案》的文章。这篇文章为《泉》的艺术合法性地位做出辩护，后来的事实证明，它对当代艺术的发展产生了深远影响。与文章一起收录的，还有被称为“现代摄影之父“的阿尔弗雷德·斯蒂格利茨为《泉》拍摄的展览照片。另一位美国作家露易丝·瓦雷兹的文章《厕所神像》，指出《泉》除了戏谑外还有严肃的面向。这些艺术体制内具有话语权的权威们做出声明，称《泉》有资格作为一件艺术品参与展出，由此引发了界内对于艺术的反思和大讨论。其结果是：2004年，《泉》被500位英国艺术界的艺术家、艺术史学者票选为20世纪最具影响力的艺术作品，获得评审团64%的票数，比西班牙画家毕加索的著名画作《[亚维农的少女](https://zh.wikipedia.org/wiki/亞維農的少女)》还要多。\n\n经过上面的讨论，似乎可以得到一个论点：涉及艺术的审美评价仅仅是社会特定群体特定消费行为的经验事实，它与文化权力和视角有关。这种说法暗含一种指控，即艺术的审美趣味仅仅是艺术圈内人士的自娱自乐和自我感动，以及对于身份区分的表达，不具有任何普遍性意义。可笑的是，这恰恰是杜尚的小便池所要嘲弄的。那么，艺术的价值到底在哪里呢？它的本质又是什么？\n\n需要警惕，完全按照经验社会学的观点来看，把价值问题简化为权力和视角问题，就会落入相对主义的陷阱。这时，需要我们回到规范性的视角，从康德《判断力批判》确立的美学思想传统中，重新发现艺术的价值。在康德关于审美判断的预设中，有一个普遍性假设：**当一个人做出审美判断后，他应当有充分的理由让别人也接受这个判断**。这意味着，一个人的审美判断超越了个人的主观状态，可推广于他人。这种可推广性意味着对审美判断有效性的主张，而艺术的价值其实就寓含于这种有效性当中——因为是真实存在的价值支撑着审美主张的有效。因此，一件艺术品应该具有这样的品质：它不仅可以成为个人喜好的对象，还应是蕴含巨大的智力与情感价值的对象，能让每个熟悉它的人，都能领略它的价值和优秀品质。也只有从这个角度讲，一个工厂生产的普通陶瓷小便池才得以从它的孪生兄弟们中分离出来，成为能带给人“反思快感”的艺术品。","slug":"Marcel Duchamp's Urinal","published":1,"updated":"2020-12-14T16:13:40.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd5z000u2sfycdwngfek","content":"<p>1917年4月，纽约独立艺术家协会计划在中央大厦举办一场艺术展览。美籍法裔艺术家马赛尔·杜尚把一个署有“R.Mutt”名字的陶瓷小便池提交给协会作为参展作品，命名为《泉》。尽管协会理事会拒绝将其展出，但这件充满挑衅性的作品引起了广泛的讨论，日后成为当代艺术的出发点。<a id=\"more\"></a></p>\n<p>“<em>文化资本</em>”是布迪厄在艺术消费习性的社会学研究中确立的概念。他借鉴韦伯对社会分层的分析，将其应用到审美价值领域，认为社会结构的不平等不仅存在于经济资本，而且在文化领域还存在一种“文化资本”的不平等。布迪厄认为，拥有较多文化资本的人，对于复杂、抽象、“难懂”的艺术品的美学品质有更强的辨识欣赏技能，因此造成了艺术趣味的分层和区隔。而文化资本又很大程度依赖于经济资本：相比于工人阶级，一个在富裕的中产阶级家庭成长的孩子有更多的机会接触到高雅艺术（对欣赏技能有较高要求），这种机会体现在家庭环境的熏陶、教育背景、额外的金钱以及时间等方面。考察杜尚的家庭背景，不难发现他成长于一个典型的中产阶级家庭。杜尚的父亲是一名公证人，收入可观，受人尊敬，杜尚和他的家人住在一栋乡间别墅里。杜尚15岁开始画画，中学毕业后进入朱莉亚艺术学院学习，他的两个哥哥和一个妹妹也都学习艺术。在1917年杜尚递交《泉》给独立艺术家协会之前，他已经凭借《下楼梯的裸女》二号在美国取得知名度，移民美国后，他很快便与当地的艺术家及艺术赞助人们熟识。这表明杜尚在早年得以积累文化资本，并在创作《泉》时已经具有被当时的艺术家群体认可的文化资本，考虑到文化资本造成的趣味区隔是社会分层的重要依据，这一点对于极具反叛精神的《泉》得以被艺术界接纳至关重要。谈及“<em>艺术界</em>”，下面就要涉及到分析哲学的艺术体制理论。</p>\n<p>艺术体制理论认为，艺术和非艺术的区别是不能用肉眼直接区分的，仅通过物理属性无法识别艺术品，它们的识别仅仅取决于某种授予它们身份的社会体制。这种体制就是所谓的“<em>艺术界</em>”。用该理论代表学者迪基的话说：“在分类意义上，一件艺术品是：（1）一件人工制品；（2）代表某种社会体制的一些人，授予它作为鉴赏候选者的身份。” 透过这一视角，让我们关注当时是哪些力量推动了《泉》被纳入艺术范畴。在杜尚的小便池被拒之门外一个月后，达达主义艺术杂志《盲人》刊登了美国艺术家贝翠丝·伍德和沃尔特·康拉德·阿伦斯伯格共同撰写的题为《理察·马特一案》的文章。这篇文章为《泉》的艺术合法性地位做出辩护，后来的事实证明，它对当代艺术的发展产生了深远影响。与文章一起收录的，还有被称为“现代摄影之父“的阿尔弗雷德·斯蒂格利茨为《泉》拍摄的展览照片。另一位美国作家露易丝·瓦雷兹的文章《厕所神像》，指出《泉》除了戏谑外还有严肃的面向。这些艺术体制内具有话语权的权威们做出声明，称《泉》有资格作为一件艺术品参与展出，由此引发了界内对于艺术的反思和大讨论。其结果是：2004年，《泉》被500位英国艺术界的艺术家、艺术史学者票选为20世纪最具影响力的艺术作品，获得评审团64%的票数，比西班牙画家毕加索的著名画作《<a href=\"https://zh.wikipedia.org/wiki/亞維農的少女\" target=\"_blank\" rel=\"noopener\">亚维农的少女</a>》还要多。</p>\n<p>经过上面的讨论，似乎可以得到一个论点：涉及艺术的审美评价仅仅是社会特定群体特定消费行为的经验事实，它与文化权力和视角有关。这种说法暗含一种指控，即艺术的审美趣味仅仅是艺术圈内人士的自娱自乐和自我感动，以及对于身份区分的表达，不具有任何普遍性意义。可笑的是，这恰恰是杜尚的小便池所要嘲弄的。那么，艺术的价值到底在哪里呢？它的本质又是什么？</p>\n<p>需要警惕，完全按照经验社会学的观点来看，把价值问题简化为权力和视角问题，就会落入相对主义的陷阱。这时，需要我们回到规范性的视角，从康德《判断力批判》确立的美学思想传统中，重新发现艺术的价值。在康德关于审美判断的预设中，有一个普遍性假设：<strong>当一个人做出审美判断后，他应当有充分的理由让别人也接受这个判断</strong>。这意味着，一个人的审美判断超越了个人的主观状态，可推广于他人。这种可推广性意味着对审美判断有效性的主张，而艺术的价值其实就寓含于这种有效性当中——因为是真实存在的价值支撑着审美主张的有效。因此，一件艺术品应该具有这样的品质：它不仅可以成为个人喜好的对象，还应是蕴含巨大的智力与情感价值的对象，能让每个熟悉它的人，都能领略它的价值和优秀品质。也只有从这个角度讲，一个工厂生产的普通陶瓷小便池才得以从它的孪生兄弟们中分离出来，成为能带给人“反思快感”的艺术品。</p>\n","site":{"data":{}},"excerpt":"<p>1917年4月，纽约独立艺术家协会计划在中央大厦举办一场艺术展览。美籍法裔艺术家马赛尔·杜尚把一个署有“R.Mutt”名字的陶瓷小便池提交给协会作为参展作品，命名为《泉》。尽管协会理事会拒绝将其展出，但这件充满挑衅性的作品引起了广泛的讨论，日后成为当代艺术的出发点。","more":"</p>\n<p>“<em>文化资本</em>”是布迪厄在艺术消费习性的社会学研究中确立的概念。他借鉴韦伯对社会分层的分析，将其应用到审美价值领域，认为社会结构的不平等不仅存在于经济资本，而且在文化领域还存在一种“文化资本”的不平等。布迪厄认为，拥有较多文化资本的人，对于复杂、抽象、“难懂”的艺术品的美学品质有更强的辨识欣赏技能，因此造成了艺术趣味的分层和区隔。而文化资本又很大程度依赖于经济资本：相比于工人阶级，一个在富裕的中产阶级家庭成长的孩子有更多的机会接触到高雅艺术（对欣赏技能有较高要求），这种机会体现在家庭环境的熏陶、教育背景、额外的金钱以及时间等方面。考察杜尚的家庭背景，不难发现他成长于一个典型的中产阶级家庭。杜尚的父亲是一名公证人，收入可观，受人尊敬，杜尚和他的家人住在一栋乡间别墅里。杜尚15岁开始画画，中学毕业后进入朱莉亚艺术学院学习，他的两个哥哥和一个妹妹也都学习艺术。在1917年杜尚递交《泉》给独立艺术家协会之前，他已经凭借《下楼梯的裸女》二号在美国取得知名度，移民美国后，他很快便与当地的艺术家及艺术赞助人们熟识。这表明杜尚在早年得以积累文化资本，并在创作《泉》时已经具有被当时的艺术家群体认可的文化资本，考虑到文化资本造成的趣味区隔是社会分层的重要依据，这一点对于极具反叛精神的《泉》得以被艺术界接纳至关重要。谈及“<em>艺术界</em>”，下面就要涉及到分析哲学的艺术体制理论。</p>\n<p>艺术体制理论认为，艺术和非艺术的区别是不能用肉眼直接区分的，仅通过物理属性无法识别艺术品，它们的识别仅仅取决于某种授予它们身份的社会体制。这种体制就是所谓的“<em>艺术界</em>”。用该理论代表学者迪基的话说：“在分类意义上，一件艺术品是：（1）一件人工制品；（2）代表某种社会体制的一些人，授予它作为鉴赏候选者的身份。” 透过这一视角，让我们关注当时是哪些力量推动了《泉》被纳入艺术范畴。在杜尚的小便池被拒之门外一个月后，达达主义艺术杂志《盲人》刊登了美国艺术家贝翠丝·伍德和沃尔特·康拉德·阿伦斯伯格共同撰写的题为《理察·马特一案》的文章。这篇文章为《泉》的艺术合法性地位做出辩护，后来的事实证明，它对当代艺术的发展产生了深远影响。与文章一起收录的，还有被称为“现代摄影之父“的阿尔弗雷德·斯蒂格利茨为《泉》拍摄的展览照片。另一位美国作家露易丝·瓦雷兹的文章《厕所神像》，指出《泉》除了戏谑外还有严肃的面向。这些艺术体制内具有话语权的权威们做出声明，称《泉》有资格作为一件艺术品参与展出，由此引发了界内对于艺术的反思和大讨论。其结果是：2004年，《泉》被500位英国艺术界的艺术家、艺术史学者票选为20世纪最具影响力的艺术作品，获得评审团64%的票数，比西班牙画家毕加索的著名画作《<a href=\"https://zh.wikipedia.org/wiki/亞維農的少女\" target=\"_blank\" rel=\"noopener\">亚维农的少女</a>》还要多。</p>\n<p>经过上面的讨论，似乎可以得到一个论点：涉及艺术的审美评价仅仅是社会特定群体特定消费行为的经验事实，它与文化权力和视角有关。这种说法暗含一种指控，即艺术的审美趣味仅仅是艺术圈内人士的自娱自乐和自我感动，以及对于身份区分的表达，不具有任何普遍性意义。可笑的是，这恰恰是杜尚的小便池所要嘲弄的。那么，艺术的价值到底在哪里呢？它的本质又是什么？</p>\n<p>需要警惕，完全按照经验社会学的观点来看，把价值问题简化为权力和视角问题，就会落入相对主义的陷阱。这时，需要我们回到规范性的视角，从康德《判断力批判》确立的美学思想传统中，重新发现艺术的价值。在康德关于审美判断的预设中，有一个普遍性假设：<strong>当一个人做出审美判断后，他应当有充分的理由让别人也接受这个判断</strong>。这意味着，一个人的审美判断超越了个人的主观状态，可推广于他人。这种可推广性意味着对审美判断有效性的主张，而艺术的价值其实就寓含于这种有效性当中——因为是真实存在的价值支撑着审美主张的有效。因此，一件艺术品应该具有这样的品质：它不仅可以成为个人喜好的对象，还应是蕴含巨大的智力与情感价值的对象，能让每个熟悉它的人，都能领略它的价值和优秀品质。也只有从这个角度讲，一个工厂生产的普通陶瓷小便池才得以从它的孪生兄弟们中分离出来，成为能带给人“反思快感”的艺术品。</p>"},{"title":"与鲁迅相遇","date":"2018-06-26T17:29:00.000Z","_content":"\n我很早便看过鲁迅的作品，因为初等教育的语文课本辑了他许多文章进去。《从百草园到三味书屋》《孔乙己》《祝福》《社戏》《少年闰土》，这些文章构成了我对鲁迅的最初印象。然而很长一段时间，我并没能理解这位作家和他的作品。随着心智的成熟，当我回头重读鲁迅作品，又看到他人眼中的鲁迅时，才有些豁然开朗之感。<!--more-->\n\n正像余华写的那样，出现在中小学语文课本上的鲁迅，对多数的孩子来说只是一个“词汇”，而不是一个作家，立体的人。四十多年前，“鲁迅”在少年余华眼里，是地位仅次于毛泽东的人，是永远伟大正确，是“鲁迅先生说过”；多年以后，“鲁迅”仍作为中国历史上“伟大的文学家，思想家，革命家”，出现在语文课本上供学习和瞻仰。我最初读到的他的几篇文章里，印象深刻的有《从百草园到三味书屋》《少年闰土》和《祝福》。当初应付答卷背过的那些写作技巧、中心思想分析都不记得了，倒是些背景模糊的图像零零散散地搁浅在了脑子里。“碧绿的菜畦，光滑的石井栏，高大的皂荚树，紫红的桑椹”，低唱的油蛉，蟋蟀，人形的何首乌……尽管儿时的我只见过蟋蟀，其他多半不认得名字或没有见过实物，但还是凭着孩子的想象力勾勒出了百草园的样子。看到《少年闰土》，就会想起那幅图画：“深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜。其间有一个十一二岁的少年，项带银圈，手捏一柄钢叉，向一匹猹尽力地刺去。那[猹](https://baike.baidu.com/item/猹)却将身一扭，反从他的胯下逃走了。”《祝福》给我留下深刻的印象，是因为祥林嫂悲剧的命运引起了我深深的同情和沉重的悲哀。这些就是鲁迅给少年时的我留下的全部印象了。在我还不懂得什么叫“革命”的年纪，除了祥林嫂的悲惨命运使我感到沉重之外，孔乙己，阿Q这些人物，我只觉得奇怪和好笑罢了。再则鲁迅的语言在今天读来也有点晦涩拗口，对一个孩子来说并不讨喜，所以只有那些富有童趣的场景描绘留在了我的脑海里。\n\n今天回过头看，少年时期学校的教育并没有帮我认识鲁迅。鲁迅不在试卷分析题里，不在老师的讲义里，在“伟大的文学家，思想家，革命家”之前，鲁迅是一个作家，再之前，是一个人。想要看懂一个历史人物，不了解历史、近代变革的社会背景，是很难做到的。除此之外，鲁迅的文章带有强烈的目的性，他要改造国民的思想。这就是说，读者最好有一些思想基础，保守也好，激进也好，传统也好，现代也好，多少要有些主张的。最怕的是一张白纸，既然是空白，就无所谓好坏对错，也分不清愚蠢和明智，鲁迅的“枪笔”遇到这样的读者，力量就减损多半。多数儿童就是这样的读者，所以“鲁迅”对这些儿童来说，只是课本上的一个词汇。\n\n上段言及鲁迅写作有很强的目的性。接下来就谈谈鲁迅带着什么样的目的写作。乔治·奥威尔曾在《我为何写作》中谈过他所理解的四种写作动机：\n\n*1）纯粹的个人主义*\n\n*2）美学热情。*\n\n*3）历史冲动。*\n\n*4）政治目的。*\n\n鲁迅最明显的写作目的大概是“政治目的”。奥维尔定义的“政治目的”指希望将社会朝着某个方向推动。在《文化偏至论》中，鲁迅写道：*“文明无不根旧迹而演来，亦以矫往事而偏至。”* 后来的《摩罗诗力说》里，他歌颂了西方一批“精粹界之战士”，如拜伦、雪莱、普希金、密茨凯维支、斯洛伐茨基等。他们*“无不刚健不挠，抱诚守真；不取媚于群，以随顺俗；发为雄声，以起其国人之新生，而大其国于天下。”*1933年的一篇文章里，鲁迅更直接地说：*“我也并没有想把小说抬进’文苑’的意思，不过想利用他的力量，来改良社会。”*可以看出，鲁迅相信社会中存在一些先驱者，他们能够推动文化偏至之钟摆，影响社会变革的进程。鲁迅要扮演的就是这个角色，文学就是他改良社会的板斧。《呐喊》《彷徨》就是带着这种目的创造出的产物。这时候，他明确表达了自己的文学的艺术性让位于政治目的，即写作的政治目的大于美学热情。\n\n李欧梵先生从心理学角度对鲁迅走上文学道路的影响进行阐释，也很有趣。他认为鲁迅父亲病死给青年鲁迅的心理注入了某种“黑暗力量”，类似于某种心理的“诅咒”。艾利克逊也在马丁·路德金、甘地等人的生活中找到了类似的“诅咒”，即*“带着不惜一切代价超越和创新的责任感，并被罪恶感缠绕而成长起来。”*如果说鲁迅最初学医的选择是因为父亲的病未能被中医治好而背负了某种精神上的罪恶感和责任感，以及将日本的医学教育条件良好纳入考量，那在学医之后，鲁迅的思想慢慢发生了转变。鲁迅曾告诉许寿裳，他学医是为了通过科学解决三个问题——“怎样才是最理想的人性？中国国民性中缺乏的是什么？病根在哪里？”可见，彼时的鲁迅已经逐渐将文化道德置于科学之上。直到后来的“幻灯片事件”，鲁迅最终做出了弃医从文，以文学改造国民思想的决定。 \n\n作为一个文人，鲁迅是颇具知识分子的浪漫主义情怀的。这种浪漫情怀对知识分子很有吸引力。一个“精粹界之战士”在愚昧或疯狂的时代独醒独行，叛逆诗人，孤独斗士，理想主义的殉道者，多么浪漫的悲剧啊。鲁迅是决意了要做这样的人物的。《随感录三十八》是他的一篇战斗宣言：*“他们必定觉得自己思想见识高出庸众之上，又为庸众所不懂，所以愤世嫉俗，渐渐变成厌世家，或’国民之敌’。但一切新思想，多从他们出来，政治上宗教上道德上的改革，也从他们发端。”*这样的人是痛苦的，但这痛苦带来的浪漫又有麻醉效果。下面就要说说鲁迅值得我们反思之处了。\n\n毫无疑问，鲁迅的文学是尖锐辛辣的，他把文字当作了板斧，挥向腐朽的中国传统社会现实。因为他看到了现实的深刻和苍凉，所以态度也是激进的，不愿与落后的传统有任何调情。左派知识分子，多有些极端偏执，正如《狂人日记》中的”疯子“一样，自大到自负，不惮与全世界为敌。这之于历史当然有进步的一面。革命，大刀阔斧的除旧替新，慷慨激昂的警世呐喊，能在腐臭淤滞的旧社会搅起激流涌动，大大加速历史的进程。但既是反思，当然要谈谈激进之弊。尖刻凶猛的文字如大棒，固然富有力量，但同样潜藏危险。鲁迅的文字太过凶猛，以致于在鲁迅被偶像化甚至神化的时期，作为神的鲁迅被用作万能兵器，批胡风，文革批周扬，文革后批”四人帮“。鲁迅成了锋利的刀枪，被不分青红皂白地利用。 后期的鲁迅愈发激进偏执，有人曾评论：*“他四面树敌，攻击’新月派’与胡适等具有自由主义倾向的知识分子，把资产阶级、小布尔乔亚挂在嘴边，要打’落水狗’，‘资本家的乏走狗’等等。在一些谁拿了帝国主义的英镑，谁拿了苏俄的卢布上耿耿于怀，几近于斤斤计较的小人心理，既小气又阴暗。”* “个人的自大”走向极端变成唯我独尊的狂妄和偏执，使得鲁迅也没有逃不过中国传统知识分子“好为人师”的臭毛病。《两地书》是鲁迅与妻子许广平的通信，却怎么也读不出是夫妻之间的谈话，分明是老师对学生自上而下的讲授。\n\n说到这里，我还尚未提及鲁迅作品的艺术价值。这并不是说鲁迅文学的艺术价值较之其思想价值不值得一提，实在是我这位读者的艺术修养不够，没有什么资格对它的文学艺术性作出一番评价。倘若打肿脸充胖子，拿些小学阅读理解标准答案来凑数，我觉得实在没有必要。这方面，国内外众多文人学者都有精彩的评述，我做一个小学生虚心学习就好。\n\n 以上就是我与鲁迅的相遇和自己的一些鄙陋之见。","source":"_posts/LuXun.md","raw":"---\ntitle: 与鲁迅相遇\ndate: 2018-06-27 01:29:00\ncategories:\n- 杂想\ntags: \n- 近代史\n---\n\n我很早便看过鲁迅的作品，因为初等教育的语文课本辑了他许多文章进去。《从百草园到三味书屋》《孔乙己》《祝福》《社戏》《少年闰土》，这些文章构成了我对鲁迅的最初印象。然而很长一段时间，我并没能理解这位作家和他的作品。随着心智的成熟，当我回头重读鲁迅作品，又看到他人眼中的鲁迅时，才有些豁然开朗之感。<!--more-->\n\n正像余华写的那样，出现在中小学语文课本上的鲁迅，对多数的孩子来说只是一个“词汇”，而不是一个作家，立体的人。四十多年前，“鲁迅”在少年余华眼里，是地位仅次于毛泽东的人，是永远伟大正确，是“鲁迅先生说过”；多年以后，“鲁迅”仍作为中国历史上“伟大的文学家，思想家，革命家”，出现在语文课本上供学习和瞻仰。我最初读到的他的几篇文章里，印象深刻的有《从百草园到三味书屋》《少年闰土》和《祝福》。当初应付答卷背过的那些写作技巧、中心思想分析都不记得了，倒是些背景模糊的图像零零散散地搁浅在了脑子里。“碧绿的菜畦，光滑的石井栏，高大的皂荚树，紫红的桑椹”，低唱的油蛉，蟋蟀，人形的何首乌……尽管儿时的我只见过蟋蟀，其他多半不认得名字或没有见过实物，但还是凭着孩子的想象力勾勒出了百草园的样子。看到《少年闰土》，就会想起那幅图画：“深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜。其间有一个十一二岁的少年，项带银圈，手捏一柄钢叉，向一匹猹尽力地刺去。那[猹](https://baike.baidu.com/item/猹)却将身一扭，反从他的胯下逃走了。”《祝福》给我留下深刻的印象，是因为祥林嫂悲剧的命运引起了我深深的同情和沉重的悲哀。这些就是鲁迅给少年时的我留下的全部印象了。在我还不懂得什么叫“革命”的年纪，除了祥林嫂的悲惨命运使我感到沉重之外，孔乙己，阿Q这些人物，我只觉得奇怪和好笑罢了。再则鲁迅的语言在今天读来也有点晦涩拗口，对一个孩子来说并不讨喜，所以只有那些富有童趣的场景描绘留在了我的脑海里。\n\n今天回过头看，少年时期学校的教育并没有帮我认识鲁迅。鲁迅不在试卷分析题里，不在老师的讲义里，在“伟大的文学家，思想家，革命家”之前，鲁迅是一个作家，再之前，是一个人。想要看懂一个历史人物，不了解历史、近代变革的社会背景，是很难做到的。除此之外，鲁迅的文章带有强烈的目的性，他要改造国民的思想。这就是说，读者最好有一些思想基础，保守也好，激进也好，传统也好，现代也好，多少要有些主张的。最怕的是一张白纸，既然是空白，就无所谓好坏对错，也分不清愚蠢和明智，鲁迅的“枪笔”遇到这样的读者，力量就减损多半。多数儿童就是这样的读者，所以“鲁迅”对这些儿童来说，只是课本上的一个词汇。\n\n上段言及鲁迅写作有很强的目的性。接下来就谈谈鲁迅带着什么样的目的写作。乔治·奥威尔曾在《我为何写作》中谈过他所理解的四种写作动机：\n\n*1）纯粹的个人主义*\n\n*2）美学热情。*\n\n*3）历史冲动。*\n\n*4）政治目的。*\n\n鲁迅最明显的写作目的大概是“政治目的”。奥维尔定义的“政治目的”指希望将社会朝着某个方向推动。在《文化偏至论》中，鲁迅写道：*“文明无不根旧迹而演来，亦以矫往事而偏至。”* 后来的《摩罗诗力说》里，他歌颂了西方一批“精粹界之战士”，如拜伦、雪莱、普希金、密茨凯维支、斯洛伐茨基等。他们*“无不刚健不挠，抱诚守真；不取媚于群，以随顺俗；发为雄声，以起其国人之新生，而大其国于天下。”*1933年的一篇文章里，鲁迅更直接地说：*“我也并没有想把小说抬进’文苑’的意思，不过想利用他的力量，来改良社会。”*可以看出，鲁迅相信社会中存在一些先驱者，他们能够推动文化偏至之钟摆，影响社会变革的进程。鲁迅要扮演的就是这个角色，文学就是他改良社会的板斧。《呐喊》《彷徨》就是带着这种目的创造出的产物。这时候，他明确表达了自己的文学的艺术性让位于政治目的，即写作的政治目的大于美学热情。\n\n李欧梵先生从心理学角度对鲁迅走上文学道路的影响进行阐释，也很有趣。他认为鲁迅父亲病死给青年鲁迅的心理注入了某种“黑暗力量”，类似于某种心理的“诅咒”。艾利克逊也在马丁·路德金、甘地等人的生活中找到了类似的“诅咒”，即*“带着不惜一切代价超越和创新的责任感，并被罪恶感缠绕而成长起来。”*如果说鲁迅最初学医的选择是因为父亲的病未能被中医治好而背负了某种精神上的罪恶感和责任感，以及将日本的医学教育条件良好纳入考量，那在学医之后，鲁迅的思想慢慢发生了转变。鲁迅曾告诉许寿裳，他学医是为了通过科学解决三个问题——“怎样才是最理想的人性？中国国民性中缺乏的是什么？病根在哪里？”可见，彼时的鲁迅已经逐渐将文化道德置于科学之上。直到后来的“幻灯片事件”，鲁迅最终做出了弃医从文，以文学改造国民思想的决定。 \n\n作为一个文人，鲁迅是颇具知识分子的浪漫主义情怀的。这种浪漫情怀对知识分子很有吸引力。一个“精粹界之战士”在愚昧或疯狂的时代独醒独行，叛逆诗人，孤独斗士，理想主义的殉道者，多么浪漫的悲剧啊。鲁迅是决意了要做这样的人物的。《随感录三十八》是他的一篇战斗宣言：*“他们必定觉得自己思想见识高出庸众之上，又为庸众所不懂，所以愤世嫉俗，渐渐变成厌世家，或’国民之敌’。但一切新思想，多从他们出来，政治上宗教上道德上的改革，也从他们发端。”*这样的人是痛苦的，但这痛苦带来的浪漫又有麻醉效果。下面就要说说鲁迅值得我们反思之处了。\n\n毫无疑问，鲁迅的文学是尖锐辛辣的，他把文字当作了板斧，挥向腐朽的中国传统社会现实。因为他看到了现实的深刻和苍凉，所以态度也是激进的，不愿与落后的传统有任何调情。左派知识分子，多有些极端偏执，正如《狂人日记》中的”疯子“一样，自大到自负，不惮与全世界为敌。这之于历史当然有进步的一面。革命，大刀阔斧的除旧替新，慷慨激昂的警世呐喊，能在腐臭淤滞的旧社会搅起激流涌动，大大加速历史的进程。但既是反思，当然要谈谈激进之弊。尖刻凶猛的文字如大棒，固然富有力量，但同样潜藏危险。鲁迅的文字太过凶猛，以致于在鲁迅被偶像化甚至神化的时期，作为神的鲁迅被用作万能兵器，批胡风，文革批周扬，文革后批”四人帮“。鲁迅成了锋利的刀枪，被不分青红皂白地利用。 后期的鲁迅愈发激进偏执，有人曾评论：*“他四面树敌，攻击’新月派’与胡适等具有自由主义倾向的知识分子，把资产阶级、小布尔乔亚挂在嘴边，要打’落水狗’，‘资本家的乏走狗’等等。在一些谁拿了帝国主义的英镑，谁拿了苏俄的卢布上耿耿于怀，几近于斤斤计较的小人心理，既小气又阴暗。”* “个人的自大”走向极端变成唯我独尊的狂妄和偏执，使得鲁迅也没有逃不过中国传统知识分子“好为人师”的臭毛病。《两地书》是鲁迅与妻子许广平的通信，却怎么也读不出是夫妻之间的谈话，分明是老师对学生自上而下的讲授。\n\n说到这里，我还尚未提及鲁迅作品的艺术价值。这并不是说鲁迅文学的艺术价值较之其思想价值不值得一提，实在是我这位读者的艺术修养不够，没有什么资格对它的文学艺术性作出一番评价。倘若打肿脸充胖子，拿些小学阅读理解标准答案来凑数，我觉得实在没有必要。这方面，国内外众多文人学者都有精彩的评述，我做一个小学生虚心学习就好。\n\n 以上就是我与鲁迅的相遇和自己的一些鄙陋之见。","slug":"LuXun","published":1,"updated":"2021-08-26T13:13:59.898Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd63000x2sfy2cue9d1p","content":"<p>我很早便看过鲁迅的作品，因为初等教育的语文课本辑了他许多文章进去。《从百草园到三味书屋》《孔乙己》《祝福》《社戏》《少年闰土》，这些文章构成了我对鲁迅的最初印象。然而很长一段时间，我并没能理解这位作家和他的作品。随着心智的成熟，当我回头重读鲁迅作品，又看到他人眼中的鲁迅时，才有些豁然开朗之感。<a id=\"more\"></a></p>\n<p>正像余华写的那样，出现在中小学语文课本上的鲁迅，对多数的孩子来说只是一个“词汇”，而不是一个作家，立体的人。四十多年前，“鲁迅”在少年余华眼里，是地位仅次于毛泽东的人，是永远伟大正确，是“鲁迅先生说过”；多年以后，“鲁迅”仍作为中国历史上“伟大的文学家，思想家，革命家”，出现在语文课本上供学习和瞻仰。我最初读到的他的几篇文章里，印象深刻的有《从百草园到三味书屋》《少年闰土》和《祝福》。当初应付答卷背过的那些写作技巧、中心思想分析都不记得了，倒是些背景模糊的图像零零散散地搁浅在了脑子里。“碧绿的菜畦，光滑的石井栏，高大的皂荚树，紫红的桑椹”，低唱的油蛉，蟋蟀，人形的何首乌……尽管儿时的我只见过蟋蟀，其他多半不认得名字或没有见过实物，但还是凭着孩子的想象力勾勒出了百草园的样子。看到《少年闰土》，就会想起那幅图画：“深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜。其间有一个十一二岁的少年，项带银圈，手捏一柄钢叉，向一匹猹尽力地刺去。那<a href=\"https://baike.baidu.com/item/猹\" target=\"_blank\" rel=\"noopener\">猹</a>却将身一扭，反从他的胯下逃走了。”《祝福》给我留下深刻的印象，是因为祥林嫂悲剧的命运引起了我深深的同情和沉重的悲哀。这些就是鲁迅给少年时的我留下的全部印象了。在我还不懂得什么叫“革命”的年纪，除了祥林嫂的悲惨命运使我感到沉重之外，孔乙己，阿Q这些人物，我只觉得奇怪和好笑罢了。再则鲁迅的语言在今天读来也有点晦涩拗口，对一个孩子来说并不讨喜，所以只有那些富有童趣的场景描绘留在了我的脑海里。</p>\n<p>今天回过头看，少年时期学校的教育并没有帮我认识鲁迅。鲁迅不在试卷分析题里，不在老师的讲义里，在“伟大的文学家，思想家，革命家”之前，鲁迅是一个作家，再之前，是一个人。想要看懂一个历史人物，不了解历史、近代变革的社会背景，是很难做到的。除此之外，鲁迅的文章带有强烈的目的性，他要改造国民的思想。这就是说，读者最好有一些思想基础，保守也好，激进也好，传统也好，现代也好，多少要有些主张的。最怕的是一张白纸，既然是空白，就无所谓好坏对错，也分不清愚蠢和明智，鲁迅的“枪笔”遇到这样的读者，力量就减损多半。多数儿童就是这样的读者，所以“鲁迅”对这些儿童来说，只是课本上的一个词汇。</p>\n<p>上段言及鲁迅写作有很强的目的性。接下来就谈谈鲁迅带着什么样的目的写作。乔治·奥威尔曾在《我为何写作》中谈过他所理解的四种写作动机：</p>\n<p><em>1）纯粹的个人主义</em></p>\n<p><em>2）美学热情。</em></p>\n<p><em>3）历史冲动。</em></p>\n<p><em>4）政治目的。</em></p>\n<p>鲁迅最明显的写作目的大概是“政治目的”。奥维尔定义的“政治目的”指希望将社会朝着某个方向推动。在《文化偏至论》中，鲁迅写道：<em>“文明无不根旧迹而演来，亦以矫往事而偏至。”</em> 后来的《摩罗诗力说》里，他歌颂了西方一批“精粹界之战士”，如拜伦、雪莱、普希金、密茨凯维支、斯洛伐茨基等。他们<em>“无不刚健不挠，抱诚守真；不取媚于群，以随顺俗；发为雄声，以起其国人之新生，而大其国于天下。”</em>1933年的一篇文章里，鲁迅更直接地说：<em>“我也并没有想把小说抬进’文苑’的意思，不过想利用他的力量，来改良社会。”</em>可以看出，鲁迅相信社会中存在一些先驱者，他们能够推动文化偏至之钟摆，影响社会变革的进程。鲁迅要扮演的就是这个角色，文学就是他改良社会的板斧。《呐喊》《彷徨》就是带着这种目的创造出的产物。这时候，他明确表达了自己的文学的艺术性让位于政治目的，即写作的政治目的大于美学热情。</p>\n<p>李欧梵先生从心理学角度对鲁迅走上文学道路的影响进行阐释，也很有趣。他认为鲁迅父亲病死给青年鲁迅的心理注入了某种“黑暗力量”，类似于某种心理的“诅咒”。艾利克逊也在马丁·路德金、甘地等人的生活中找到了类似的“诅咒”，即<em>“带着不惜一切代价超越和创新的责任感，并被罪恶感缠绕而成长起来。”</em>如果说鲁迅最初学医的选择是因为父亲的病未能被中医治好而背负了某种精神上的罪恶感和责任感，以及将日本的医学教育条件良好纳入考量，那在学医之后，鲁迅的思想慢慢发生了转变。鲁迅曾告诉许寿裳，他学医是为了通过科学解决三个问题——“怎样才是最理想的人性？中国国民性中缺乏的是什么？病根在哪里？”可见，彼时的鲁迅已经逐渐将文化道德置于科学之上。直到后来的“幻灯片事件”，鲁迅最终做出了弃医从文，以文学改造国民思想的决定。 </p>\n<p>作为一个文人，鲁迅是颇具知识分子的浪漫主义情怀的。这种浪漫情怀对知识分子很有吸引力。一个“精粹界之战士”在愚昧或疯狂的时代独醒独行，叛逆诗人，孤独斗士，理想主义的殉道者，多么浪漫的悲剧啊。鲁迅是决意了要做这样的人物的。《随感录三十八》是他的一篇战斗宣言：<em>“他们必定觉得自己思想见识高出庸众之上，又为庸众所不懂，所以愤世嫉俗，渐渐变成厌世家，或’国民之敌’。但一切新思想，多从他们出来，政治上宗教上道德上的改革，也从他们发端。”</em>这样的人是痛苦的，但这痛苦带来的浪漫又有麻醉效果。下面就要说说鲁迅值得我们反思之处了。</p>\n<p>毫无疑问，鲁迅的文学是尖锐辛辣的，他把文字当作了板斧，挥向腐朽的中国传统社会现实。因为他看到了现实的深刻和苍凉，所以态度也是激进的，不愿与落后的传统有任何调情。左派知识分子，多有些极端偏执，正如《狂人日记》中的”疯子“一样，自大到自负，不惮与全世界为敌。这之于历史当然有进步的一面。革命，大刀阔斧的除旧替新，慷慨激昂的警世呐喊，能在腐臭淤滞的旧社会搅起激流涌动，大大加速历史的进程。但既是反思，当然要谈谈激进之弊。尖刻凶猛的文字如大棒，固然富有力量，但同样潜藏危险。鲁迅的文字太过凶猛，以致于在鲁迅被偶像化甚至神化的时期，作为神的鲁迅被用作万能兵器，批胡风，文革批周扬，文革后批”四人帮“。鲁迅成了锋利的刀枪，被不分青红皂白地利用。 后期的鲁迅愈发激进偏执，有人曾评论：<em>“他四面树敌，攻击’新月派’与胡适等具有自由主义倾向的知识分子，把资产阶级、小布尔乔亚挂在嘴边，要打’落水狗’，‘资本家的乏走狗’等等。在一些谁拿了帝国主义的英镑，谁拿了苏俄的卢布上耿耿于怀，几近于斤斤计较的小人心理，既小气又阴暗。”</em> “个人的自大”走向极端变成唯我独尊的狂妄和偏执，使得鲁迅也没有逃不过中国传统知识分子“好为人师”的臭毛病。《两地书》是鲁迅与妻子许广平的通信，却怎么也读不出是夫妻之间的谈话，分明是老师对学生自上而下的讲授。</p>\n<p>说到这里，我还尚未提及鲁迅作品的艺术价值。这并不是说鲁迅文学的艺术价值较之其思想价值不值得一提，实在是我这位读者的艺术修养不够，没有什么资格对它的文学艺术性作出一番评价。倘若打肿脸充胖子，拿些小学阅读理解标准答案来凑数，我觉得实在没有必要。这方面，国内外众多文人学者都有精彩的评述，我做一个小学生虚心学习就好。</p>\n<p> 以上就是我与鲁迅的相遇和自己的一些鄙陋之见。</p>\n","site":{"data":{}},"excerpt":"<p>我很早便看过鲁迅的作品，因为初等教育的语文课本辑了他许多文章进去。《从百草园到三味书屋》《孔乙己》《祝福》《社戏》《少年闰土》，这些文章构成了我对鲁迅的最初印象。然而很长一段时间，我并没能理解这位作家和他的作品。随着心智的成熟，当我回头重读鲁迅作品，又看到他人眼中的鲁迅时，才有些豁然开朗之感。","more":"</p>\n<p>正像余华写的那样，出现在中小学语文课本上的鲁迅，对多数的孩子来说只是一个“词汇”，而不是一个作家，立体的人。四十多年前，“鲁迅”在少年余华眼里，是地位仅次于毛泽东的人，是永远伟大正确，是“鲁迅先生说过”；多年以后，“鲁迅”仍作为中国历史上“伟大的文学家，思想家，革命家”，出现在语文课本上供学习和瞻仰。我最初读到的他的几篇文章里，印象深刻的有《从百草园到三味书屋》《少年闰土》和《祝福》。当初应付答卷背过的那些写作技巧、中心思想分析都不记得了，倒是些背景模糊的图像零零散散地搁浅在了脑子里。“碧绿的菜畦，光滑的石井栏，高大的皂荚树，紫红的桑椹”，低唱的油蛉，蟋蟀，人形的何首乌……尽管儿时的我只见过蟋蟀，其他多半不认得名字或没有见过实物，但还是凭着孩子的想象力勾勒出了百草园的样子。看到《少年闰土》，就会想起那幅图画：“深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜。其间有一个十一二岁的少年，项带银圈，手捏一柄钢叉，向一匹猹尽力地刺去。那<a href=\"https://baike.baidu.com/item/猹\" target=\"_blank\" rel=\"noopener\">猹</a>却将身一扭，反从他的胯下逃走了。”《祝福》给我留下深刻的印象，是因为祥林嫂悲剧的命运引起了我深深的同情和沉重的悲哀。这些就是鲁迅给少年时的我留下的全部印象了。在我还不懂得什么叫“革命”的年纪，除了祥林嫂的悲惨命运使我感到沉重之外，孔乙己，阿Q这些人物，我只觉得奇怪和好笑罢了。再则鲁迅的语言在今天读来也有点晦涩拗口，对一个孩子来说并不讨喜，所以只有那些富有童趣的场景描绘留在了我的脑海里。</p>\n<p>今天回过头看，少年时期学校的教育并没有帮我认识鲁迅。鲁迅不在试卷分析题里，不在老师的讲义里，在“伟大的文学家，思想家，革命家”之前，鲁迅是一个作家，再之前，是一个人。想要看懂一个历史人物，不了解历史、近代变革的社会背景，是很难做到的。除此之外，鲁迅的文章带有强烈的目的性，他要改造国民的思想。这就是说，读者最好有一些思想基础，保守也好，激进也好，传统也好，现代也好，多少要有些主张的。最怕的是一张白纸，既然是空白，就无所谓好坏对错，也分不清愚蠢和明智，鲁迅的“枪笔”遇到这样的读者，力量就减损多半。多数儿童就是这样的读者，所以“鲁迅”对这些儿童来说，只是课本上的一个词汇。</p>\n<p>上段言及鲁迅写作有很强的目的性。接下来就谈谈鲁迅带着什么样的目的写作。乔治·奥威尔曾在《我为何写作》中谈过他所理解的四种写作动机：</p>\n<p><em>1）纯粹的个人主义</em></p>\n<p><em>2）美学热情。</em></p>\n<p><em>3）历史冲动。</em></p>\n<p><em>4）政治目的。</em></p>\n<p>鲁迅最明显的写作目的大概是“政治目的”。奥维尔定义的“政治目的”指希望将社会朝着某个方向推动。在《文化偏至论》中，鲁迅写道：<em>“文明无不根旧迹而演来，亦以矫往事而偏至。”</em> 后来的《摩罗诗力说》里，他歌颂了西方一批“精粹界之战士”，如拜伦、雪莱、普希金、密茨凯维支、斯洛伐茨基等。他们<em>“无不刚健不挠，抱诚守真；不取媚于群，以随顺俗；发为雄声，以起其国人之新生，而大其国于天下。”</em>1933年的一篇文章里，鲁迅更直接地说：<em>“我也并没有想把小说抬进’文苑’的意思，不过想利用他的力量，来改良社会。”</em>可以看出，鲁迅相信社会中存在一些先驱者，他们能够推动文化偏至之钟摆，影响社会变革的进程。鲁迅要扮演的就是这个角色，文学就是他改良社会的板斧。《呐喊》《彷徨》就是带着这种目的创造出的产物。这时候，他明确表达了自己的文学的艺术性让位于政治目的，即写作的政治目的大于美学热情。</p>\n<p>李欧梵先生从心理学角度对鲁迅走上文学道路的影响进行阐释，也很有趣。他认为鲁迅父亲病死给青年鲁迅的心理注入了某种“黑暗力量”，类似于某种心理的“诅咒”。艾利克逊也在马丁·路德金、甘地等人的生活中找到了类似的“诅咒”，即<em>“带着不惜一切代价超越和创新的责任感，并被罪恶感缠绕而成长起来。”</em>如果说鲁迅最初学医的选择是因为父亲的病未能被中医治好而背负了某种精神上的罪恶感和责任感，以及将日本的医学教育条件良好纳入考量，那在学医之后，鲁迅的思想慢慢发生了转变。鲁迅曾告诉许寿裳，他学医是为了通过科学解决三个问题——“怎样才是最理想的人性？中国国民性中缺乏的是什么？病根在哪里？”可见，彼时的鲁迅已经逐渐将文化道德置于科学之上。直到后来的“幻灯片事件”，鲁迅最终做出了弃医从文，以文学改造国民思想的决定。 </p>\n<p>作为一个文人，鲁迅是颇具知识分子的浪漫主义情怀的。这种浪漫情怀对知识分子很有吸引力。一个“精粹界之战士”在愚昧或疯狂的时代独醒独行，叛逆诗人，孤独斗士，理想主义的殉道者，多么浪漫的悲剧啊。鲁迅是决意了要做这样的人物的。《随感录三十八》是他的一篇战斗宣言：<em>“他们必定觉得自己思想见识高出庸众之上，又为庸众所不懂，所以愤世嫉俗，渐渐变成厌世家，或’国民之敌’。但一切新思想，多从他们出来，政治上宗教上道德上的改革，也从他们发端。”</em>这样的人是痛苦的，但这痛苦带来的浪漫又有麻醉效果。下面就要说说鲁迅值得我们反思之处了。</p>\n<p>毫无疑问，鲁迅的文学是尖锐辛辣的，他把文字当作了板斧，挥向腐朽的中国传统社会现实。因为他看到了现实的深刻和苍凉，所以态度也是激进的，不愿与落后的传统有任何调情。左派知识分子，多有些极端偏执，正如《狂人日记》中的”疯子“一样，自大到自负，不惮与全世界为敌。这之于历史当然有进步的一面。革命，大刀阔斧的除旧替新，慷慨激昂的警世呐喊，能在腐臭淤滞的旧社会搅起激流涌动，大大加速历史的进程。但既是反思，当然要谈谈激进之弊。尖刻凶猛的文字如大棒，固然富有力量，但同样潜藏危险。鲁迅的文字太过凶猛，以致于在鲁迅被偶像化甚至神化的时期，作为神的鲁迅被用作万能兵器，批胡风，文革批周扬，文革后批”四人帮“。鲁迅成了锋利的刀枪，被不分青红皂白地利用。 后期的鲁迅愈发激进偏执，有人曾评论：<em>“他四面树敌，攻击’新月派’与胡适等具有自由主义倾向的知识分子，把资产阶级、小布尔乔亚挂在嘴边，要打’落水狗’，‘资本家的乏走狗’等等。在一些谁拿了帝国主义的英镑，谁拿了苏俄的卢布上耿耿于怀，几近于斤斤计较的小人心理，既小气又阴暗。”</em> “个人的自大”走向极端变成唯我独尊的狂妄和偏执，使得鲁迅也没有逃不过中国传统知识分子“好为人师”的臭毛病。《两地书》是鲁迅与妻子许广平的通信，却怎么也读不出是夫妻之间的谈话，分明是老师对学生自上而下的讲授。</p>\n<p>说到这里，我还尚未提及鲁迅作品的艺术价值。这并不是说鲁迅文学的艺术价值较之其思想价值不值得一提，实在是我这位读者的艺术修养不够，没有什么资格对它的文学艺术性作出一番评价。倘若打肿脸充胖子，拿些小学阅读理解标准答案来凑数，我觉得实在没有必要。这方面，国内外众多文人学者都有精彩的评述，我做一个小学生虚心学习就好。</p>\n<p> 以上就是我与鲁迅的相遇和自己的一些鄙陋之见。</p>"},{"title":"PCA、CCA、PLS","date":"2020-06-04T18:06:00.000Z","mathjax":true,"_content":"\nPrincipal Component Analysis（主成分分析）、Canonical Correspondence Analysis（典型相关分析）、Partial Least Square（偏最小二乘）<!--more-->\n\n### 从多元线性回归说起\n\n•目的：p个自变量x 对1个因变量y的回归\n\n$$\nY=y,\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]\n$$\n\n•回归方程为：\n$$\nY=X\\beta+\\epsilon\n$$\n\n•手上有n个观测（相当于训练集），用这n个观测来估计β\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\\n\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_{11}&\nx_{12}&\n\\cdots&\nx_{1p}\\\\\nx_{21}&\nx_{22}&\n\\cdots&\nx_{2p}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nx_{n1}&\nx_{n2}&\n\\cdots&\nx_{np}\\\\\n\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]\n$$\n\n 根据最小二乘法：\n\n$$\n\\beta=(X^TX)^{-1}X^TY\n$$\n\n<font color=green>一个小问题：若Y也有多个变量时怎么办？</font>\n\n假如Y有q个变量，给Y添加一个长度为q的维度就可以了，回归公式仍然成立。\n\n•**但多元线性回归存在缺陷**：\n\n1.理论假设需要满足自变量x1, x2 ··· xp互相独立（现实中往往具有多重共线性）；\n\n2.当X的变量数非常多时，求解β时需要求$(X^TX)^{-1}$，这是一个p*p的高维方阵，求解困难；\n\n3.而且不一定每个x因子都是显著的，包含冗余信息。\n\n<br/>\n\n### PCA（主成分分析）\n\n•目的：提取变量中最能代表其特征的成分\n\n•变量X（已标准化）\n$$\nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n 有n个观测时X就是n*p的矩阵\n\n•记u是X的一个主成分，a为载荷向量\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\\right]\n$$\n\n•要求是a使得u的方差最大（其实就是最小二乘），即令\n$$\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\n$$\n\n最大\n\n「为什么要令u的方差最大呢？」下面以简单的X有两个变量的例子说明：\n\n<img src=\"/images/PCA.png\" alt=\"img\" style=\"zoom:60%;\" />\n\nVar(u)的几何意义其实是X在主成分载荷向量a上投影点到原点距离平方和，可以从两个角度来理解这个问题。\n\n令Var(u)尽量大，也就是该主成分载荷向量（PC1蓝线）可以把X的样本点分得尽量开，更能反映样本的差异（差异其实就是特征）；另外，这其实也是最小二乘思想，令Var(u)最大，就是令各样本点到PC1的距离平方和最小（想想看，各样本点到原点的距离是固定的）。\n\n•求主成分的问题就可以提炼为\n$$\nmaximize (\\textbf{a}^TC_{xx}\\textbf{a})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1\n$$\n最终转化为熟悉的求特征值和特征向量问题，保留k个主成分原来p维的X就降维到了k维。\n\n<br/>\n\n### CCA（典型相关分析）\n\n•目的：提取出因变量和自变量中最相关的成分\n\n•因变量Y和自变量X（都已标准化）\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n•记典型变量u，v，分别是X各变量和Y各变量的线性组合\n\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]\n$$\n\n•要使得u，v相关性最大，即令相关系数\n$$\nCorr(u,v)=\\frac{COV(u,v)}{s_us_v}=\\frac{\\textbf{a}^TX^TY\\textbf{b}}{\\sqrt{\\textbf{a}^TX^TX\\textbf{a}}\\sqrt{\\textbf{b}^TY^TY\\textbf{b}}}=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}\n$$\n\n 最大\n\n•求典型相关变量的问题就提炼为\n$$\nmaximize(\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}})\\\\\n约束条件：\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}=1, \\ \\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}=1\n$$\n同样最后转化为求特征值和特征向量的问题。\n\n<br/>\n\n### PLSR(偏最小二乘回归)\n\n•目的：用自变量中与因变量最相关的“典型变量”做回归\n\n•因变量Y和自变量X（都已标准化）\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n•记典型变量u，v，分别是X各变量和Y各变量的线性组合\n\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]\n$$\n\n•要求\n\n1. u,v的方差最大（PCA思想）\n\n\n$$\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\\\\\nVar(v)=\\frac{\\textbf{b}^TY^TY\\textbf{b}}{n-1}=\\textbf{b}^TC_{yy}\\textbf{b}\n$$\n\n2. 使得u,v相关性最大（CCA思想）\n\n$$\nCorr(u,v)=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}\n$$\n\n•问题提炼为\n$$\nmaximize(\\textbf{a}^TC_{xy}\\textbf{b})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1,\\ \\textbf{b}^T\\textbf{b}=1\n$$\n<font color=green>PLS是PCA和CCA的结合的说法是网上看到的（老师也这么讲）。但有个问题是：既然给定了样本，X和Y的协方差矩阵相当于就是固定的，那最后这个求解模型的约束条件不就和CCA是一样的？事实上就是求了个典型相关变量？</font>\n\n•得到了“典型变量”之后，建立Y对u1的回归，X对u1的回归\n$$\nY =\\beta_1u_1+\\epsilon_{y1} \\\\\nX =\\alpha_1u_1+\\epsilon_{x1}\n$$\n\n•用余项（$\\beta_1u_1$和$\\alpha_1u_1$解释后的残余信息）再建立对u2的回归\n$$\n\\epsilon_{y1} =\\beta_2u_2+\\epsilon_{y2} \\\\\n\\epsilon_{x1} =\\alpha_2u_2+\\epsilon_{x2}\n$$\n\n•如此重复只要达到满意的精度就可以停止，得到\n$$\nY =\\beta_1u_1+\\beta_2u_2+...\\beta_ku_k+\\epsilon \\\\\n$$\n\n•代入u的定义式\n$$\nu_1=X\\textbf{a}_1,\\ u_2=X\\textbf{a}_2,\\ ...u_k=X\\textbf{a}_k\n$$\n\n 就可以得到最终的偏最小二乘回归方程\n$$\nY =\\theta_1x_1+\\theta_2x_2+...\\theta_kx_k+\\epsilon \\\\\n$$\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[知乎 学弱猹 回归分析笔记](https://zhuanlan.zhihu.com/p/52476330)*</small>\n\n<small>*[StatQuest with Josh Starmer @YouTube](https://www.youtube.com/watch?v=FgakZw6K1QQ&list=PLGbayVYnCbodh-unkk0rDS98MdsZelTZL&index=3&t=1112s)*</small>\n\n<small>*[主成分分析的原理和简单推导 江安神犬 @Bilibili](https://www.bilibili.com/video/BV1X54y1R7g7?from=search&seid=2559086691074282442)*</small>\n\n<small>*[知乎 文琪 典型相关分析](https://zhuanlan.zhihu.com/p/52110862)*</small>\n\n<small>[JerryLead 典型相关分析](https://www.cnblogs.com/jerrylead/archive/2011/06/20/2085491.html)</small>\n\n<small>[A Simple Explanation of Partial Least Squares-Kee Siong Ng](http://users.cecs.anu.edu.au/~kee/pls.pdf)</small>\n\n<small>[JerryLead 偏最小二乘回归](https://www.cnblogs.com/jerrylead/archive/2011/08/21/2148625.html)</small>\n\n<small>[Partial least squares regression and projection on latent structure regression (PLS Regression) -Herve ́ Abdi∗](https://personal.utdallas.edu/~herve/abdi-wireCS-PLS2010.pdf)</small>\n\n","source":"_posts/PCA_CCA_PLS.md","raw":"---\ntitle: PCA、CCA、PLS\ndate: 2020-06-05 02:06:00\ncategories:\n- 统计\ntags: \n- 数学\n- 统计\n- 回归分析\nmathjax: true\n---\n\nPrincipal Component Analysis（主成分分析）、Canonical Correspondence Analysis（典型相关分析）、Partial Least Square（偏最小二乘）<!--more-->\n\n### 从多元线性回归说起\n\n•目的：p个自变量x 对1个因变量y的回归\n\n$$\nY=y,\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]\n$$\n\n•回归方程为：\n$$\nY=X\\beta+\\epsilon\n$$\n\n•手上有n个观测（相当于训练集），用这n个观测来估计β\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\\n\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_{11}&\nx_{12}&\n\\cdots&\nx_{1p}\\\\\nx_{21}&\nx_{22}&\n\\cdots&\nx_{2p}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nx_{n1}&\nx_{n2}&\n\\cdots&\nx_{np}\\\\\n\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]\n$$\n\n 根据最小二乘法：\n\n$$\n\\beta=(X^TX)^{-1}X^TY\n$$\n\n<font color=green>一个小问题：若Y也有多个变量时怎么办？</font>\n\n假如Y有q个变量，给Y添加一个长度为q的维度就可以了，回归公式仍然成立。\n\n•**但多元线性回归存在缺陷**：\n\n1.理论假设需要满足自变量x1, x2 ··· xp互相独立（现实中往往具有多重共线性）；\n\n2.当X的变量数非常多时，求解β时需要求$(X^TX)^{-1}$，这是一个p*p的高维方阵，求解困难；\n\n3.而且不一定每个x因子都是显著的，包含冗余信息。\n\n<br/>\n\n### PCA（主成分分析）\n\n•目的：提取变量中最能代表其特征的成分\n\n•变量X（已标准化）\n$$\nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n 有n个观测时X就是n*p的矩阵\n\n•记u是X的一个主成分，a为载荷向量\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\\right]\n$$\n\n•要求是a使得u的方差最大（其实就是最小二乘），即令\n$$\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\n$$\n\n最大\n\n「为什么要令u的方差最大呢？」下面以简单的X有两个变量的例子说明：\n\n<img src=\"/images/PCA.png\" alt=\"img\" style=\"zoom:60%;\" />\n\nVar(u)的几何意义其实是X在主成分载荷向量a上投影点到原点距离平方和，可以从两个角度来理解这个问题。\n\n令Var(u)尽量大，也就是该主成分载荷向量（PC1蓝线）可以把X的样本点分得尽量开，更能反映样本的差异（差异其实就是特征）；另外，这其实也是最小二乘思想，令Var(u)最大，就是令各样本点到PC1的距离平方和最小（想想看，各样本点到原点的距离是固定的）。\n\n•求主成分的问题就可以提炼为\n$$\nmaximize (\\textbf{a}^TC_{xx}\\textbf{a})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1\n$$\n最终转化为熟悉的求特征值和特征向量问题，保留k个主成分原来p维的X就降维到了k维。\n\n<br/>\n\n### CCA（典型相关分析）\n\n•目的：提取出因变量和自变量中最相关的成分\n\n•因变量Y和自变量X（都已标准化）\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n•记典型变量u，v，分别是X各变量和Y各变量的线性组合\n\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]\n$$\n\n•要使得u，v相关性最大，即令相关系数\n$$\nCorr(u,v)=\\frac{COV(u,v)}{s_us_v}=\\frac{\\textbf{a}^TX^TY\\textbf{b}}{\\sqrt{\\textbf{a}^TX^TX\\textbf{a}}\\sqrt{\\textbf{b}^TY^TY\\textbf{b}}}=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}\n$$\n\n 最大\n\n•求典型相关变量的问题就提炼为\n$$\nmaximize(\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}})\\\\\n约束条件：\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}=1, \\ \\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}=1\n$$\n同样最后转化为求特征值和特征向量的问题。\n\n<br/>\n\n### PLSR(偏最小二乘回归)\n\n•目的：用自变量中与因变量最相关的“典型变量”做回归\n\n•因变量Y和自变量X（都已标准化）\n$$\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n$$\n•记典型变量u，v，分别是X各变量和Y各变量的线性组合\n\n$$\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\ \t其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]\n$$\n\n•要求\n\n1. u,v的方差最大（PCA思想）\n\n\n$$\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\\\\\nVar(v)=\\frac{\\textbf{b}^TY^TY\\textbf{b}}{n-1}=\\textbf{b}^TC_{yy}\\textbf{b}\n$$\n\n2. 使得u,v相关性最大（CCA思想）\n\n$$\nCorr(u,v)=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}\n$$\n\n•问题提炼为\n$$\nmaximize(\\textbf{a}^TC_{xy}\\textbf{b})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1,\\ \\textbf{b}^T\\textbf{b}=1\n$$\n<font color=green>PLS是PCA和CCA的结合的说法是网上看到的（老师也这么讲）。但有个问题是：既然给定了样本，X和Y的协方差矩阵相当于就是固定的，那最后这个求解模型的约束条件不就和CCA是一样的？事实上就是求了个典型相关变量？</font>\n\n•得到了“典型变量”之后，建立Y对u1的回归，X对u1的回归\n$$\nY =\\beta_1u_1+\\epsilon_{y1} \\\\\nX =\\alpha_1u_1+\\epsilon_{x1}\n$$\n\n•用余项（$\\beta_1u_1$和$\\alpha_1u_1$解释后的残余信息）再建立对u2的回归\n$$\n\\epsilon_{y1} =\\beta_2u_2+\\epsilon_{y2} \\\\\n\\epsilon_{x1} =\\alpha_2u_2+\\epsilon_{x2}\n$$\n\n•如此重复只要达到满意的精度就可以停止，得到\n$$\nY =\\beta_1u_1+\\beta_2u_2+...\\beta_ku_k+\\epsilon \\\\\n$$\n\n•代入u的定义式\n$$\nu_1=X\\textbf{a}_1,\\ u_2=X\\textbf{a}_2,\\ ...u_k=X\\textbf{a}_k\n$$\n\n 就可以得到最终的偏最小二乘回归方程\n$$\nY =\\theta_1x_1+\\theta_2x_2+...\\theta_kx_k+\\epsilon \\\\\n$$\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[知乎 学弱猹 回归分析笔记](https://zhuanlan.zhihu.com/p/52476330)*</small>\n\n<small>*[StatQuest with Josh Starmer @YouTube](https://www.youtube.com/watch?v=FgakZw6K1QQ&list=PLGbayVYnCbodh-unkk0rDS98MdsZelTZL&index=3&t=1112s)*</small>\n\n<small>*[主成分分析的原理和简单推导 江安神犬 @Bilibili](https://www.bilibili.com/video/BV1X54y1R7g7?from=search&seid=2559086691074282442)*</small>\n\n<small>*[知乎 文琪 典型相关分析](https://zhuanlan.zhihu.com/p/52110862)*</small>\n\n<small>[JerryLead 典型相关分析](https://www.cnblogs.com/jerrylead/archive/2011/06/20/2085491.html)</small>\n\n<small>[A Simple Explanation of Partial Least Squares-Kee Siong Ng](http://users.cecs.anu.edu.au/~kee/pls.pdf)</small>\n\n<small>[JerryLead 偏最小二乘回归](https://www.cnblogs.com/jerrylead/archive/2011/08/21/2148625.html)</small>\n\n<small>[Partial least squares regression and projection on latent structure regression (PLS Regression) -Herve ́ Abdi∗](https://personal.utdallas.edu/~herve/abdi-wireCS-PLS2010.pdf)</small>\n\n","slug":"PCA_CCA_PLS","published":1,"updated":"2020-11-05T08:07:45.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6500112sfy7yaxbh60","content":"<p>Principal Component Analysis（主成分分析）、Canonical Correspondence Analysis（典型相关分析）、Partial Least Square（偏最小二乘）<a id=\"more\"></a></p>\n<h3 id=\"从多元线性回归说起\"><a href=\"#从多元线性回归说起\" class=\"headerlink\" title=\"从多元线性回归说起\"></a>从多元线性回归说起</h3><p>•目的：p个自变量x 对1个因变量y的回归</p>\n<script type=\"math/tex; mode=display\">\nY=y,\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]</script><p>•回归方程为：</p>\n<script type=\"math/tex; mode=display\">\nY=X\\beta+\\epsilon</script><p>•手上有n个观测（相当于训练集），用这n个观测来估计β</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\\n\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_{11}&\nx_{12}&\n\\cdots&\nx_{1p}\\\\\nx_{21}&\nx_{22}&\n\\cdots&\nx_{2p}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nx_{n1}&\nx_{n2}&\n\\cdots&\nx_{np}\\\\\n\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]</script><p> 根据最小二乘法：</p>\n<script type=\"math/tex; mode=display\">\n\\beta=(X^TX)^{-1}X^TY</script><font color=green>一个小问题：若Y也有多个变量时怎么办？</font>\n\n<p>假如Y有q个变量，给Y添加一个长度为q的维度就可以了，回归公式仍然成立。</p>\n<p>•<strong>但多元线性回归存在缺陷</strong>：</p>\n<p>1.理论假设需要满足自变量x1, x2 ··· xp互相独立（现实中往往具有多重共线性）；</p>\n<p>2.当X的变量数非常多时，求解β时需要求$(X^TX)^{-1}$，这是一个p*p的高维方阵，求解困难；</p>\n<p>3.而且不一定每个x因子都是显著的，包含冗余信息。</p>\n<p><br/></p>\n<h3 id=\"PCA（主成分分析）\"><a href=\"#PCA（主成分分析）\" class=\"headerlink\" title=\"PCA（主成分分析）\"></a>PCA（主成分分析）</h3><p>•目的：提取变量中最能代表其特征的成分</p>\n<p>•变量X（已标准化）</p>\n<script type=\"math/tex; mode=display\">\nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p> 有n个观测时X就是n*p的矩阵</p>\n<p>•记u是X的一个主成分，a为载荷向量</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\\right]</script><p>•要求是a使得u的方差最大（其实就是最小二乘），即令</p>\n<script type=\"math/tex; mode=display\">\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}</script><p>最大</p>\n<p>「为什么要令u的方差最大呢？」下面以简单的X有两个变量的例子说明：</p>\n<p><img src=\"/images/PCA.png\" alt=\"img\" style=\"zoom:60%;\" /></p>\n<p>Var(u)的几何意义其实是X在主成分载荷向量a上投影点到原点距离平方和，可以从两个角度来理解这个问题。</p>\n<p>令Var(u)尽量大，也就是该主成分载荷向量（PC1蓝线）可以把X的样本点分得尽量开，更能反映样本的差异（差异其实就是特征）；另外，这其实也是最小二乘思想，令Var(u)最大，就是令各样本点到PC1的距离平方和最小（想想看，各样本点到原点的距离是固定的）。</p>\n<p>•求主成分的问题就可以提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize (\\textbf{a}^TC_{xx}\\textbf{a})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1</script><p>最终转化为熟悉的求特征值和特征向量问题，保留k个主成分原来p维的X就降维到了k维。</p>\n<p><br/></p>\n<h3 id=\"CCA（典型相关分析）\"><a href=\"#CCA（典型相关分析）\" class=\"headerlink\" title=\"CCA（典型相关分析）\"></a>CCA（典型相关分析）</h3><p>•目的：提取出因变量和自变量中最相关的成分</p>\n<p>•因变量Y和自变量X（都已标准化）</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p>•记典型变量u，v，分别是X各变量和Y各变量的线性组合</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]</script><p>•要使得u，v相关性最大，即令相关系数</p>\n<script type=\"math/tex; mode=display\">\nCorr(u,v)=\\frac{COV(u,v)}{s_us_v}=\\frac{\\textbf{a}^TX^TY\\textbf{b}}{\\sqrt{\\textbf{a}^TX^TX\\textbf{a}}\\sqrt{\\textbf{b}^TY^TY\\textbf{b}}}=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}</script><p> 最大</p>\n<p>•求典型相关变量的问题就提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize(\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}})\\\\\n约束条件：\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}=1, \\ \\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}=1</script><p>同样最后转化为求特征值和特征向量的问题。</p>\n<p><br/></p>\n<h3 id=\"PLSR-偏最小二乘回归\"><a href=\"#PLSR-偏最小二乘回归\" class=\"headerlink\" title=\"PLSR(偏最小二乘回归)\"></a>PLSR(偏最小二乘回归)</h3><p>•目的：用自变量中与因变量最相关的“典型变量”做回归</p>\n<p>•因变量Y和自变量X（都已标准化）</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p>•记典型变量u，v，分别是X各变量和Y各变量的线性组合</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]</script><p>•要求</p>\n<ol>\n<li>u,v的方差最大（PCA思想）</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\\\\\nVar(v)=\\frac{\\textbf{b}^TY^TY\\textbf{b}}{n-1}=\\textbf{b}^TC_{yy}\\textbf{b}</script><ol>\n<li>使得u,v相关性最大（CCA思想）</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nCorr(u,v)=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}</script><p>•问题提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize(\\textbf{a}^TC_{xy}\\textbf{b})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1,\\ \\textbf{b}^T\\textbf{b}=1</script><font color=green>PLS是PCA和CCA的结合的说法是网上看到的（老师也这么讲）。但有个问题是：既然给定了样本，X和Y的协方差矩阵相当于就是固定的，那最后这个求解模型的约束条件不就和CCA是一样的？事实上就是求了个典型相关变量？</font>\n\n<p>•得到了“典型变量”之后，建立Y对u1的回归，X对u1的回归</p>\n<script type=\"math/tex; mode=display\">\nY =\\beta_1u_1+\\epsilon_{y1} \\\\\nX =\\alpha_1u_1+\\epsilon_{x1}</script><p>•用余项（$\\beta_1u_1$和$\\alpha_1u_1$解释后的残余信息）再建立对u2的回归</p>\n<script type=\"math/tex; mode=display\">\n\\epsilon_{y1} =\\beta_2u_2+\\epsilon_{y2} \\\\\n\\epsilon_{x1} =\\alpha_2u_2+\\epsilon_{x2}</script><p>•如此重复只要达到满意的精度就可以停止，得到</p>\n<script type=\"math/tex; mode=display\">\nY =\\beta_1u_1+\\beta_2u_2+...\\beta_ku_k+\\epsilon \\\\</script><p>•代入u的定义式</p>\n<script type=\"math/tex; mode=display\">\nu_1=X\\textbf{a}_1,\\ u_2=X\\textbf{a}_2,\\ ...u_k=X\\textbf{a}_k</script><p> 就可以得到最终的偏最小二乘回归方程</p>\n<script type=\"math/tex; mode=display\">\nY =\\theta_1x_1+\\theta_2x_2+...\\theta_kx_k+\\epsilon \\\\</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://zhuanlan.zhihu.com/p/52476330\" target=\"_blank\" rel=\"noopener\">知乎 学弱猹 回归分析笔记</a></em></small></p>\n<p><small><em><a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ&amp;list=PLGbayVYnCbodh-unkk0rDS98MdsZelTZL&amp;index=3&amp;t=1112s\" target=\"_blank\" rel=\"noopener\">StatQuest with Josh Starmer @YouTube</a></em></small></p>\n<p><small><em><a href=\"https://www.bilibili.com/video/BV1X54y1R7g7?from=search&amp;seid=2559086691074282442\" target=\"_blank\" rel=\"noopener\">主成分分析的原理和简单推导 江安神犬 @Bilibili</a></em></small></p>\n<p><small><em><a href=\"https://zhuanlan.zhihu.com/p/52110862\" target=\"_blank\" rel=\"noopener\">知乎 文琪 典型相关分析</a></em></small></p>\n<p><small><a href=\"https://www.cnblogs.com/jerrylead/archive/2011/06/20/2085491.html\" target=\"_blank\" rel=\"noopener\">JerryLead 典型相关分析</a></small></p>\n<p><small><a href=\"http://users.cecs.anu.edu.au/~kee/pls.pdf\" target=\"_blank\" rel=\"noopener\">A Simple Explanation of Partial Least Squares-Kee Siong Ng</a></small></p>\n<p><small><a href=\"https://www.cnblogs.com/jerrylead/archive/2011/08/21/2148625.html\" target=\"_blank\" rel=\"noopener\">JerryLead 偏最小二乘回归</a></small></p>\n<p><small><a href=\"https://personal.utdallas.edu/~herve/abdi-wireCS-PLS2010.pdf\" target=\"_blank\" rel=\"noopener\">Partial least squares regression and projection on latent structure regression (PLS Regression) -Herve ́ Abdi∗</a></small></p>\n","site":{"data":{}},"excerpt":"<p>Principal Component Analysis（主成分分析）、Canonical Correspondence Analysis（典型相关分析）、Partial Least Square（偏最小二乘）","more":"</p>\n<h3 id=\"从多元线性回归说起\"><a href=\"#从多元线性回归说起\" class=\"headerlink\" title=\"从多元线性回归说起\"></a>从多元线性回归说起</h3><p>•目的：p个自变量x 对1个因变量y的回归</p>\n<script type=\"math/tex; mode=display\">\nY=y,\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]</script><p>•回归方程为：</p>\n<script type=\"math/tex; mode=display\">\nY=X\\beta+\\epsilon</script><p>•手上有n个观测（相当于训练集），用这n个观测来估计β</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\\n\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_{11}&\nx_{12}&\n\\cdots&\nx_{1p}\\\\\nx_{21}&\nx_{22}&\n\\cdots&\nx_{2p}\\\\\n\\vdots &\\vdots& \\ddots & \\vdots\\\\\nx_{n1}&\nx_{n2}&\n\\cdots&\nx_{np}\\\\\n\n\\end{matrix}\n\\right]\n,\\ \\ \\ \\ \\ \\ \\beta=\\left[\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_p\n\\end{matrix}\n\\right]</script><p> 根据最小二乘法：</p>\n<script type=\"math/tex; mode=display\">\n\\beta=(X^TX)^{-1}X^TY</script><font color=green>一个小问题：若Y也有多个变量时怎么办？</font>\n\n<p>假如Y有q个变量，给Y添加一个长度为q的维度就可以了，回归公式仍然成立。</p>\n<p>•<strong>但多元线性回归存在缺陷</strong>：</p>\n<p>1.理论假设需要满足自变量x1, x2 ··· xp互相独立（现实中往往具有多重共线性）；</p>\n<p>2.当X的变量数非常多时，求解β时需要求$(X^TX)^{-1}$，这是一个p*p的高维方阵，求解困难；</p>\n<p>3.而且不一定每个x因子都是显著的，包含冗余信息。</p>\n<p><br/></p>\n<h3 id=\"PCA（主成分分析）\"><a href=\"#PCA（主成分分析）\" class=\"headerlink\" title=\"PCA（主成分分析）\"></a>PCA（主成分分析）</h3><p>•目的：提取变量中最能代表其特征的成分</p>\n<p>•变量X（已标准化）</p>\n<script type=\"math/tex; mode=display\">\nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p> 有n个观测时X就是n*p的矩阵</p>\n<p>•记u是X的一个主成分，a为载荷向量</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\\right]</script><p>•要求是a使得u的方差最大（其实就是最小二乘），即令</p>\n<script type=\"math/tex; mode=display\">\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}</script><p>最大</p>\n<p>「为什么要令u的方差最大呢？」下面以简单的X有两个变量的例子说明：</p>\n<p><img src=\"/images/PCA.png\" alt=\"img\" style=\"zoom:60%;\" /></p>\n<p>Var(u)的几何意义其实是X在主成分载荷向量a上投影点到原点距离平方和，可以从两个角度来理解这个问题。</p>\n<p>令Var(u)尽量大，也就是该主成分载荷向量（PC1蓝线）可以把X的样本点分得尽量开，更能反映样本的差异（差异其实就是特征）；另外，这其实也是最小二乘思想，令Var(u)最大，就是令各样本点到PC1的距离平方和最小（想想看，各样本点到原点的距离是固定的）。</p>\n<p>•求主成分的问题就可以提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize (\\textbf{a}^TC_{xx}\\textbf{a})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1</script><p>最终转化为熟悉的求特征值和特征向量问题，保留k个主成分原来p维的X就降维到了k维。</p>\n<p><br/></p>\n<h3 id=\"CCA（典型相关分析）\"><a href=\"#CCA（典型相关分析）\" class=\"headerlink\" title=\"CCA（典型相关分析）\"></a>CCA（典型相关分析）</h3><p>•目的：提取出因变量和自变量中最相关的成分</p>\n<p>•因变量Y和自变量X（都已标准化）</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p>•记典型变量u，v，分别是X各变量和Y各变量的线性组合</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]</script><p>•要使得u，v相关性最大，即令相关系数</p>\n<script type=\"math/tex; mode=display\">\nCorr(u,v)=\\frac{COV(u,v)}{s_us_v}=\\frac{\\textbf{a}^TX^TY\\textbf{b}}{\\sqrt{\\textbf{a}^TX^TX\\textbf{a}}\\sqrt{\\textbf{b}^TY^TY\\textbf{b}}}=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}</script><p> 最大</p>\n<p>•求典型相关变量的问题就提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize(\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}})\\\\\n约束条件：\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}=1, \\ \\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}=1</script><p>同样最后转化为求特征值和特征向量的问题。</p>\n<p><br/></p>\n<h3 id=\"PLSR-偏最小二乘回归\"><a href=\"#PLSR-偏最小二乘回归\" class=\"headerlink\" title=\"PLSR(偏最小二乘回归)\"></a>PLSR(偏最小二乘回归)</h3><p>•目的：用自变量中与因变量最相关的“典型变量”做回归</p>\n<p>•因变量Y和自变量X（都已标准化）</p>\n<script type=\"math/tex; mode=display\">\nY=\\left[\n\\begin{matrix}\ny_1\\\ny_2\\\n\\cdots\\\ny_q\n\\end{matrix}\n\\right],\\ \\ \\ \\ \\ \\ \nX=\\left[\n\\begin{matrix}\nx_1\\\nx_2\\\n\\cdots\\\nx_p\n\\end{matrix}\n\\right]</script><p>•记典型变量u，v，分别是X各变量和Y各变量的线性组合</p>\n<script type=\"math/tex; mode=display\">\nu=X\\textbf{a},\\ \\ \\ \\ \\ v =Y\\textbf{b},  \\ \\ \\ \\ \\     其中\\ \n\\textbf{a}=\\left[\n\\begin{matrix}\na_1\\\\\na_2\\\\\n\\vdots\\\\\na_p\n\\end{matrix}\n\n\\right]\n,\\ \\ \\ \\ \\  \n\\textbf{b}=\\left[\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_q\n\\end{matrix}\n\\right]</script><p>•要求</p>\n<ol>\n<li>u,v的方差最大（PCA思想）</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nVar(u)=\\frac{\\textbf{a}^TX^TX\\textbf{a}}{n-1}=\\textbf{a}^TC_{xx}\\textbf{a}\\\\\nVar(v)=\\frac{\\textbf{b}^TY^TY\\textbf{b}}{n-1}=\\textbf{b}^TC_{yy}\\textbf{b}</script><ol>\n<li>使得u,v相关性最大（CCA思想）</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nCorr(u,v)=\\frac{\\textbf{a}^TC_{xy}\\textbf{b}}{\\sqrt{\\textbf{a}^TC_{xx}\\textbf{a}}\\sqrt{\\textbf{b}^TC_{yy}\\textbf{b}}}</script><p>•问题提炼为</p>\n<script type=\"math/tex; mode=display\">\nmaximize(\\textbf{a}^TC_{xy}\\textbf{b})\\\\\n约束条件：\\textbf{a}^T\\textbf{a}=1,\\ \\textbf{b}^T\\textbf{b}=1</script><font color=green>PLS是PCA和CCA的结合的说法是网上看到的（老师也这么讲）。但有个问题是：既然给定了样本，X和Y的协方差矩阵相当于就是固定的，那最后这个求解模型的约束条件不就和CCA是一样的？事实上就是求了个典型相关变量？</font>\n\n<p>•得到了“典型变量”之后，建立Y对u1的回归，X对u1的回归</p>\n<script type=\"math/tex; mode=display\">\nY =\\beta_1u_1+\\epsilon_{y1} \\\\\nX =\\alpha_1u_1+\\epsilon_{x1}</script><p>•用余项（$\\beta_1u_1$和$\\alpha_1u_1$解释后的残余信息）再建立对u2的回归</p>\n<script type=\"math/tex; mode=display\">\n\\epsilon_{y1} =\\beta_2u_2+\\epsilon_{y2} \\\\\n\\epsilon_{x1} =\\alpha_2u_2+\\epsilon_{x2}</script><p>•如此重复只要达到满意的精度就可以停止，得到</p>\n<script type=\"math/tex; mode=display\">\nY =\\beta_1u_1+\\beta_2u_2+...\\beta_ku_k+\\epsilon \\\\</script><p>•代入u的定义式</p>\n<script type=\"math/tex; mode=display\">\nu_1=X\\textbf{a}_1,\\ u_2=X\\textbf{a}_2,\\ ...u_k=X\\textbf{a}_k</script><p> 就可以得到最终的偏最小二乘回归方程</p>\n<script type=\"math/tex; mode=display\">\nY =\\theta_1x_1+\\theta_2x_2+...\\theta_kx_k+\\epsilon \\\\</script><p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://zhuanlan.zhihu.com/p/52476330\" target=\"_blank\" rel=\"noopener\">知乎 学弱猹 回归分析笔记</a></em></small></p>\n<p><small><em><a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ&amp;list=PLGbayVYnCbodh-unkk0rDS98MdsZelTZL&amp;index=3&amp;t=1112s\" target=\"_blank\" rel=\"noopener\">StatQuest with Josh Starmer @YouTube</a></em></small></p>\n<p><small><em><a href=\"https://www.bilibili.com/video/BV1X54y1R7g7?from=search&amp;seid=2559086691074282442\" target=\"_blank\" rel=\"noopener\">主成分分析的原理和简单推导 江安神犬 @Bilibili</a></em></small></p>\n<p><small><em><a href=\"https://zhuanlan.zhihu.com/p/52110862\" target=\"_blank\" rel=\"noopener\">知乎 文琪 典型相关分析</a></em></small></p>\n<p><small><a href=\"https://www.cnblogs.com/jerrylead/archive/2011/06/20/2085491.html\" target=\"_blank\" rel=\"noopener\">JerryLead 典型相关分析</a></small></p>\n<p><small><a href=\"http://users.cecs.anu.edu.au/~kee/pls.pdf\" target=\"_blank\" rel=\"noopener\">A Simple Explanation of Partial Least Squares-Kee Siong Ng</a></small></p>\n<p><small><a href=\"https://www.cnblogs.com/jerrylead/archive/2011/08/21/2148625.html\" target=\"_blank\" rel=\"noopener\">JerryLead 偏最小二乘回归</a></small></p>\n<p><small><a href=\"https://personal.utdallas.edu/~herve/abdi-wireCS-PLS2010.pdf\" target=\"_blank\" rel=\"noopener\">Partial least squares regression and projection on latent structure regression (PLS Regression) -Herve ́ Abdi∗</a></small></p>"},{"title":"集合卡尔曼滤波中的采样误差","date":"2022-01-18T20:27:00.000Z","mathjax":true,"_content":"\n一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。<!--more-->\n\n#### 概率预报与集合卡尔曼滤波\n\n自1969年Edward Epstein发表《Stochastic Dynamic Prediction》以来，概率预报已经深入人心，改变了人们对气象预报的看法。Epstein认为，即使大气的动力机制本身是确定性的，由于观测有限，同时模式也非完美，我们永远无法确切知道所谓的“真值”。所以对大气的预报应该是概率的，而非确定性的。如何将概率引入气象预报呢？相较于过去只做一个预报，我们可以从不同的初始条件出发，或使用不同的模式，同时进行多组预报，每个预报成员都可以看作是系统概率分布的一个随机采样，这就是集合预报。\n\n集合卡尔曼滤波（EnKF）是基于集合预报的同化方法。它可以结合模式和观测的信息，得到更准确的后验初始场。在集合卡尔曼滤波中，对模式不确定性的估计是至关重要的一步，模式不确定性量化表示为一个误差协方差矩阵$\\mathrm {P}^b$，由预报集合估计。但在实践中我们会发现，对$\\mathrm {P}^b$的估计常常受集合数量的影响。例如，将协方差矩阵化为相关系数矩阵，用较少集合估计出的$\\mathrm {P}^b$会在某些位置出现虚假相关，这些虚假相关在使用足够多集合去估计$\\mathrm {P}^b$时是不存在的（图1）。\n\n<img src=\"/images/image-20220119004115826.png\" alt=\"image-20220119004115826\" style=\"zoom:80%;\" />\n\n<center><small>图1. (Hamill 2004)</small></center>\n\n这些虚假相关，其实是由于采样样本数量不足导致的误差，称为采样误差（Sampling Error）。\n\n#### 采样误差\n\n采样误差其实在统计推断中无处不在。当我们想要估计总体（Population）分布的某个参数，通常并不会，或者不能统计到总体的每一个个体，而只会取总体的一部分，作为我们的样本（Sample），从样本来估计总体的参数。比如我们想要知道总体分布的期望大概是多少，就会用样本的均值作为对期望的一个估计。\n\n<img src=\"/images/image-20220119014952338.png\" alt=\"image-20220119014952338\" style=\"zoom:45%;\" />\n\n假如我们现在要估计一个服从正态分布$X \\sim \\mathcal{N}(0.5,\\sigma ^2) $总体的均值，总体容量为10亿个（若总体容量可为无穷多，总体的均值就严格等于总体分布的期望）。从总体里随机抽取100个样本，以这100个样本的均值作为对总体均值的估计。算出来这100个样本的均值，几乎一定不会等于0.5，总是偏大或偏小一点。而且，重复多次采样，每次估计到的均值也大概率不会相同。多次采样估计出的均值，也会构成一个分布\n\n<img src=\"/images/image-20220119022046007.png\" alt=\"image-20220119022046007\" style=\"zoom:25%;\" />\n\n<center><small>图2. Margin of error (Wikipedia)</small></center>\n\n每次估计得到的均值与真实的期望的差距就是**采样误差**。抽取2000个样本和抽取100个样本去估计均值，其采样误差的范围是不同的。2000个样本估计得到的均值分布高高瘦瘦，100个样本得到的均值分布相比起来就矮胖一些。显然，我们更希望均值像2000个样本估计的那样，更接近真实的期望，同时又有更小的方差。有一个指标来量化采样误差的大小范围，叫误差限（Margin of Error）\n\n<img src=\"/images/image-20220119024539984.png\" alt=\"image-20220119024539984\" style=\"zoom:22%;\" />\n\n它与总体分布本身的方差有关，同时也与样本数量n有关。容易看出，样本数量越大，误差限就越小，表示采样误差就越小。\n\n下面举一个稍复杂点的例子，这次不再是估计单变量的分布特征，而是估计多变量分布的相关特征。假如有两个变量$X_1,X_2$都服从正态分布$X_1,X_2 \\sim \\mathcal{N}(0,1) $，它们的协方差为$c_{12}$，可用一个回归方程描述二者的关系\n\n<img src=\"/images/image-20220119025638519.png\" alt=\"image-20220119025638519\" style=\"zoom:45%;\" />\n\n其中\n\n<img src=\"/images/image-20220119025731907.png\" alt=\"image-20220119025731907\" style=\"zoom:45%;\" />\n\n现在要用采样的方式估计回归系数$\\beta$，我们看一下使用不同样本数量估计得到的结果\n\n<img src=\"/images/image-20220119025848148.png\" alt=\"image-20220119025848148\" style=\"zoom:80%;\" />\n\n<center><small>图3. 不同样本数量（esize）估计得到的回归系数概率密度分布。第一行beta真值为0.01，第二行beta真值为0.1，第三行beta真值为0.4。黑色竖线是beta真值，红色竖线是估计得到的beta的均值，绿色横线是估计的beta的方差，红色曲线是拟合的高斯分布。</small></center>\n\n同样的，样本数量越多，估计量分布就越集中，越接近真实值。\n\n换成估计相关系数试试。\n\n<img src=\"/images/image-20220119031216615.png\" alt=\"image-20220119031216615\" style=\"zoom:80%;\" />\n\n<center><small>图4. 不同样本数量（esize）估计得到的相关系数概率密度分布。其余同图3。</small></center>\n\n这次稍有不同，相关系数是有边界的量（[-1,1]），所以估计量不再满足正态分布，但同样能看出样本数量越多，估计量分布越集中且越靠近真值。所以结论是一致的：样本数量越大，采样误差就越小。\n\n#### 消减虚假相关：局地化方法\n\n既然增大样本数量就能减小采样误差，那何不用更多的预报集合呢。的确，在钱不是问题的情况下，采样误差也不成问题。但现实是，集合预报需要耗费大量的计算资源，无限度地增加集合数量目前尚不可行。那么问题就变成，如何在有限的条件下，更大限度地提取高质量的信息。具体到EnKF，如何用100个集合成员就达到或逼近2000个集合成员估计的准确度呢。\n\nHamill在2001年的文章中指出，两变量间的相关系数越小，使用的集合数量越少，对协方差估计的相对误差就越大\n\n<img src=\"/images/image-20220119034108763.png\" alt=\"image-20220119034108763\" style=\"zoom:25%;\" />\n\n<center><small>图5. （Hamill 2001）</small></center>\n\n其中协方差$c_{12}$采样误差的方差为\n\n<img src=\"/images/image-20220119034518723.png\" alt=\"image-20220119034518723\" style=\"zoom:26%;\" />\n\n所以在本应接近无相关的位置出现虚假相关，其影响并非是无足轻重的。那么，如何消除虚假相关，用更少的集合得到更准确的估计效果呢。\n\n前面提到，我们的问题是在如何在100个集合的限制条件下，提取更多更好的信息。其实还有另一种思路，如果我们无法提取更多的信息，是否可以人为地加入一些先验信息？比如，我们相信随着距离增大，两变量间的相关应该是减小的，那么能不能定义一个随距离减小的函数，作用在协方差矩阵$\\mathrm {P}^b$上呢？事实上的确能这么做，Gaspari and Cohn函数就是这么一个函数。它由一个单一可调参数确定形状，作用在协方差矩阵或卡尔曼增益上以消除虚假相关。这种类似的方法在数据同化中都称为局地化方法。所以，一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。\n\n<br/><br/><br/><small>*参考*</small>\n\n<small>Gaspari, G., and S. E. Cohn, 1999: Construction of correlation functions in two and three dimensions. *Quart. J. Roy. Meteor. Soc.*, **125**, 723–757. </small>\n\n<small>Hamill, Thomas M., Jeffrey S. Whitaker, and Chris Snyder. \" Distance-Dependent Filtering of Background Error Covariance Estimates in an Ensemble Kalman Filter\". *Monthly Weather Review* 129.11 (2001): 2776-2790.</small>\n\n<small>Hamill, Thomas. (2004). Ensemble-based atmospheric data assimilation. Predictability of Weather and Climate. 10.1017/CBO9780511617652.007. </small>\n","source":"_posts/Sampling Error.md","raw":"---\ntitle: 集合卡尔曼滤波中的采样误差\ndate: 2022-01-19 04:27:00\ncategories:\n- 统计\ntags: \n- 数学\n- 统计\nmathjax: true\n\n\n---\n\n一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。<!--more-->\n\n#### 概率预报与集合卡尔曼滤波\n\n自1969年Edward Epstein发表《Stochastic Dynamic Prediction》以来，概率预报已经深入人心，改变了人们对气象预报的看法。Epstein认为，即使大气的动力机制本身是确定性的，由于观测有限，同时模式也非完美，我们永远无法确切知道所谓的“真值”。所以对大气的预报应该是概率的，而非确定性的。如何将概率引入气象预报呢？相较于过去只做一个预报，我们可以从不同的初始条件出发，或使用不同的模式，同时进行多组预报，每个预报成员都可以看作是系统概率分布的一个随机采样，这就是集合预报。\n\n集合卡尔曼滤波（EnKF）是基于集合预报的同化方法。它可以结合模式和观测的信息，得到更准确的后验初始场。在集合卡尔曼滤波中，对模式不确定性的估计是至关重要的一步，模式不确定性量化表示为一个误差协方差矩阵$\\mathrm {P}^b$，由预报集合估计。但在实践中我们会发现，对$\\mathrm {P}^b$的估计常常受集合数量的影响。例如，将协方差矩阵化为相关系数矩阵，用较少集合估计出的$\\mathrm {P}^b$会在某些位置出现虚假相关，这些虚假相关在使用足够多集合去估计$\\mathrm {P}^b$时是不存在的（图1）。\n\n<img src=\"/images/image-20220119004115826.png\" alt=\"image-20220119004115826\" style=\"zoom:80%;\" />\n\n<center><small>图1. (Hamill 2004)</small></center>\n\n这些虚假相关，其实是由于采样样本数量不足导致的误差，称为采样误差（Sampling Error）。\n\n#### 采样误差\n\n采样误差其实在统计推断中无处不在。当我们想要估计总体（Population）分布的某个参数，通常并不会，或者不能统计到总体的每一个个体，而只会取总体的一部分，作为我们的样本（Sample），从样本来估计总体的参数。比如我们想要知道总体分布的期望大概是多少，就会用样本的均值作为对期望的一个估计。\n\n<img src=\"/images/image-20220119014952338.png\" alt=\"image-20220119014952338\" style=\"zoom:45%;\" />\n\n假如我们现在要估计一个服从正态分布$X \\sim \\mathcal{N}(0.5,\\sigma ^2) $总体的均值，总体容量为10亿个（若总体容量可为无穷多，总体的均值就严格等于总体分布的期望）。从总体里随机抽取100个样本，以这100个样本的均值作为对总体均值的估计。算出来这100个样本的均值，几乎一定不会等于0.5，总是偏大或偏小一点。而且，重复多次采样，每次估计到的均值也大概率不会相同。多次采样估计出的均值，也会构成一个分布\n\n<img src=\"/images/image-20220119022046007.png\" alt=\"image-20220119022046007\" style=\"zoom:25%;\" />\n\n<center><small>图2. Margin of error (Wikipedia)</small></center>\n\n每次估计得到的均值与真实的期望的差距就是**采样误差**。抽取2000个样本和抽取100个样本去估计均值，其采样误差的范围是不同的。2000个样本估计得到的均值分布高高瘦瘦，100个样本得到的均值分布相比起来就矮胖一些。显然，我们更希望均值像2000个样本估计的那样，更接近真实的期望，同时又有更小的方差。有一个指标来量化采样误差的大小范围，叫误差限（Margin of Error）\n\n<img src=\"/images/image-20220119024539984.png\" alt=\"image-20220119024539984\" style=\"zoom:22%;\" />\n\n它与总体分布本身的方差有关，同时也与样本数量n有关。容易看出，样本数量越大，误差限就越小，表示采样误差就越小。\n\n下面举一个稍复杂点的例子，这次不再是估计单变量的分布特征，而是估计多变量分布的相关特征。假如有两个变量$X_1,X_2$都服从正态分布$X_1,X_2 \\sim \\mathcal{N}(0,1) $，它们的协方差为$c_{12}$，可用一个回归方程描述二者的关系\n\n<img src=\"/images/image-20220119025638519.png\" alt=\"image-20220119025638519\" style=\"zoom:45%;\" />\n\n其中\n\n<img src=\"/images/image-20220119025731907.png\" alt=\"image-20220119025731907\" style=\"zoom:45%;\" />\n\n现在要用采样的方式估计回归系数$\\beta$，我们看一下使用不同样本数量估计得到的结果\n\n<img src=\"/images/image-20220119025848148.png\" alt=\"image-20220119025848148\" style=\"zoom:80%;\" />\n\n<center><small>图3. 不同样本数量（esize）估计得到的回归系数概率密度分布。第一行beta真值为0.01，第二行beta真值为0.1，第三行beta真值为0.4。黑色竖线是beta真值，红色竖线是估计得到的beta的均值，绿色横线是估计的beta的方差，红色曲线是拟合的高斯分布。</small></center>\n\n同样的，样本数量越多，估计量分布就越集中，越接近真实值。\n\n换成估计相关系数试试。\n\n<img src=\"/images/image-20220119031216615.png\" alt=\"image-20220119031216615\" style=\"zoom:80%;\" />\n\n<center><small>图4. 不同样本数量（esize）估计得到的相关系数概率密度分布。其余同图3。</small></center>\n\n这次稍有不同，相关系数是有边界的量（[-1,1]），所以估计量不再满足正态分布，但同样能看出样本数量越多，估计量分布越集中且越靠近真值。所以结论是一致的：样本数量越大，采样误差就越小。\n\n#### 消减虚假相关：局地化方法\n\n既然增大样本数量就能减小采样误差，那何不用更多的预报集合呢。的确，在钱不是问题的情况下，采样误差也不成问题。但现实是，集合预报需要耗费大量的计算资源，无限度地增加集合数量目前尚不可行。那么问题就变成，如何在有限的条件下，更大限度地提取高质量的信息。具体到EnKF，如何用100个集合成员就达到或逼近2000个集合成员估计的准确度呢。\n\nHamill在2001年的文章中指出，两变量间的相关系数越小，使用的集合数量越少，对协方差估计的相对误差就越大\n\n<img src=\"/images/image-20220119034108763.png\" alt=\"image-20220119034108763\" style=\"zoom:25%;\" />\n\n<center><small>图5. （Hamill 2001）</small></center>\n\n其中协方差$c_{12}$采样误差的方差为\n\n<img src=\"/images/image-20220119034518723.png\" alt=\"image-20220119034518723\" style=\"zoom:26%;\" />\n\n所以在本应接近无相关的位置出现虚假相关，其影响并非是无足轻重的。那么，如何消除虚假相关，用更少的集合得到更准确的估计效果呢。\n\n前面提到，我们的问题是在如何在100个集合的限制条件下，提取更多更好的信息。其实还有另一种思路，如果我们无法提取更多的信息，是否可以人为地加入一些先验信息？比如，我们相信随着距离增大，两变量间的相关应该是减小的，那么能不能定义一个随距离减小的函数，作用在协方差矩阵$\\mathrm {P}^b$上呢？事实上的确能这么做，Gaspari and Cohn函数就是这么一个函数。它由一个单一可调参数确定形状，作用在协方差矩阵或卡尔曼增益上以消除虚假相关。这种类似的方法在数据同化中都称为局地化方法。所以，一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。\n\n<br/><br/><br/><small>*参考*</small>\n\n<small>Gaspari, G., and S. E. Cohn, 1999: Construction of correlation functions in two and three dimensions. *Quart. J. Roy. Meteor. Soc.*, **125**, 723–757. </small>\n\n<small>Hamill, Thomas M., Jeffrey S. Whitaker, and Chris Snyder. \" Distance-Dependent Filtering of Background Error Covariance Estimates in an Ensemble Kalman Filter\". *Monthly Weather Review* 129.11 (2001): 2776-2790.</small>\n\n<small>Hamill, Thomas. (2004). Ensemble-based atmospheric data assimilation. Predictability of Weather and Climate. 10.1017/CBO9780511617652.007. </small>\n","slug":"Sampling Error","published":1,"updated":"2022-01-21T19:35:52.714Z","_id":"ckyoogd6600142sfy5any6s9y","comments":1,"layout":"post","photos":[],"link":"","content":"<p>一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。<a id=\"more\"></a></p>\n<h4 id=\"概率预报与集合卡尔曼滤波\"><a href=\"#概率预报与集合卡尔曼滤波\" class=\"headerlink\" title=\"概率预报与集合卡尔曼滤波\"></a>概率预报与集合卡尔曼滤波</h4><p>自1969年Edward Epstein发表《Stochastic Dynamic Prediction》以来，概率预报已经深入人心，改变了人们对气象预报的看法。Epstein认为，即使大气的动力机制本身是确定性的，由于观测有限，同时模式也非完美，我们永远无法确切知道所谓的“真值”。所以对大气的预报应该是概率的，而非确定性的。如何将概率引入气象预报呢？相较于过去只做一个预报，我们可以从不同的初始条件出发，或使用不同的模式，同时进行多组预报，每个预报成员都可以看作是系统概率分布的一个随机采样，这就是集合预报。</p>\n<p>集合卡尔曼滤波（EnKF）是基于集合预报的同化方法。它可以结合模式和观测的信息，得到更准确的后验初始场。在集合卡尔曼滤波中，对模式不确定性的估计是至关重要的一步，模式不确定性量化表示为一个误差协方差矩阵$\\mathrm {P}^b$，由预报集合估计。但在实践中我们会发现，对$\\mathrm {P}^b$的估计常常受集合数量的影响。例如，将协方差矩阵化为相关系数矩阵，用较少集合估计出的$\\mathrm {P}^b$会在某些位置出现虚假相关，这些虚假相关在使用足够多集合去估计$\\mathrm {P}^b$时是不存在的（图1）。</p>\n<p><img src=\"/images/image-20220119004115826.png\" alt=\"image-20220119004115826\" style=\"zoom:80%;\" /></p>\n<center><small>图1. (Hamill 2004)</small></center>\n\n<p>这些虚假相关，其实是由于采样样本数量不足导致的误差，称为采样误差（Sampling Error）。</p>\n<h4 id=\"采样误差\"><a href=\"#采样误差\" class=\"headerlink\" title=\"采样误差\"></a>采样误差</h4><p>采样误差其实在统计推断中无处不在。当我们想要估计总体（Population）分布的某个参数，通常并不会，或者不能统计到总体的每一个个体，而只会取总体的一部分，作为我们的样本（Sample），从样本来估计总体的参数。比如我们想要知道总体分布的期望大概是多少，就会用样本的均值作为对期望的一个估计。</p>\n<p><img src=\"/images/image-20220119014952338.png\" alt=\"image-20220119014952338\" style=\"zoom:45%;\" /></p>\n<p>假如我们现在要估计一个服从正态分布$X \\sim \\mathcal{N}(0.5,\\sigma ^2) $总体的均值，总体容量为10亿个（若总体容量可为无穷多，总体的均值就严格等于总体分布的期望）。从总体里随机抽取100个样本，以这100个样本的均值作为对总体均值的估计。算出来这100个样本的均值，几乎一定不会等于0.5，总是偏大或偏小一点。而且，重复多次采样，每次估计到的均值也大概率不会相同。多次采样估计出的均值，也会构成一个分布</p>\n<p><img src=\"/images/image-20220119022046007.png\" alt=\"image-20220119022046007\" style=\"zoom:25%;\" /></p>\n<center><small>图2. Margin of error (Wikipedia)</small></center>\n\n<p>每次估计得到的均值与真实的期望的差距就是<strong>采样误差</strong>。抽取2000个样本和抽取100个样本去估计均值，其采样误差的范围是不同的。2000个样本估计得到的均值分布高高瘦瘦，100个样本得到的均值分布相比起来就矮胖一些。显然，我们更希望均值像2000个样本估计的那样，更接近真实的期望，同时又有更小的方差。有一个指标来量化采样误差的大小范围，叫误差限（Margin of Error）</p>\n<p><img src=\"/images/image-20220119024539984.png\" alt=\"image-20220119024539984\" style=\"zoom:22%;\" /></p>\n<p>它与总体分布本身的方差有关，同时也与样本数量n有关。容易看出，样本数量越大，误差限就越小，表示采样误差就越小。</p>\n<p>下面举一个稍复杂点的例子，这次不再是估计单变量的分布特征，而是估计多变量分布的相关特征。假如有两个变量$X_1,X_2$都服从正态分布$X_1,X_2 \\sim \\mathcal{N}(0,1) $，它们的协方差为$c_{12}$，可用一个回归方程描述二者的关系</p>\n<p><img src=\"/images/image-20220119025638519.png\" alt=\"image-20220119025638519\" style=\"zoom:45%;\" /></p>\n<p>其中</p>\n<p><img src=\"/images/image-20220119025731907.png\" alt=\"image-20220119025731907\" style=\"zoom:45%;\" /></p>\n<p>现在要用采样的方式估计回归系数$\\beta$，我们看一下使用不同样本数量估计得到的结果</p>\n<p><img src=\"/images/image-20220119025848148.png\" alt=\"image-20220119025848148\" style=\"zoom:80%;\" /></p>\n<center><small>图3. 不同样本数量（esize）估计得到的回归系数概率密度分布。第一行beta真值为0.01，第二行beta真值为0.1，第三行beta真值为0.4。黑色竖线是beta真值，红色竖线是估计得到的beta的均值，绿色横线是估计的beta的方差，红色曲线是拟合的高斯分布。</small></center>\n\n<p>同样的，样本数量越多，估计量分布就越集中，越接近真实值。</p>\n<p>换成估计相关系数试试。</p>\n<p><img src=\"/images/image-20220119031216615.png\" alt=\"image-20220119031216615\" style=\"zoom:80%;\" /></p>\n<center><small>图4. 不同样本数量（esize）估计得到的相关系数概率密度分布。其余同图3。</small></center>\n\n<p>这次稍有不同，相关系数是有边界的量（[-1,1]），所以估计量不再满足正态分布，但同样能看出样本数量越多，估计量分布越集中且越靠近真值。所以结论是一致的：样本数量越大，采样误差就越小。</p>\n<h4 id=\"消减虚假相关：局地化方法\"><a href=\"#消减虚假相关：局地化方法\" class=\"headerlink\" title=\"消减虚假相关：局地化方法\"></a>消减虚假相关：局地化方法</h4><p>既然增大样本数量就能减小采样误差，那何不用更多的预报集合呢。的确，在钱不是问题的情况下，采样误差也不成问题。但现实是，集合预报需要耗费大量的计算资源，无限度地增加集合数量目前尚不可行。那么问题就变成，如何在有限的条件下，更大限度地提取高质量的信息。具体到EnKF，如何用100个集合成员就达到或逼近2000个集合成员估计的准确度呢。</p>\n<p>Hamill在2001年的文章中指出，两变量间的相关系数越小，使用的集合数量越少，对协方差估计的相对误差就越大</p>\n<p><img src=\"/images/image-20220119034108763.png\" alt=\"image-20220119034108763\" style=\"zoom:25%;\" /></p>\n<center><small>图5. （Hamill 2001）</small></center>\n\n<p>其中协方差$c_{12}$采样误差的方差为</p>\n<p><img src=\"/images/image-20220119034518723.png\" alt=\"image-20220119034518723\" style=\"zoom:26%;\" /></p>\n<p>所以在本应接近无相关的位置出现虚假相关，其影响并非是无足轻重的。那么，如何消除虚假相关，用更少的集合得到更准确的估计效果呢。</p>\n<p>前面提到，我们的问题是在如何在100个集合的限制条件下，提取更多更好的信息。其实还有另一种思路，如果我们无法提取更多的信息，是否可以人为地加入一些先验信息？比如，我们相信随着距离增大，两变量间的相关应该是减小的，那么能不能定义一个随距离减小的函数，作用在协方差矩阵$\\mathrm {P}^b$上呢？事实上的确能这么做，Gaspari and Cohn函数就是这么一个函数。它由一个单一可调参数确定形状，作用在协方差矩阵或卡尔曼增益上以消除虚假相关。这种类似的方法在数据同化中都称为局地化方法。所以，一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。</p>\n<p><br/><br/><br/><small><em>参考</em></small></p>\n<p><small>Gaspari, G., and S. E. Cohn, 1999: Construction of correlation functions in two and three dimensions. <em>Quart. J. Roy. Meteor. Soc.</em>, <strong>125</strong>, 723–757. </small></p>\n<p><small>Hamill, Thomas M., Jeffrey S. Whitaker, and Chris Snyder. “ Distance-Dependent Filtering of Background Error Covariance Estimates in an Ensemble Kalman Filter”. <em>Monthly Weather Review</em> 129.11 (2001): 2776-2790.</small></p>\n<p><small>Hamill, Thomas. (2004). Ensemble-based atmospheric data assimilation. Predictability of Weather and Climate. 10.1017/CBO9780511617652.007. </small></p>\n","site":{"data":{}},"excerpt":"<p>一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。","more":"</p>\n<h4 id=\"概率预报与集合卡尔曼滤波\"><a href=\"#概率预报与集合卡尔曼滤波\" class=\"headerlink\" title=\"概率预报与集合卡尔曼滤波\"></a>概率预报与集合卡尔曼滤波</h4><p>自1969年Edward Epstein发表《Stochastic Dynamic Prediction》以来，概率预报已经深入人心，改变了人们对气象预报的看法。Epstein认为，即使大气的动力机制本身是确定性的，由于观测有限，同时模式也非完美，我们永远无法确切知道所谓的“真值”。所以对大气的预报应该是概率的，而非确定性的。如何将概率引入气象预报呢？相较于过去只做一个预报，我们可以从不同的初始条件出发，或使用不同的模式，同时进行多组预报，每个预报成员都可以看作是系统概率分布的一个随机采样，这就是集合预报。</p>\n<p>集合卡尔曼滤波（EnKF）是基于集合预报的同化方法。它可以结合模式和观测的信息，得到更准确的后验初始场。在集合卡尔曼滤波中，对模式不确定性的估计是至关重要的一步，模式不确定性量化表示为一个误差协方差矩阵$\\mathrm {P}^b$，由预报集合估计。但在实践中我们会发现，对$\\mathrm {P}^b$的估计常常受集合数量的影响。例如，将协方差矩阵化为相关系数矩阵，用较少集合估计出的$\\mathrm {P}^b$会在某些位置出现虚假相关，这些虚假相关在使用足够多集合去估计$\\mathrm {P}^b$时是不存在的（图1）。</p>\n<p><img src=\"/images/image-20220119004115826.png\" alt=\"image-20220119004115826\" style=\"zoom:80%;\" /></p>\n<center><small>图1. (Hamill 2004)</small></center>\n\n<p>这些虚假相关，其实是由于采样样本数量不足导致的误差，称为采样误差（Sampling Error）。</p>\n<h4 id=\"采样误差\"><a href=\"#采样误差\" class=\"headerlink\" title=\"采样误差\"></a>采样误差</h4><p>采样误差其实在统计推断中无处不在。当我们想要估计总体（Population）分布的某个参数，通常并不会，或者不能统计到总体的每一个个体，而只会取总体的一部分，作为我们的样本（Sample），从样本来估计总体的参数。比如我们想要知道总体分布的期望大概是多少，就会用样本的均值作为对期望的一个估计。</p>\n<p><img src=\"/images/image-20220119014952338.png\" alt=\"image-20220119014952338\" style=\"zoom:45%;\" /></p>\n<p>假如我们现在要估计一个服从正态分布$X \\sim \\mathcal{N}(0.5,\\sigma ^2) $总体的均值，总体容量为10亿个（若总体容量可为无穷多，总体的均值就严格等于总体分布的期望）。从总体里随机抽取100个样本，以这100个样本的均值作为对总体均值的估计。算出来这100个样本的均值，几乎一定不会等于0.5，总是偏大或偏小一点。而且，重复多次采样，每次估计到的均值也大概率不会相同。多次采样估计出的均值，也会构成一个分布</p>\n<p><img src=\"/images/image-20220119022046007.png\" alt=\"image-20220119022046007\" style=\"zoom:25%;\" /></p>\n<center><small>图2. Margin of error (Wikipedia)</small></center>\n\n<p>每次估计得到的均值与真实的期望的差距就是<strong>采样误差</strong>。抽取2000个样本和抽取100个样本去估计均值，其采样误差的范围是不同的。2000个样本估计得到的均值分布高高瘦瘦，100个样本得到的均值分布相比起来就矮胖一些。显然，我们更希望均值像2000个样本估计的那样，更接近真实的期望，同时又有更小的方差。有一个指标来量化采样误差的大小范围，叫误差限（Margin of Error）</p>\n<p><img src=\"/images/image-20220119024539984.png\" alt=\"image-20220119024539984\" style=\"zoom:22%;\" /></p>\n<p>它与总体分布本身的方差有关，同时也与样本数量n有关。容易看出，样本数量越大，误差限就越小，表示采样误差就越小。</p>\n<p>下面举一个稍复杂点的例子，这次不再是估计单变量的分布特征，而是估计多变量分布的相关特征。假如有两个变量$X_1,X_2$都服从正态分布$X_1,X_2 \\sim \\mathcal{N}(0,1) $，它们的协方差为$c_{12}$，可用一个回归方程描述二者的关系</p>\n<p><img src=\"/images/image-20220119025638519.png\" alt=\"image-20220119025638519\" style=\"zoom:45%;\" /></p>\n<p>其中</p>\n<p><img src=\"/images/image-20220119025731907.png\" alt=\"image-20220119025731907\" style=\"zoom:45%;\" /></p>\n<p>现在要用采样的方式估计回归系数$\\beta$，我们看一下使用不同样本数量估计得到的结果</p>\n<p><img src=\"/images/image-20220119025848148.png\" alt=\"image-20220119025848148\" style=\"zoom:80%;\" /></p>\n<center><small>图3. 不同样本数量（esize）估计得到的回归系数概率密度分布。第一行beta真值为0.01，第二行beta真值为0.1，第三行beta真值为0.4。黑色竖线是beta真值，红色竖线是估计得到的beta的均值，绿色横线是估计的beta的方差，红色曲线是拟合的高斯分布。</small></center>\n\n<p>同样的，样本数量越多，估计量分布就越集中，越接近真实值。</p>\n<p>换成估计相关系数试试。</p>\n<p><img src=\"/images/image-20220119031216615.png\" alt=\"image-20220119031216615\" style=\"zoom:80%;\" /></p>\n<center><small>图4. 不同样本数量（esize）估计得到的相关系数概率密度分布。其余同图3。</small></center>\n\n<p>这次稍有不同，相关系数是有边界的量（[-1,1]），所以估计量不再满足正态分布，但同样能看出样本数量越多，估计量分布越集中且越靠近真值。所以结论是一致的：样本数量越大，采样误差就越小。</p>\n<h4 id=\"消减虚假相关：局地化方法\"><a href=\"#消减虚假相关：局地化方法\" class=\"headerlink\" title=\"消减虚假相关：局地化方法\"></a>消减虚假相关：局地化方法</h4><p>既然增大样本数量就能减小采样误差，那何不用更多的预报集合呢。的确，在钱不是问题的情况下，采样误差也不成问题。但现实是，集合预报需要耗费大量的计算资源，无限度地增加集合数量目前尚不可行。那么问题就变成，如何在有限的条件下，更大限度地提取高质量的信息。具体到EnKF，如何用100个集合成员就达到或逼近2000个集合成员估计的准确度呢。</p>\n<p>Hamill在2001年的文章中指出，两变量间的相关系数越小，使用的集合数量越少，对协方差估计的相对误差就越大</p>\n<p><img src=\"/images/image-20220119034108763.png\" alt=\"image-20220119034108763\" style=\"zoom:25%;\" /></p>\n<center><small>图5. （Hamill 2001）</small></center>\n\n<p>其中协方差$c_{12}$采样误差的方差为</p>\n<p><img src=\"/images/image-20220119034518723.png\" alt=\"image-20220119034518723\" style=\"zoom:26%;\" /></p>\n<p>所以在本应接近无相关的位置出现虚假相关，其影响并非是无足轻重的。那么，如何消除虚假相关，用更少的集合得到更准确的估计效果呢。</p>\n<p>前面提到，我们的问题是在如何在100个集合的限制条件下，提取更多更好的信息。其实还有另一种思路，如果我们无法提取更多的信息，是否可以人为地加入一些先验信息？比如，我们相信随着距离增大，两变量间的相关应该是减小的，那么能不能定义一个随距离减小的函数，作用在协方差矩阵$\\mathrm {P}^b$上呢？事实上的确能这么做，Gaspari and Cohn函数就是这么一个函数。它由一个单一可调参数确定形状，作用在协方差矩阵或卡尔曼增益上以消除虚假相关。这种类似的方法在数据同化中都称为局地化方法。所以，一个好的局地化函数，也是提升集合同化方法表现的一块重要拼图。</p>\n<p><br/><br/><br/><small><em>参考</em></small></p>\n<p><small>Gaspari, G., and S. E. Cohn, 1999: Construction of correlation functions in two and three dimensions. <em>Quart. J. Roy. Meteor. Soc.</em>, <strong>125</strong>, 723–757. </small></p>\n<p><small>Hamill, Thomas M., Jeffrey S. Whitaker, and Chris Snyder. “ Distance-Dependent Filtering of Background Error Covariance Estimates in an Ensemble Kalman Filter”. <em>Monthly Weather Review</em> 129.11 (2001): 2776-2790.</small></p>\n<p><small>Hamill, Thomas. (2004). Ensemble-based atmospheric data assimilation. Predictability of Weather and Climate. 10.1017/CBO9780511617652.007. </small></p>"},{"title":"集合论与关系","date":"2020-10-26T06:13:00.000Z","mathjax":true,"_content":"\n\n\n集合的基本模型无法描述一种广泛存在的\"序\"的概念，因为集合中的元素是无序的。能不能在集合论的基础上描述“序”呢？<!--more-->\n\n### 一、关系的引入\n\n#### 1.1 有序对\n\n在使用集合论对“序”给出定义之前，我们先用自然一点的语言试着描述什么是“序”。设a, b为对象，我们记(a,b)为一个**有序对**（ordered pair），满足$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$。其中a称为第一分量，b称为第二分量。\n\n有序对的条件$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$实际上规定了第一分量和第二分量是不可交换的，这就是对“序”的数学描述。1914年，Wiener第一次尝试在集合论的基础上给出了有序对的定义\n$$\n(a,b)=\\{\\{\\{a\\},\\emptyset\\},\\{\\{b\\}\\}\\}\n$$\n1921年，Kuratowsk给出了另一种更简洁的定义\n$$\n(a,b)=\\{\\{a\\},\\{a,b\\}\\}\n$$\n可以证明这两种定义都满足有序对的条件。\n\n#### 1.2 笛卡尔积\n\n对集合A与B，称$A\\times B=\\{(a,b)|a \\in A\\wedge b \\in B \\}$为集合A与B的**笛卡尔积**。\n\n例如，假设$A=\\{1,2,3\\},B=\\{a,b\\}$，则$A\\times B=\\{(1,a),(1,b),(2,a),(2,b),(3,a),(3,b)\\}$。\n\n笛卡尔积有若干定理：\n$$\n\\begin{align}\n&(1)\\ A\\times \\emptyset=\\emptyset \\times A = \\emptyset\\\\\n&(2)\\ A \\times B=B \\times A\\\\\n&(3)\\ A \\times (B\\cup C)=(A\\times B)\\cup (A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\ A\\times (B\\cap C)=(A \\times B)\\cap(A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cup C)\\times A=(B \\times A)\\cup(C\\times A)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cap C)\\times A=(B \\times A)\\cap(C\\times A)\n\\end{align}\n$$\n\n\n### 二、关系的定义\n\n现在定义了有序对，下一步可以定义关系：关系是有序对的集合。集合R为**关系**指\n$$\n(\\forall r \\in R)(\\exists x,y)(r=(x,y))\n$$\n设A，B为集合，若$R\\subseteq A\\times B$，称$R$为从A到B的**二元关系**。当$A=B$时，称R为A上的二元关系，在无歧义时一般简称关系。若$(a,b)\\in R$，可简记为$aRb$。\n\n#### 2.1 空关系、全关系、恒同关系\n\n- 空关系：$\\emptyset \\subseteq A \\times A$\n\n- 全关系：$E_A=\\{(x,y)|x,y \\in A\\}$\n\n- 恒同关系：$I_A=\\{(x,x)|x\\in A\\}$\n\n#### 2.2 定义域、值域、域\n\n定义与关系R有关的3个重要集合，设$R\\subseteq A\\times B$\n\n- R的定义域：$Dom(R)=\\{x|(\\exists y \\in B)(x,y)\\in R\\}$\n\n- R的值域：$Ran(R)=\\{y|(\\exists x \\in A)(x,y)\\in R\\}$\n\n- R的域：$Fld(R)=Dom(R) \\cup Ran(R)$\n\n#### 2.3 用矩阵或有向图表示二元关系\n\n设$A=\\{a_1,a_2...a_m\\}$和$B=\\{b_1,b_2...b_n\\}$，$R\\subseteq A\\times B$为A到B的二元关系，则可用一个$m \\times n$的矩阵表示关系R，关系矩阵$M_R=[m_{ij}]_{m\\times n}$\n$$\nm_{ij}=\n\\left\\{\n\\begin{array}{lr}\n1,\\ if (a_i,b_j )\\in R\\\\\n0,\\ if(a_i,b_j )\\notin R\n\\end{array}\n\\right.\n$$\n举个例子，$A=\\{1,2,3\\}$，$B=\\{a,b\\}$，$R=\\{(1,a),(1,b),(2,a),(3,b)\\}$，用矩阵表示为\n$$\n\\left[\n\\begin{matrix}\n1&\n1\\\\\n1&\n0\n\\\\\n0&\n1\n\n\\end{matrix}\n\\right]\n$$\n用有向图表示为\n\n<img src=\"/images/IMG_0788.jpeg\" alt=\"IMG_0788\" style=\"zoom:40%;\" />\n\n### 三、 关系的运算\n\n#### 3.1 关系的逆\n\n设$R\\subseteq A\\times B$，则R的逆\n$$\nR^{-1}=\\{(y,x)|(x,y)\\in R\\}\n$$\n\n$R^{-1}$是从B到A的关系，$R^{-1}\\subseteq B \\times A$。\n\n#### 3.2 关系的复合\n\n设$S \\subseteq A \\times B,\\ R\\subseteq B \\times C$，R与S的复合为\n$$\nR\\circ S=\\{(x,y)|(\\exists t \\in B)((x,t)\\in S \\wedge (t,y)\\in R)\\}\n$$\n其实就是$x(R\\circ S)y \\Leftrightarrow \\exists t(xStRy) $。\n\n有运算规则\n$$\n\\begin{align}\n&(1)\\ (R^{-1})^{-1}=R\\\\\n&(2)\\ R_1 \\circ (R_2\\circ R_3)=(R_1 \\circ R_2)\\circ R_3\\\\\n&(3)\\ (R\\circ S)^{-1}=S^{-1}\\circ R^{-1}\\\\\n&(4)\\ I_B\\circ R=R\\circ I_C =R\n\\end{align}\n$$\n假如你熟悉线性代数里矩阵运算规则就会意识到，关系复合的运算规则和矩阵运算规则形式是一致的。回到集合论的原始观点，**矩阵可以视为关系的表示，矩阵运算其实就是对关系的运算**。\n\n#### 3.3 关系的幂\n\n设$R\\subseteq A\\times A$（注意这里R是集合A到自身的关系，对应的矩阵为方阵），则归纳定义R的n次幂\n$$\nR^0=I_A,\\ R^{n+1}=R \\circ R^n\n$$\n关系的幂的朴素含义：乘一次幂就相当于多加入一个中间点。\n\n<img src=\"/images/IMG_1515.jpg\" alt=\"IMG_1515\" style=\"zoom:20%;\" />\n\n一些关于关系的幂的定理\n\n$$\n\\begin{align}\n&(1)\\ R^{m}\\circ R^n{=R^{m+n}}\\\\\n&(2)\\ (R^{m})^n=R^{mn}\\\\\n&(3)\\ 若\\exists S\\in \\mathbb{N},T\\in \\mathbb{N}^+使R^S=R^{S+T},则\\\\\n&\\ \\ \\ \\ \\ \\ a.(\\forall k \\geq S)(R^k= R^{k+T})\\\\\n&\\ \\ \\ \\ \\ \\ b.(\\forall k \\geq S)(\\forall n \\in \\mathbb{N})(R^k= R^{k+nT})\\\\\n&\\ \\ \\ \\ \\ \\ c.\\{R^0,R^1,...,R^{S+T-1}=\\{R^0,R^1,...,R^n,...\\}\\}\\\\\n&(4)\\ 若|A|=n,则(\\exists s,t \\in \\mathbb{N})(R^s=R^t \\wedge 0 \\leq s \\leq t\\leq 2^{n^2})\n\\end{align}\n$$\n\n\n### 四、关系的性质\n#### 4.1 自反性、对称性和传递性\n\n* ##### 自反性\n\n  R在A上**自反（reflexive）**：$(\\forall x \\in A)(xRx)$\n\n  R在A上**反自反（irreflexive）**：$(\\forall x \\in A)(\\neg xRx)$\n\n  <img src=\"/images/IMG_0802.jpeg\" alt=\"IMG_0802\" style=\"zoom:40%;\" />\n\n  定理：$R是A上的自反关系\\Leftrightarrow I_A\\subseteq R$\n\n* ##### 对称性\n\n  R在A上**对称（symmetric）**：$(\\forall x,y \\in A)(xRy \\to yRx)$\n\n  R在A上**反对称（anti-symmetric）**：$(\\forall x,y \\in A)(xRyRx \\to x=y)$\n\n  R在A上**强反对称（anti-symmetric）**：$(\\forall x,y \\in A)(xRy\\to \\neg yRx)$\n\n  <img src=\"/images/IMG_0802 2.jpeg\" alt=\"IMG_0802 2\" style=\"zoom:40%;\" />\n\n  定理：$R是集合A上的对称关系\\Leftrightarrow R=R^{-1}$\n\n  定理：$R是集合A上的反对称关系\\Leftrightarrow R\\cap R^{-1}\\subseteq I_A$\n\n* ##### 传递性\n\n  R在A上**传递（transitive）**：$(\\forall x,y,z \\in A)(xRyRz \\to xRz)$\n\n  <img src=\"/images/IMG_0802 3.jpeg\" alt=\"IMG_0802 3\" style=\"zoom:40%;\" />\n\n  定理：$R在A上传递\\Leftrightarrow R\\circ R\\subseteq R$\n\n\n\n#### 4.2 等价关系\n\n##### 4.2.1 等价关系\n\n设R为集合A上的关系，若R自反、对称且传递，则称R为A上的**等价关系（equivalence relation）**，记为$x～ _{R}y$或$x～y$。\n\n一个例子：整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系。\n\n##### 4.2.2 等价类\n\n令R为A上的等价关系，对任意$a\\in A$，a关于R的**等价类（equivalence class）**$[a]_R$\n$$\n[a]_R\\equiv \\{b\\in A|aRb\\}\n$$\n简记为$[a]$。\n\n一个例子：R为整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系，则$[x]=\\{y \\in \\mathbb{Z}|x \\equiv y(mod\\ n)\\}=\\{x+kn|k\\in \\mathbb{Z}\\}$\n\n##### 4.2.3 商集\n\n设R为非空集合A上的等价关系，以R的所有等价类作为元素的集合称为A关于R的**商集（quotient set）**$A/R$\n$$\nA/R=\\{[x]_R|x\\in A\\}\n$$\n例子：设$A=\\{1,2,...8\\}$，A关于模3等价关系R的商集为$A/R=\\{\\{1,4,7\\},\\{2,5,8\\},\\{3,6\\}\\}$\n\n##### 4.2.4 集合的划分\n\n设A为非空集合，若A的子集族$\\Pi$（A的子集构成的集合）满足：\n$$\n\\begin{align}\n&(1)\\ \\emptyset \\not \\in \\Pi\n\\\\&(2)\\ (\\forall X,Y\\in \\Pi)(X\\not=Y\\to X\\cap Y=\\emptyset)\\\\\n&(3)\\ \\cup\\Pi =A\n\\end{align}\n$$\n则称$\\Pi$是A的一个**划分（partition）**，称$\\Pi$中的元素为A的**划分块（block）**。\n\n对于非空集合A：\n\n* 每个商集--唯一划分\n* 等价关系--不同的划分方式\n\n\n\n#### 4.3 关系的闭包\n\n设$R$为集合$A$上的关系，$P$为某个性质（自反性、对称性、传递性之一），若存在$S\\subseteq A \\times A$，满足\n$$\n \\begin{align}\n&(1)\\ R \\subseteq  S\n\\\\&(2)\\ S具有性质P\\\\\n&(3)\\ \\forall T(R\\subseteq T \\wedge T具有性质P \\to S \\subseteq T)\n\\end{align}\n$$\n则称S为**相对于P的R闭包**（R的P闭包）。\n\n定理：R的P闭包存在且唯一。\n\n##### 4.3.1 闭包的实用构造\n\n设$R\\subseteq A \\times A$，\n\n- R的自反闭包$r(R)=R\\cup R^0=R\\cup I_A$\n- R的对称闭包$s(R)=R\\cup R^{-1}$\n- R的传递闭包$t(R)=R\\cup R^2 \\cup R^3\\cup ...=\\cup \\{R^n|n\\in \\mathbb{N}^+\\}$\n\n##### 4.3.2 闭包的关系矩阵表示\n\n设$R\\subseteq A\\times A$且$A=\\{a_1,...,a_n\\}$，$M_R$为R的关系矩阵，则\n\n- $M_{r(R)}=M_R\\vee M_{I_A}$\n\n- $M_{s(R)}=M_R\\vee M_R^T$\n\n- $M_{t(R)}=M_R\\vee M_R^{[2]}\\vee ... \\vee M_R^{[n]}$，其中$M_R^{[k]}=\\underbrace{M_R\\odot M_R\\odot ...\\odot M_R}_{k}$\n\n  \n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*主要整理自吴楠老师《离散数学》课堂讲授*</small>\n\n","source":"_posts/Relation.md","raw":"---\ntitle: 集合论与关系 \ndate: 2020-10-26 14:13:00\ncategories:\n- 数理逻辑\ntags: \n- 数学\n- 集合论\nmathjax: true\n---\n\n\n\n集合的基本模型无法描述一种广泛存在的\"序\"的概念，因为集合中的元素是无序的。能不能在集合论的基础上描述“序”呢？<!--more-->\n\n### 一、关系的引入\n\n#### 1.1 有序对\n\n在使用集合论对“序”给出定义之前，我们先用自然一点的语言试着描述什么是“序”。设a, b为对象，我们记(a,b)为一个**有序对**（ordered pair），满足$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$。其中a称为第一分量，b称为第二分量。\n\n有序对的条件$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$实际上规定了第一分量和第二分量是不可交换的，这就是对“序”的数学描述。1914年，Wiener第一次尝试在集合论的基础上给出了有序对的定义\n$$\n(a,b)=\\{\\{\\{a\\},\\emptyset\\},\\{\\{b\\}\\}\\}\n$$\n1921年，Kuratowsk给出了另一种更简洁的定义\n$$\n(a,b)=\\{\\{a\\},\\{a,b\\}\\}\n$$\n可以证明这两种定义都满足有序对的条件。\n\n#### 1.2 笛卡尔积\n\n对集合A与B，称$A\\times B=\\{(a,b)|a \\in A\\wedge b \\in B \\}$为集合A与B的**笛卡尔积**。\n\n例如，假设$A=\\{1,2,3\\},B=\\{a,b\\}$，则$A\\times B=\\{(1,a),(1,b),(2,a),(2,b),(3,a),(3,b)\\}$。\n\n笛卡尔积有若干定理：\n$$\n\\begin{align}\n&(1)\\ A\\times \\emptyset=\\emptyset \\times A = \\emptyset\\\\\n&(2)\\ A \\times B=B \\times A\\\\\n&(3)\\ A \\times (B\\cup C)=(A\\times B)\\cup (A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\ A\\times (B\\cap C)=(A \\times B)\\cap(A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cup C)\\times A=(B \\times A)\\cup(C\\times A)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cap C)\\times A=(B \\times A)\\cap(C\\times A)\n\\end{align}\n$$\n\n\n### 二、关系的定义\n\n现在定义了有序对，下一步可以定义关系：关系是有序对的集合。集合R为**关系**指\n$$\n(\\forall r \\in R)(\\exists x,y)(r=(x,y))\n$$\n设A，B为集合，若$R\\subseteq A\\times B$，称$R$为从A到B的**二元关系**。当$A=B$时，称R为A上的二元关系，在无歧义时一般简称关系。若$(a,b)\\in R$，可简记为$aRb$。\n\n#### 2.1 空关系、全关系、恒同关系\n\n- 空关系：$\\emptyset \\subseteq A \\times A$\n\n- 全关系：$E_A=\\{(x,y)|x,y \\in A\\}$\n\n- 恒同关系：$I_A=\\{(x,x)|x\\in A\\}$\n\n#### 2.2 定义域、值域、域\n\n定义与关系R有关的3个重要集合，设$R\\subseteq A\\times B$\n\n- R的定义域：$Dom(R)=\\{x|(\\exists y \\in B)(x,y)\\in R\\}$\n\n- R的值域：$Ran(R)=\\{y|(\\exists x \\in A)(x,y)\\in R\\}$\n\n- R的域：$Fld(R)=Dom(R) \\cup Ran(R)$\n\n#### 2.3 用矩阵或有向图表示二元关系\n\n设$A=\\{a_1,a_2...a_m\\}$和$B=\\{b_1,b_2...b_n\\}$，$R\\subseteq A\\times B$为A到B的二元关系，则可用一个$m \\times n$的矩阵表示关系R，关系矩阵$M_R=[m_{ij}]_{m\\times n}$\n$$\nm_{ij}=\n\\left\\{\n\\begin{array}{lr}\n1,\\ if (a_i,b_j )\\in R\\\\\n0,\\ if(a_i,b_j )\\notin R\n\\end{array}\n\\right.\n$$\n举个例子，$A=\\{1,2,3\\}$，$B=\\{a,b\\}$，$R=\\{(1,a),(1,b),(2,a),(3,b)\\}$，用矩阵表示为\n$$\n\\left[\n\\begin{matrix}\n1&\n1\\\\\n1&\n0\n\\\\\n0&\n1\n\n\\end{matrix}\n\\right]\n$$\n用有向图表示为\n\n<img src=\"/images/IMG_0788.jpeg\" alt=\"IMG_0788\" style=\"zoom:40%;\" />\n\n### 三、 关系的运算\n\n#### 3.1 关系的逆\n\n设$R\\subseteq A\\times B$，则R的逆\n$$\nR^{-1}=\\{(y,x)|(x,y)\\in R\\}\n$$\n\n$R^{-1}$是从B到A的关系，$R^{-1}\\subseteq B \\times A$。\n\n#### 3.2 关系的复合\n\n设$S \\subseteq A \\times B,\\ R\\subseteq B \\times C$，R与S的复合为\n$$\nR\\circ S=\\{(x,y)|(\\exists t \\in B)((x,t)\\in S \\wedge (t,y)\\in R)\\}\n$$\n其实就是$x(R\\circ S)y \\Leftrightarrow \\exists t(xStRy) $。\n\n有运算规则\n$$\n\\begin{align}\n&(1)\\ (R^{-1})^{-1}=R\\\\\n&(2)\\ R_1 \\circ (R_2\\circ R_3)=(R_1 \\circ R_2)\\circ R_3\\\\\n&(3)\\ (R\\circ S)^{-1}=S^{-1}\\circ R^{-1}\\\\\n&(4)\\ I_B\\circ R=R\\circ I_C =R\n\\end{align}\n$$\n假如你熟悉线性代数里矩阵运算规则就会意识到，关系复合的运算规则和矩阵运算规则形式是一致的。回到集合论的原始观点，**矩阵可以视为关系的表示，矩阵运算其实就是对关系的运算**。\n\n#### 3.3 关系的幂\n\n设$R\\subseteq A\\times A$（注意这里R是集合A到自身的关系，对应的矩阵为方阵），则归纳定义R的n次幂\n$$\nR^0=I_A,\\ R^{n+1}=R \\circ R^n\n$$\n关系的幂的朴素含义：乘一次幂就相当于多加入一个中间点。\n\n<img src=\"/images/IMG_1515.jpg\" alt=\"IMG_1515\" style=\"zoom:20%;\" />\n\n一些关于关系的幂的定理\n\n$$\n\\begin{align}\n&(1)\\ R^{m}\\circ R^n{=R^{m+n}}\\\\\n&(2)\\ (R^{m})^n=R^{mn}\\\\\n&(3)\\ 若\\exists S\\in \\mathbb{N},T\\in \\mathbb{N}^+使R^S=R^{S+T},则\\\\\n&\\ \\ \\ \\ \\ \\ a.(\\forall k \\geq S)(R^k= R^{k+T})\\\\\n&\\ \\ \\ \\ \\ \\ b.(\\forall k \\geq S)(\\forall n \\in \\mathbb{N})(R^k= R^{k+nT})\\\\\n&\\ \\ \\ \\ \\ \\ c.\\{R^0,R^1,...,R^{S+T-1}=\\{R^0,R^1,...,R^n,...\\}\\}\\\\\n&(4)\\ 若|A|=n,则(\\exists s,t \\in \\mathbb{N})(R^s=R^t \\wedge 0 \\leq s \\leq t\\leq 2^{n^2})\n\\end{align}\n$$\n\n\n### 四、关系的性质\n#### 4.1 自反性、对称性和传递性\n\n* ##### 自反性\n\n  R在A上**自反（reflexive）**：$(\\forall x \\in A)(xRx)$\n\n  R在A上**反自反（irreflexive）**：$(\\forall x \\in A)(\\neg xRx)$\n\n  <img src=\"/images/IMG_0802.jpeg\" alt=\"IMG_0802\" style=\"zoom:40%;\" />\n\n  定理：$R是A上的自反关系\\Leftrightarrow I_A\\subseteq R$\n\n* ##### 对称性\n\n  R在A上**对称（symmetric）**：$(\\forall x,y \\in A)(xRy \\to yRx)$\n\n  R在A上**反对称（anti-symmetric）**：$(\\forall x,y \\in A)(xRyRx \\to x=y)$\n\n  R在A上**强反对称（anti-symmetric）**：$(\\forall x,y \\in A)(xRy\\to \\neg yRx)$\n\n  <img src=\"/images/IMG_0802 2.jpeg\" alt=\"IMG_0802 2\" style=\"zoom:40%;\" />\n\n  定理：$R是集合A上的对称关系\\Leftrightarrow R=R^{-1}$\n\n  定理：$R是集合A上的反对称关系\\Leftrightarrow R\\cap R^{-1}\\subseteq I_A$\n\n* ##### 传递性\n\n  R在A上**传递（transitive）**：$(\\forall x,y,z \\in A)(xRyRz \\to xRz)$\n\n  <img src=\"/images/IMG_0802 3.jpeg\" alt=\"IMG_0802 3\" style=\"zoom:40%;\" />\n\n  定理：$R在A上传递\\Leftrightarrow R\\circ R\\subseteq R$\n\n\n\n#### 4.2 等价关系\n\n##### 4.2.1 等价关系\n\n设R为集合A上的关系，若R自反、对称且传递，则称R为A上的**等价关系（equivalence relation）**，记为$x～ _{R}y$或$x～y$。\n\n一个例子：整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系。\n\n##### 4.2.2 等价类\n\n令R为A上的等价关系，对任意$a\\in A$，a关于R的**等价类（equivalence class）**$[a]_R$\n$$\n[a]_R\\equiv \\{b\\in A|aRb\\}\n$$\n简记为$[a]$。\n\n一个例子：R为整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系，则$[x]=\\{y \\in \\mathbb{Z}|x \\equiv y(mod\\ n)\\}=\\{x+kn|k\\in \\mathbb{Z}\\}$\n\n##### 4.2.3 商集\n\n设R为非空集合A上的等价关系，以R的所有等价类作为元素的集合称为A关于R的**商集（quotient set）**$A/R$\n$$\nA/R=\\{[x]_R|x\\in A\\}\n$$\n例子：设$A=\\{1,2,...8\\}$，A关于模3等价关系R的商集为$A/R=\\{\\{1,4,7\\},\\{2,5,8\\},\\{3,6\\}\\}$\n\n##### 4.2.4 集合的划分\n\n设A为非空集合，若A的子集族$\\Pi$（A的子集构成的集合）满足：\n$$\n\\begin{align}\n&(1)\\ \\emptyset \\not \\in \\Pi\n\\\\&(2)\\ (\\forall X,Y\\in \\Pi)(X\\not=Y\\to X\\cap Y=\\emptyset)\\\\\n&(3)\\ \\cup\\Pi =A\n\\end{align}\n$$\n则称$\\Pi$是A的一个**划分（partition）**，称$\\Pi$中的元素为A的**划分块（block）**。\n\n对于非空集合A：\n\n* 每个商集--唯一划分\n* 等价关系--不同的划分方式\n\n\n\n#### 4.3 关系的闭包\n\n设$R$为集合$A$上的关系，$P$为某个性质（自反性、对称性、传递性之一），若存在$S\\subseteq A \\times A$，满足\n$$\n \\begin{align}\n&(1)\\ R \\subseteq  S\n\\\\&(2)\\ S具有性质P\\\\\n&(3)\\ \\forall T(R\\subseteq T \\wedge T具有性质P \\to S \\subseteq T)\n\\end{align}\n$$\n则称S为**相对于P的R闭包**（R的P闭包）。\n\n定理：R的P闭包存在且唯一。\n\n##### 4.3.1 闭包的实用构造\n\n设$R\\subseteq A \\times A$，\n\n- R的自反闭包$r(R)=R\\cup R^0=R\\cup I_A$\n- R的对称闭包$s(R)=R\\cup R^{-1}$\n- R的传递闭包$t(R)=R\\cup R^2 \\cup R^3\\cup ...=\\cup \\{R^n|n\\in \\mathbb{N}^+\\}$\n\n##### 4.3.2 闭包的关系矩阵表示\n\n设$R\\subseteq A\\times A$且$A=\\{a_1,...,a_n\\}$，$M_R$为R的关系矩阵，则\n\n- $M_{r(R)}=M_R\\vee M_{I_A}$\n\n- $M_{s(R)}=M_R\\vee M_R^T$\n\n- $M_{t(R)}=M_R\\vee M_R^{[2]}\\vee ... \\vee M_R^{[n]}$，其中$M_R^{[k]}=\\underbrace{M_R\\odot M_R\\odot ...\\odot M_R}_{k}$\n\n  \n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*主要整理自吴楠老师《离散数学》课堂讲授*</small>\n\n","slug":"Relation","published":1,"updated":"2021-03-07T06:39:24.666Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6700172sfy0f1t8hry","content":"<p>集合的基本模型无法描述一种广泛存在的”序”的概念，因为集合中的元素是无序的。能不能在集合论的基础上描述“序”呢？<a id=\"more\"></a></p>\n<h3 id=\"一、关系的引入\"><a href=\"#一、关系的引入\" class=\"headerlink\" title=\"一、关系的引入\"></a>一、关系的引入</h3><h4 id=\"1-1-有序对\"><a href=\"#1-1-有序对\" class=\"headerlink\" title=\"1.1 有序对\"></a>1.1 有序对</h4><p>在使用集合论对“序”给出定义之前，我们先用自然一点的语言试着描述什么是“序”。设a, b为对象，我们记(a,b)为一个<strong>有序对</strong>（ordered pair），满足$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$。其中a称为第一分量，b称为第二分量。</p>\n<p>有序对的条件$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$实际上规定了第一分量和第二分量是不可交换的，这就是对“序”的数学描述。1914年，Wiener第一次尝试在集合论的基础上给出了有序对的定义</p>\n<script type=\"math/tex; mode=display\">\n(a,b)=\\{\\{\\{a\\},\\emptyset\\},\\{\\{b\\}\\}\\}</script><p>1921年，Kuratowsk给出了另一种更简洁的定义</p>\n<script type=\"math/tex; mode=display\">\n(a,b)=\\{\\{a\\},\\{a,b\\}\\}</script><p>可以证明这两种定义都满足有序对的条件。</p>\n<h4 id=\"1-2-笛卡尔积\"><a href=\"#1-2-笛卡尔积\" class=\"headerlink\" title=\"1.2 笛卡尔积\"></a>1.2 笛卡尔积</h4><p>对集合A与B，称$A\\times B=\\{(a,b)|a \\in A\\wedge b \\in B \\}$为集合A与B的<strong>笛卡尔积</strong>。</p>\n<p>例如，假设$A=\\{1,2,3\\},B=\\{a,b\\}$，则$A\\times B=\\{(1,a),(1,b),(2,a),(2,b),(3,a),(3,b)\\}$。</p>\n<p>笛卡尔积有若干定理：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ A\\times \\emptyset=\\emptyset \\times A = \\emptyset\\\\\n&(2)\\ A \\times B=B \\times A\\\\\n&(3)\\ A \\times (B\\cup C)=(A\\times B)\\cup (A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\ A\\times (B\\cap C)=(A \\times B)\\cap(A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cup C)\\times A=(B \\times A)\\cup(C\\times A)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cap C)\\times A=(B \\times A)\\cap(C\\times A)\n\\end{align}</script><h3 id=\"二、关系的定义\"><a href=\"#二、关系的定义\" class=\"headerlink\" title=\"二、关系的定义\"></a>二、关系的定义</h3><p>现在定义了有序对，下一步可以定义关系：关系是有序对的集合。集合R为<strong>关系</strong>指</p>\n<script type=\"math/tex; mode=display\">\n(\\forall r \\in R)(\\exists x,y)(r=(x,y))</script><p>设A，B为集合，若$R\\subseteq A\\times B$，称$R$为从A到B的<strong>二元关系</strong>。当$A=B$时，称R为A上的二元关系，在无歧义时一般简称关系。若$(a,b)\\in R$，可简记为$aRb$。</p>\n<h4 id=\"2-1-空关系、全关系、恒同关系\"><a href=\"#2-1-空关系、全关系、恒同关系\" class=\"headerlink\" title=\"2.1 空关系、全关系、恒同关系\"></a>2.1 空关系、全关系、恒同关系</h4><ul>\n<li><p>空关系：$\\emptyset \\subseteq A \\times A$</p>\n</li>\n<li><p>全关系：$E_A=\\{(x,y)|x,y \\in A\\}$</p>\n</li>\n<li><p>恒同关系：$I_A=\\{(x,x)|x\\in A\\}$</p>\n</li>\n</ul>\n<h4 id=\"2-2-定义域、值域、域\"><a href=\"#2-2-定义域、值域、域\" class=\"headerlink\" title=\"2.2 定义域、值域、域\"></a>2.2 定义域、值域、域</h4><p>定义与关系R有关的3个重要集合，设$R\\subseteq A\\times B$</p>\n<ul>\n<li><p>R的定义域：$Dom(R)=\\{x|(\\exists y \\in B)(x,y)\\in R\\}$</p>\n</li>\n<li><p>R的值域：$Ran(R)=\\{y|(\\exists x \\in A)(x,y)\\in R\\}$</p>\n</li>\n<li><p>R的域：$Fld(R)=Dom(R) \\cup Ran(R)$</p>\n</li>\n</ul>\n<h4 id=\"2-3-用矩阵或有向图表示二元关系\"><a href=\"#2-3-用矩阵或有向图表示二元关系\" class=\"headerlink\" title=\"2.3 用矩阵或有向图表示二元关系\"></a>2.3 用矩阵或有向图表示二元关系</h4><p>设$A=\\{a_1,a_2…a_m\\}$和$B=\\{b_1,b_2…b_n\\}$，$R\\subseteq A\\times B$为A到B的二元关系，则可用一个$m \\times n$的矩阵表示关系R，关系矩阵$M_R=[m_{ij}]_{m\\times n}$</p>\n<script type=\"math/tex; mode=display\">\nm_{ij}=\n\\left\\{\n\\begin{array}{lr}\n1,\\ if (a_i,b_j )\\in R\\\\\n0,\\ if(a_i,b_j )\\notin R\n\\end{array}\n\\right.</script><p>举个例子，$A=\\{1,2,3\\}$，$B=\\{a,b\\}$，$R=\\{(1,a),(1,b),(2,a),(3,b)\\}$，用矩阵表示为</p>\n<script type=\"math/tex; mode=display\">\n\\left[\n\\begin{matrix}\n1&\n1\\\\\n1&\n0\n\\\\\n0&\n1\n\n\\end{matrix}\n\\right]</script><p>用有向图表示为</p>\n<p><img src=\"/images/IMG_0788.jpeg\" alt=\"IMG_0788\" style=\"zoom:40%;\" /></p>\n<h3 id=\"三、-关系的运算\"><a href=\"#三、-关系的运算\" class=\"headerlink\" title=\"三、 关系的运算\"></a>三、 关系的运算</h3><h4 id=\"3-1-关系的逆\"><a href=\"#3-1-关系的逆\" class=\"headerlink\" title=\"3.1 关系的逆\"></a>3.1 关系的逆</h4><p>设$R\\subseteq A\\times B$，则R的逆</p>\n<script type=\"math/tex; mode=display\">\nR^{-1}=\\{(y,x)|(x,y)\\in R\\}</script><p>$R^{-1}$是从B到A的关系，$R^{-1}\\subseteq B \\times A$。</p>\n<h4 id=\"3-2-关系的复合\"><a href=\"#3-2-关系的复合\" class=\"headerlink\" title=\"3.2 关系的复合\"></a>3.2 关系的复合</h4><p>设$S \\subseteq A \\times B,\\ R\\subseteq B \\times C$，R与S的复合为</p>\n<script type=\"math/tex; mode=display\">\nR\\circ S=\\{(x,y)|(\\exists t \\in B)((x,t)\\in S \\wedge (t,y)\\in R)\\}</script><p>其实就是$x(R\\circ S)y \\Leftrightarrow \\exists t(xStRy) $。</p>\n<p>有运算规则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ (R^{-1})^{-1}=R\\\\\n&(2)\\ R_1 \\circ (R_2\\circ R_3)=(R_1 \\circ R_2)\\circ R_3\\\\\n&(3)\\ (R\\circ S)^{-1}=S^{-1}\\circ R^{-1}\\\\\n&(4)\\ I_B\\circ R=R\\circ I_C =R\n\\end{align}</script><p>假如你熟悉线性代数里矩阵运算规则就会意识到，关系复合的运算规则和矩阵运算规则形式是一致的。回到集合论的原始观点，<strong>矩阵可以视为关系的表示，矩阵运算其实就是对关系的运算</strong>。</p>\n<h4 id=\"3-3-关系的幂\"><a href=\"#3-3-关系的幂\" class=\"headerlink\" title=\"3.3 关系的幂\"></a>3.3 关系的幂</h4><p>设$R\\subseteq A\\times A$（注意这里R是集合A到自身的关系，对应的矩阵为方阵），则归纳定义R的n次幂</p>\n<script type=\"math/tex; mode=display\">\nR^0=I_A,\\ R^{n+1}=R \\circ R^n</script><p>关系的幂的朴素含义：乘一次幂就相当于多加入一个中间点。</p>\n<p><img src=\"/images/IMG_1515.jpg\" alt=\"IMG_1515\" style=\"zoom:20%;\" /></p>\n<p>一些关于关系的幂的定理</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ R^{m}\\circ R^n{=R^{m+n}}\\\\\n&(2)\\ (R^{m})^n=R^{mn}\\\\\n&(3)\\ 若\\exists S\\in \\mathbb{N},T\\in \\mathbb{N}^+使R^S=R^{S+T},则\\\\\n&\\ \\ \\ \\ \\ \\ a.(\\forall k \\geq S)(R^k= R^{k+T})\\\\\n&\\ \\ \\ \\ \\ \\ b.(\\forall k \\geq S)(\\forall n \\in \\mathbb{N})(R^k= R^{k+nT})\\\\\n&\\ \\ \\ \\ \\ \\ c.\\{R^0,R^1,...,R^{S+T-1}=\\{R^0,R^1,...,R^n,...\\}\\}\\\\\n&(4)\\ 若|A|=n,则(\\exists s,t \\in \\mathbb{N})(R^s=R^t \\wedge 0 \\leq s \\leq t\\leq 2^{n^2})\n\\end{align}</script><h3 id=\"四、关系的性质\"><a href=\"#四、关系的性质\" class=\"headerlink\" title=\"四、关系的性质\"></a>四、关系的性质</h3><h4 id=\"4-1-自反性、对称性和传递性\"><a href=\"#4-1-自反性、对称性和传递性\" class=\"headerlink\" title=\"4.1 自反性、对称性和传递性\"></a>4.1 自反性、对称性和传递性</h4><ul>\n<li><h5 id=\"自反性\"><a href=\"#自反性\" class=\"headerlink\" title=\"自反性\"></a>自反性</h5><p>R在A上<strong>自反（reflexive）</strong>：$(\\forall x \\in A)(xRx)$</p>\n<p>R在A上<strong>反自反（irreflexive）</strong>：$(\\forall x \\in A)(\\neg xRx)$</p>\n<p><img src=\"/images/IMG_0802.jpeg\" alt=\"IMG_0802\" style=\"zoom:40%;\" /></p>\n<p>定理：$R是A上的自反关系\\Leftrightarrow I_A\\subseteq R$</p>\n</li>\n<li><h5 id=\"对称性\"><a href=\"#对称性\" class=\"headerlink\" title=\"对称性\"></a>对称性</h5><p>R在A上<strong>对称（symmetric）</strong>：$(\\forall x,y \\in A)(xRy \\to yRx)$</p>\n<p>R在A上<strong>反对称（anti-symmetric）</strong>：$(\\forall x,y \\in A)(xRyRx \\to x=y)$</p>\n<p>R在A上<strong>强反对称（anti-symmetric）</strong>：$(\\forall x,y \\in A)(xRy\\to \\neg yRx)$</p>\n<p><img src=\"/images/IMG_0802 2.jpeg\" alt=\"IMG_0802 2\" style=\"zoom:40%;\" /></p>\n<p>定理：$R是集合A上的对称关系\\Leftrightarrow R=R^{-1}$</p>\n<p>定理：$R是集合A上的反对称关系\\Leftrightarrow R\\cap R^{-1}\\subseteq I_A$</p>\n</li>\n<li><h5 id=\"传递性\"><a href=\"#传递性\" class=\"headerlink\" title=\"传递性\"></a>传递性</h5><p>R在A上<strong>传递（transitive）</strong>：$(\\forall x,y,z \\in A)(xRyRz \\to xRz)$</p>\n<p><img src=\"/images/IMG_0802 3.jpeg\" alt=\"IMG_0802 3\" style=\"zoom:40%;\" /></p>\n<p>定理：$R在A上传递\\Leftrightarrow R\\circ R\\subseteq R$</p>\n</li>\n</ul>\n<h4 id=\"4-2-等价关系\"><a href=\"#4-2-等价关系\" class=\"headerlink\" title=\"4.2 等价关系\"></a>4.2 等价关系</h4><h5 id=\"4-2-1-等价关系\"><a href=\"#4-2-1-等价关系\" class=\"headerlink\" title=\"4.2.1 等价关系\"></a>4.2.1 等价关系</h5><p>设R为集合A上的关系，若R自反、对称且传递，则称R为A上的<strong>等价关系（equivalence relation）</strong>，记为$x～ _{R}y$或$x～y$。</p>\n<p>一个例子：整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系。</p>\n<h5 id=\"4-2-2-等价类\"><a href=\"#4-2-2-等价类\" class=\"headerlink\" title=\"4.2.2 等价类\"></a>4.2.2 等价类</h5><p>令R为A上的等价关系，对任意$a\\in A$，a关于R的<strong>等价类（equivalence class）</strong>$[a]_R$</p>\n<script type=\"math/tex; mode=display\">\n[a]_R\\equiv \\{b\\in A|aRb\\}</script><p>简记为$[a]$。</p>\n<p>一个例子：R为整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系，则$[x]=\\{y \\in \\mathbb{Z}|x \\equiv y(mod\\ n)\\}=\\{x+kn|k\\in \\mathbb{Z}\\}$</p>\n<h5 id=\"4-2-3-商集\"><a href=\"#4-2-3-商集\" class=\"headerlink\" title=\"4.2.3 商集\"></a>4.2.3 商集</h5><p>设R为非空集合A上的等价关系，以R的所有等价类作为元素的集合称为A关于R的<strong>商集（quotient set）</strong>$A/R$</p>\n<script type=\"math/tex; mode=display\">\nA/R=\\{[x]_R|x\\in A\\}</script><p>例子：设$A=\\{1,2,…8\\}$，A关于模3等价关系R的商集为$A/R=\\{\\{1,4,7\\},\\{2,5,8\\},\\{3,6\\}\\}$</p>\n<h5 id=\"4-2-4-集合的划分\"><a href=\"#4-2-4-集合的划分\" class=\"headerlink\" title=\"4.2.4 集合的划分\"></a>4.2.4 集合的划分</h5><p>设A为非空集合，若A的子集族$\\Pi$（A的子集构成的集合）满足：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ \\emptyset \\not \\in \\Pi\n\\\\&(2)\\ (\\forall X,Y\\in \\Pi)(X\\not=Y\\to X\\cap Y=\\emptyset)\\\\\n&(3)\\ \\cup\\Pi =A\n\\end{align}</script><p>则称$\\Pi$是A的一个<strong>划分（partition）</strong>，称$\\Pi$中的元素为A的<strong>划分块（block）</strong>。</p>\n<p>对于非空集合A：</p>\n<ul>\n<li>每个商集—唯一划分</li>\n<li>等价关系—不同的划分方式</li>\n</ul>\n<h4 id=\"4-3-关系的闭包\"><a href=\"#4-3-关系的闭包\" class=\"headerlink\" title=\"4.3 关系的闭包\"></a>4.3 关系的闭包</h4><p>设$R$为集合$A$上的关系，$P$为某个性质（自反性、对称性、传递性之一），若存在$S\\subseteq A \\times A$，满足</p>\n<script type=\"math/tex; mode=display\">\n \\begin{align}\n&(1)\\ R \\subseteq  S\n\\\\&(2)\\ S具有性质P\\\\\n&(3)\\ \\forall T(R\\subseteq T \\wedge T具有性质P \\to S \\subseteq T)\n\\end{align}</script><p>则称S为<strong>相对于P的R闭包</strong>（R的P闭包）。</p>\n<p>定理：R的P闭包存在且唯一。</p>\n<h5 id=\"4-3-1-闭包的实用构造\"><a href=\"#4-3-1-闭包的实用构造\" class=\"headerlink\" title=\"4.3.1 闭包的实用构造\"></a>4.3.1 闭包的实用构造</h5><p>设$R\\subseteq A \\times A$，</p>\n<ul>\n<li>R的自反闭包$r(R)=R\\cup R^0=R\\cup I_A$</li>\n<li>R的对称闭包$s(R)=R\\cup R^{-1}$</li>\n<li>R的传递闭包$t(R)=R\\cup R^2 \\cup R^3\\cup …=\\cup \\{R^n|n\\in \\mathbb{N}^+\\}$</li>\n</ul>\n<h5 id=\"4-3-2-闭包的关系矩阵表示\"><a href=\"#4-3-2-闭包的关系矩阵表示\" class=\"headerlink\" title=\"4.3.2 闭包的关系矩阵表示\"></a>4.3.2 闭包的关系矩阵表示</h5><p>设$R\\subseteq A\\times A$且$A=\\{a_1,…,a_n\\}$，$M_R$为R的关系矩阵，则</p>\n<ul>\n<li><p>$M_{r(R)}=M_R\\vee M_{I_A}$</p>\n</li>\n<li><p>$M_{s(R)}=M_R\\vee M_R^T$</p>\n</li>\n<li><p>$M_{t(R)}=M_R\\vee M_R^{[2]}\\vee … \\vee M_R^{[n]}$，其中$M_R^{[k]}=\\underbrace{M_R\\odot M_R\\odot …\\odot M_R}_{k}$</p>\n</li>\n</ul>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>主要整理自吴楠老师《离散数学》课堂讲授</em></small></p>\n","site":{"data":{}},"excerpt":"<p>集合的基本模型无法描述一种广泛存在的”序”的概念，因为集合中的元素是无序的。能不能在集合论的基础上描述“序”呢？","more":"</p>\n<h3 id=\"一、关系的引入\"><a href=\"#一、关系的引入\" class=\"headerlink\" title=\"一、关系的引入\"></a>一、关系的引入</h3><h4 id=\"1-1-有序对\"><a href=\"#1-1-有序对\" class=\"headerlink\" title=\"1.1 有序对\"></a>1.1 有序对</h4><p>在使用集合论对“序”给出定义之前，我们先用自然一点的语言试着描述什么是“序”。设a, b为对象，我们记(a,b)为一个<strong>有序对</strong>（ordered pair），满足$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$。其中a称为第一分量，b称为第二分量。</p>\n<p>有序对的条件$(a,b)=(c,d)\\Leftrightarrow (a=c  \\wedge b=d)$实际上规定了第一分量和第二分量是不可交换的，这就是对“序”的数学描述。1914年，Wiener第一次尝试在集合论的基础上给出了有序对的定义</p>\n<script type=\"math/tex; mode=display\">\n(a,b)=\\{\\{\\{a\\},\\emptyset\\},\\{\\{b\\}\\}\\}</script><p>1921年，Kuratowsk给出了另一种更简洁的定义</p>\n<script type=\"math/tex; mode=display\">\n(a,b)=\\{\\{a\\},\\{a,b\\}\\}</script><p>可以证明这两种定义都满足有序对的条件。</p>\n<h4 id=\"1-2-笛卡尔积\"><a href=\"#1-2-笛卡尔积\" class=\"headerlink\" title=\"1.2 笛卡尔积\"></a>1.2 笛卡尔积</h4><p>对集合A与B，称$A\\times B=\\{(a,b)|a \\in A\\wedge b \\in B \\}$为集合A与B的<strong>笛卡尔积</strong>。</p>\n<p>例如，假设$A=\\{1,2,3\\},B=\\{a,b\\}$，则$A\\times B=\\{(1,a),(1,b),(2,a),(2,b),(3,a),(3,b)\\}$。</p>\n<p>笛卡尔积有若干定理：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ A\\times \\emptyset=\\emptyset \\times A = \\emptyset\\\\\n&(2)\\ A \\times B=B \\times A\\\\\n&(3)\\ A \\times (B\\cup C)=(A\\times B)\\cup (A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\ A\\times (B\\cap C)=(A \\times B)\\cap(A\\times C)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cup C)\\times A=(B \\times A)\\cup(C\\times A)\\\\\n&\\  \\ \\ \\ \\ \\  (B\\cap C)\\times A=(B \\times A)\\cap(C\\times A)\n\\end{align}</script><h3 id=\"二、关系的定义\"><a href=\"#二、关系的定义\" class=\"headerlink\" title=\"二、关系的定义\"></a>二、关系的定义</h3><p>现在定义了有序对，下一步可以定义关系：关系是有序对的集合。集合R为<strong>关系</strong>指</p>\n<script type=\"math/tex; mode=display\">\n(\\forall r \\in R)(\\exists x,y)(r=(x,y))</script><p>设A，B为集合，若$R\\subseteq A\\times B$，称$R$为从A到B的<strong>二元关系</strong>。当$A=B$时，称R为A上的二元关系，在无歧义时一般简称关系。若$(a,b)\\in R$，可简记为$aRb$。</p>\n<h4 id=\"2-1-空关系、全关系、恒同关系\"><a href=\"#2-1-空关系、全关系、恒同关系\" class=\"headerlink\" title=\"2.1 空关系、全关系、恒同关系\"></a>2.1 空关系、全关系、恒同关系</h4><ul>\n<li><p>空关系：$\\emptyset \\subseteq A \\times A$</p>\n</li>\n<li><p>全关系：$E_A=\\{(x,y)|x,y \\in A\\}$</p>\n</li>\n<li><p>恒同关系：$I_A=\\{(x,x)|x\\in A\\}$</p>\n</li>\n</ul>\n<h4 id=\"2-2-定义域、值域、域\"><a href=\"#2-2-定义域、值域、域\" class=\"headerlink\" title=\"2.2 定义域、值域、域\"></a>2.2 定义域、值域、域</h4><p>定义与关系R有关的3个重要集合，设$R\\subseteq A\\times B$</p>\n<ul>\n<li><p>R的定义域：$Dom(R)=\\{x|(\\exists y \\in B)(x,y)\\in R\\}$</p>\n</li>\n<li><p>R的值域：$Ran(R)=\\{y|(\\exists x \\in A)(x,y)\\in R\\}$</p>\n</li>\n<li><p>R的域：$Fld(R)=Dom(R) \\cup Ran(R)$</p>\n</li>\n</ul>\n<h4 id=\"2-3-用矩阵或有向图表示二元关系\"><a href=\"#2-3-用矩阵或有向图表示二元关系\" class=\"headerlink\" title=\"2.3 用矩阵或有向图表示二元关系\"></a>2.3 用矩阵或有向图表示二元关系</h4><p>设$A=\\{a_1,a_2…a_m\\}$和$B=\\{b_1,b_2…b_n\\}$，$R\\subseteq A\\times B$为A到B的二元关系，则可用一个$m \\times n$的矩阵表示关系R，关系矩阵$M_R=[m_{ij}]_{m\\times n}$</p>\n<script type=\"math/tex; mode=display\">\nm_{ij}=\n\\left\\{\n\\begin{array}{lr}\n1,\\ if (a_i,b_j )\\in R\\\\\n0,\\ if(a_i,b_j )\\notin R\n\\end{array}\n\\right.</script><p>举个例子，$A=\\{1,2,3\\}$，$B=\\{a,b\\}$，$R=\\{(1,a),(1,b),(2,a),(3,b)\\}$，用矩阵表示为</p>\n<script type=\"math/tex; mode=display\">\n\\left[\n\\begin{matrix}\n1&\n1\\\\\n1&\n0\n\\\\\n0&\n1\n\n\\end{matrix}\n\\right]</script><p>用有向图表示为</p>\n<p><img src=\"/images/IMG_0788.jpeg\" alt=\"IMG_0788\" style=\"zoom:40%;\" /></p>\n<h3 id=\"三、-关系的运算\"><a href=\"#三、-关系的运算\" class=\"headerlink\" title=\"三、 关系的运算\"></a>三、 关系的运算</h3><h4 id=\"3-1-关系的逆\"><a href=\"#3-1-关系的逆\" class=\"headerlink\" title=\"3.1 关系的逆\"></a>3.1 关系的逆</h4><p>设$R\\subseteq A\\times B$，则R的逆</p>\n<script type=\"math/tex; mode=display\">\nR^{-1}=\\{(y,x)|(x,y)\\in R\\}</script><p>$R^{-1}$是从B到A的关系，$R^{-1}\\subseteq B \\times A$。</p>\n<h4 id=\"3-2-关系的复合\"><a href=\"#3-2-关系的复合\" class=\"headerlink\" title=\"3.2 关系的复合\"></a>3.2 关系的复合</h4><p>设$S \\subseteq A \\times B,\\ R\\subseteq B \\times C$，R与S的复合为</p>\n<script type=\"math/tex; mode=display\">\nR\\circ S=\\{(x,y)|(\\exists t \\in B)((x,t)\\in S \\wedge (t,y)\\in R)\\}</script><p>其实就是$x(R\\circ S)y \\Leftrightarrow \\exists t(xStRy) $。</p>\n<p>有运算规则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ (R^{-1})^{-1}=R\\\\\n&(2)\\ R_1 \\circ (R_2\\circ R_3)=(R_1 \\circ R_2)\\circ R_3\\\\\n&(3)\\ (R\\circ S)^{-1}=S^{-1}\\circ R^{-1}\\\\\n&(4)\\ I_B\\circ R=R\\circ I_C =R\n\\end{align}</script><p>假如你熟悉线性代数里矩阵运算规则就会意识到，关系复合的运算规则和矩阵运算规则形式是一致的。回到集合论的原始观点，<strong>矩阵可以视为关系的表示，矩阵运算其实就是对关系的运算</strong>。</p>\n<h4 id=\"3-3-关系的幂\"><a href=\"#3-3-关系的幂\" class=\"headerlink\" title=\"3.3 关系的幂\"></a>3.3 关系的幂</h4><p>设$R\\subseteq A\\times A$（注意这里R是集合A到自身的关系，对应的矩阵为方阵），则归纳定义R的n次幂</p>\n<script type=\"math/tex; mode=display\">\nR^0=I_A,\\ R^{n+1}=R \\circ R^n</script><p>关系的幂的朴素含义：乘一次幂就相当于多加入一个中间点。</p>\n<p><img src=\"/images/IMG_1515.jpg\" alt=\"IMG_1515\" style=\"zoom:20%;\" /></p>\n<p>一些关于关系的幂的定理</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ R^{m}\\circ R^n{=R^{m+n}}\\\\\n&(2)\\ (R^{m})^n=R^{mn}\\\\\n&(3)\\ 若\\exists S\\in \\mathbb{N},T\\in \\mathbb{N}^+使R^S=R^{S+T},则\\\\\n&\\ \\ \\ \\ \\ \\ a.(\\forall k \\geq S)(R^k= R^{k+T})\\\\\n&\\ \\ \\ \\ \\ \\ b.(\\forall k \\geq S)(\\forall n \\in \\mathbb{N})(R^k= R^{k+nT})\\\\\n&\\ \\ \\ \\ \\ \\ c.\\{R^0,R^1,...,R^{S+T-1}=\\{R^0,R^1,...,R^n,...\\}\\}\\\\\n&(4)\\ 若|A|=n,则(\\exists s,t \\in \\mathbb{N})(R^s=R^t \\wedge 0 \\leq s \\leq t\\leq 2^{n^2})\n\\end{align}</script><h3 id=\"四、关系的性质\"><a href=\"#四、关系的性质\" class=\"headerlink\" title=\"四、关系的性质\"></a>四、关系的性质</h3><h4 id=\"4-1-自反性、对称性和传递性\"><a href=\"#4-1-自反性、对称性和传递性\" class=\"headerlink\" title=\"4.1 自反性、对称性和传递性\"></a>4.1 自反性、对称性和传递性</h4><ul>\n<li><h5 id=\"自反性\"><a href=\"#自反性\" class=\"headerlink\" title=\"自反性\"></a>自反性</h5><p>R在A上<strong>自反（reflexive）</strong>：$(\\forall x \\in A)(xRx)$</p>\n<p>R在A上<strong>反自反（irreflexive）</strong>：$(\\forall x \\in A)(\\neg xRx)$</p>\n<p><img src=\"/images/IMG_0802.jpeg\" alt=\"IMG_0802\" style=\"zoom:40%;\" /></p>\n<p>定理：$R是A上的自反关系\\Leftrightarrow I_A\\subseteq R$</p>\n</li>\n<li><h5 id=\"对称性\"><a href=\"#对称性\" class=\"headerlink\" title=\"对称性\"></a>对称性</h5><p>R在A上<strong>对称（symmetric）</strong>：$(\\forall x,y \\in A)(xRy \\to yRx)$</p>\n<p>R在A上<strong>反对称（anti-symmetric）</strong>：$(\\forall x,y \\in A)(xRyRx \\to x=y)$</p>\n<p>R在A上<strong>强反对称（anti-symmetric）</strong>：$(\\forall x,y \\in A)(xRy\\to \\neg yRx)$</p>\n<p><img src=\"/images/IMG_0802 2.jpeg\" alt=\"IMG_0802 2\" style=\"zoom:40%;\" /></p>\n<p>定理：$R是集合A上的对称关系\\Leftrightarrow R=R^{-1}$</p>\n<p>定理：$R是集合A上的反对称关系\\Leftrightarrow R\\cap R^{-1}\\subseteq I_A$</p>\n</li>\n<li><h5 id=\"传递性\"><a href=\"#传递性\" class=\"headerlink\" title=\"传递性\"></a>传递性</h5><p>R在A上<strong>传递（transitive）</strong>：$(\\forall x,y,z \\in A)(xRyRz \\to xRz)$</p>\n<p><img src=\"/images/IMG_0802 3.jpeg\" alt=\"IMG_0802 3\" style=\"zoom:40%;\" /></p>\n<p>定理：$R在A上传递\\Leftrightarrow R\\circ R\\subseteq R$</p>\n</li>\n</ul>\n<h4 id=\"4-2-等价关系\"><a href=\"#4-2-等价关系\" class=\"headerlink\" title=\"4.2 等价关系\"></a>4.2 等价关系</h4><h5 id=\"4-2-1-等价关系\"><a href=\"#4-2-1-等价关系\" class=\"headerlink\" title=\"4.2.1 等价关系\"></a>4.2.1 等价关系</h5><p>设R为集合A上的关系，若R自反、对称且传递，则称R为A上的<strong>等价关系（equivalence relation）</strong>，记为$x～ _{R}y$或$x～y$。</p>\n<p>一个例子：整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系。</p>\n<h5 id=\"4-2-2-等价类\"><a href=\"#4-2-2-等价类\" class=\"headerlink\" title=\"4.2.2 等价类\"></a>4.2.2 等价类</h5><p>令R为A上的等价关系，对任意$a\\in A$，a关于R的<strong>等价类（equivalence class）</strong>$[a]_R$</p>\n<script type=\"math/tex; mode=display\">\n[a]_R\\equiv \\{b\\in A|aRb\\}</script><p>简记为$[a]$。</p>\n<p>一个例子：R为整数集$\\mathbb{Z}$上关于模n的同余关系为等价关系，则$[x]=\\{y \\in \\mathbb{Z}|x \\equiv y(mod\\ n)\\}=\\{x+kn|k\\in \\mathbb{Z}\\}$</p>\n<h5 id=\"4-2-3-商集\"><a href=\"#4-2-3-商集\" class=\"headerlink\" title=\"4.2.3 商集\"></a>4.2.3 商集</h5><p>设R为非空集合A上的等价关系，以R的所有等价类作为元素的集合称为A关于R的<strong>商集（quotient set）</strong>$A/R$</p>\n<script type=\"math/tex; mode=display\">\nA/R=\\{[x]_R|x\\in A\\}</script><p>例子：设$A=\\{1,2,…8\\}$，A关于模3等价关系R的商集为$A/R=\\{\\{1,4,7\\},\\{2,5,8\\},\\{3,6\\}\\}$</p>\n<h5 id=\"4-2-4-集合的划分\"><a href=\"#4-2-4-集合的划分\" class=\"headerlink\" title=\"4.2.4 集合的划分\"></a>4.2.4 集合的划分</h5><p>设A为非空集合，若A的子集族$\\Pi$（A的子集构成的集合）满足：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&(1)\\ \\emptyset \\not \\in \\Pi\n\\\\&(2)\\ (\\forall X,Y\\in \\Pi)(X\\not=Y\\to X\\cap Y=\\emptyset)\\\\\n&(3)\\ \\cup\\Pi =A\n\\end{align}</script><p>则称$\\Pi$是A的一个<strong>划分（partition）</strong>，称$\\Pi$中的元素为A的<strong>划分块（block）</strong>。</p>\n<p>对于非空集合A：</p>\n<ul>\n<li>每个商集—唯一划分</li>\n<li>等价关系—不同的划分方式</li>\n</ul>\n<h4 id=\"4-3-关系的闭包\"><a href=\"#4-3-关系的闭包\" class=\"headerlink\" title=\"4.3 关系的闭包\"></a>4.3 关系的闭包</h4><p>设$R$为集合$A$上的关系，$P$为某个性质（自反性、对称性、传递性之一），若存在$S\\subseteq A \\times A$，满足</p>\n<script type=\"math/tex; mode=display\">\n \\begin{align}\n&(1)\\ R \\subseteq  S\n\\\\&(2)\\ S具有性质P\\\\\n&(3)\\ \\forall T(R\\subseteq T \\wedge T具有性质P \\to S \\subseteq T)\n\\end{align}</script><p>则称S为<strong>相对于P的R闭包</strong>（R的P闭包）。</p>\n<p>定理：R的P闭包存在且唯一。</p>\n<h5 id=\"4-3-1-闭包的实用构造\"><a href=\"#4-3-1-闭包的实用构造\" class=\"headerlink\" title=\"4.3.1 闭包的实用构造\"></a>4.3.1 闭包的实用构造</h5><p>设$R\\subseteq A \\times A$，</p>\n<ul>\n<li>R的自反闭包$r(R)=R\\cup R^0=R\\cup I_A$</li>\n<li>R的对称闭包$s(R)=R\\cup R^{-1}$</li>\n<li>R的传递闭包$t(R)=R\\cup R^2 \\cup R^3\\cup …=\\cup \\{R^n|n\\in \\mathbb{N}^+\\}$</li>\n</ul>\n<h5 id=\"4-3-2-闭包的关系矩阵表示\"><a href=\"#4-3-2-闭包的关系矩阵表示\" class=\"headerlink\" title=\"4.3.2 闭包的关系矩阵表示\"></a>4.3.2 闭包的关系矩阵表示</h5><p>设$R\\subseteq A\\times A$且$A=\\{a_1,…,a_n\\}$，$M_R$为R的关系矩阵，则</p>\n<ul>\n<li><p>$M_{r(R)}=M_R\\vee M_{I_A}$</p>\n</li>\n<li><p>$M_{s(R)}=M_R\\vee M_R^T$</p>\n</li>\n<li><p>$M_{t(R)}=M_R\\vee M_R^{[2]}\\vee … \\vee M_R^{[n]}$，其中$M_R^{[k]}=\\underbrace{M_R\\odot M_R\\odot …\\odot M_R}_{k}$</p>\n</li>\n</ul>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>主要整理自吴楠老师《离散数学》课堂讲授</em></small></p>"},{"title":"“只要跑的足够快，时间就追不上你”","date":"2019-11-02T09:48:00.000Z","_content":"\n假设在格林威治发出一个脉冲光信号（1ms，惯性系)，与此同时（0～1ms间，惯性系）我以光速跟随光信号一起奔跑，则这1ms对我来说就是无限长。\n\n假如我站在格林威治，在我看来，1s后，脉冲信号到达南京，我看到南京收到脉冲信号的时间是+1s；\n\n但假如我与光信号一起奔跑，在南京停下，则南京收到电信号与格林威治发出是同时的，即在南京看来，收到脉冲信号这一事件发生的时刻是00s。\n\n","source":"_posts/aslongas fast enough.md","raw":"---\ntitle: “只要跑的足够快，时间就追不上你”\ndate: 2019-11-02 17:48:00\ncategories:\n- 杂想\ntags: \n- 相对论\n- 物理\n- 哲学\n---\n\n假设在格林威治发出一个脉冲光信号（1ms，惯性系)，与此同时（0～1ms间，惯性系）我以光速跟随光信号一起奔跑，则这1ms对我来说就是无限长。\n\n假如我站在格林威治，在我看来，1s后，脉冲信号到达南京，我看到南京收到脉冲信号的时间是+1s；\n\n但假如我与光信号一起奔跑，在南京停下，则南京收到电信号与格林威治发出是同时的，即在南京看来，收到脉冲信号这一事件发生的时刻是00s。\n\n","slug":"aslongas fast enough","published":1,"updated":"2020-01-30T15:28:41.446Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd69001a2sfy9i4da3wy","content":"<p>假设在格林威治发出一个脉冲光信号（1ms，惯性系)，与此同时（0～1ms间，惯性系）我以光速跟随光信号一起奔跑，则这1ms对我来说就是无限长。</p>\n<p>假如我站在格林威治，在我看来，1s后，脉冲信号到达南京，我看到南京收到脉冲信号的时间是+1s；</p>\n<p>但假如我与光信号一起奔跑，在南京停下，则南京收到电信号与格林威治发出是同时的，即在南京看来，收到脉冲信号这一事件发生的时刻是00s。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>假设在格林威治发出一个脉冲光信号（1ms，惯性系)，与此同时（0～1ms间，惯性系）我以光速跟随光信号一起奔跑，则这1ms对我来说就是无限长。</p>\n<p>假如我站在格林威治，在我看来，1s后，脉冲信号到达南京，我看到南京收到脉冲信号的时间是+1s；</p>\n<p>但假如我与光信号一起奔跑，在南京停下，则南京收到电信号与格林威治发出是同时的，即在南京看来，收到脉冲信号这一事件发生的时刻是00s。</p>\n"},{"title":"界限与无限","date":"2020-01-28T09:02:00.000Z","_content":"\n王小波说，有两种知识分子，一种想做一个如来佛，让别人永世跳不出他的手掌心；另一种是想倾其一生跳出如来佛的手掌心。几千年来，中国哲人们制定出的伦理纲常，都属于前一种——孔子想出来一套做人准则，招募弟子来受教，弟子学会了师父的这套理论，再推己及人。<!--more-->\n\n中国的古代哲学和宗教类似，都有一套不可亵渎的圣典，一个绝对的神圣实体。基督教有《圣经》、伊斯兰教有《古兰经》、印度教有《吠陀经》，且都有一个对应的神。孔子虽然是肉体凡胎，但中国人认为他是“圣人”。既然是圣人，就跟你我不同，圣人已经对所有的准则作出规定，后人只要照着圣人的思想行事就可以了。同样，宗教信徒们要对神绝对信仰，遵照神的旨意行事。于是，人的智慧也就被框在一本书里，对欧洲人是这一本，对中东人是那一本，中国人也有另一本。对于一个人来说，他生下来要怎么做，怎么想，早已经有本书替他规定好，只要照着做就行了。倘若不愿意，就面临着一些风险，在中国，容易背上破坏纲常的骂名，再坏一些时做不了人，只能被当作禽兽看待；在中世纪的欧洲，可能还会有生命危险。危险的大小很大程度与“神圣”的权力相关，当秦始皇罢黜百家，独尊儒术时、当政教合一时，反叛就意味着更大的风险。\n\n除了统治者的暴力威胁，人还有天然的崇古倾向。这使得大多数人想不到跳出如来佛的手掌心。中医据说起源于五千多年前黄帝时期，后来又和《易经》相伴相生。阴阳五行把世间万事万物囊括其中，于是也就没有什么新的知识可以发现了——一切道理先人已经知晓，后人只需去故纸堆里寻找解释就够了。\n\n有一个形象的比喻：不论现代科学取得了多大的成就——爱因斯坦发现没有绝对的时间，沃森克里克揭示遗传的本质，原子的裂变可以释放足以灭绝人类的能量，一只蝴蝶在巴西扇动翅膀，可以导致一个月后德克萨斯州的一场龙卷风……——神圣的祖先永远在山顶等着所有科学信徒的皈依——尽管这样一个山顶可能根本不存在，因为科学本身就有关于无尽的探索。","source":"_posts/boundary&infinity.md","raw":"---\ntitle: 界限与无限\ndate: 2020-01-28 17:02:00\ncategories:\n- 杂想\ntags: \n- 宗教\n- 哲学\n---\n\n王小波说，有两种知识分子，一种想做一个如来佛，让别人永世跳不出他的手掌心；另一种是想倾其一生跳出如来佛的手掌心。几千年来，中国哲人们制定出的伦理纲常，都属于前一种——孔子想出来一套做人准则，招募弟子来受教，弟子学会了师父的这套理论，再推己及人。<!--more-->\n\n中国的古代哲学和宗教类似，都有一套不可亵渎的圣典，一个绝对的神圣实体。基督教有《圣经》、伊斯兰教有《古兰经》、印度教有《吠陀经》，且都有一个对应的神。孔子虽然是肉体凡胎，但中国人认为他是“圣人”。既然是圣人，就跟你我不同，圣人已经对所有的准则作出规定，后人只要照着圣人的思想行事就可以了。同样，宗教信徒们要对神绝对信仰，遵照神的旨意行事。于是，人的智慧也就被框在一本书里，对欧洲人是这一本，对中东人是那一本，中国人也有另一本。对于一个人来说，他生下来要怎么做，怎么想，早已经有本书替他规定好，只要照着做就行了。倘若不愿意，就面临着一些风险，在中国，容易背上破坏纲常的骂名，再坏一些时做不了人，只能被当作禽兽看待；在中世纪的欧洲，可能还会有生命危险。危险的大小很大程度与“神圣”的权力相关，当秦始皇罢黜百家，独尊儒术时、当政教合一时，反叛就意味着更大的风险。\n\n除了统治者的暴力威胁，人还有天然的崇古倾向。这使得大多数人想不到跳出如来佛的手掌心。中医据说起源于五千多年前黄帝时期，后来又和《易经》相伴相生。阴阳五行把世间万事万物囊括其中，于是也就没有什么新的知识可以发现了——一切道理先人已经知晓，后人只需去故纸堆里寻找解释就够了。\n\n有一个形象的比喻：不论现代科学取得了多大的成就——爱因斯坦发现没有绝对的时间，沃森克里克揭示遗传的本质，原子的裂变可以释放足以灭绝人类的能量，一只蝴蝶在巴西扇动翅膀，可以导致一个月后德克萨斯州的一场龙卷风……——神圣的祖先永远在山顶等着所有科学信徒的皈依——尽管这样一个山顶可能根本不存在，因为科学本身就有关于无尽的探索。","slug":"boundary&infinity","published":1,"updated":"2021-08-26T13:15:11.711Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6b001e2sfy0wtqc7pv","content":"<p>王小波说，有两种知识分子，一种想做一个如来佛，让别人永世跳不出他的手掌心；另一种是想倾其一生跳出如来佛的手掌心。几千年来，中国哲人们制定出的伦理纲常，都属于前一种——孔子想出来一套做人准则，招募弟子来受教，弟子学会了师父的这套理论，再推己及人。<a id=\"more\"></a></p>\n<p>中国的古代哲学和宗教类似，都有一套不可亵渎的圣典，一个绝对的神圣实体。基督教有《圣经》、伊斯兰教有《古兰经》、印度教有《吠陀经》，且都有一个对应的神。孔子虽然是肉体凡胎，但中国人认为他是“圣人”。既然是圣人，就跟你我不同，圣人已经对所有的准则作出规定，后人只要照着圣人的思想行事就可以了。同样，宗教信徒们要对神绝对信仰，遵照神的旨意行事。于是，人的智慧也就被框在一本书里，对欧洲人是这一本，对中东人是那一本，中国人也有另一本。对于一个人来说，他生下来要怎么做，怎么想，早已经有本书替他规定好，只要照着做就行了。倘若不愿意，就面临着一些风险，在中国，容易背上破坏纲常的骂名，再坏一些时做不了人，只能被当作禽兽看待；在中世纪的欧洲，可能还会有生命危险。危险的大小很大程度与“神圣”的权力相关，当秦始皇罢黜百家，独尊儒术时、当政教合一时，反叛就意味着更大的风险。</p>\n<p>除了统治者的暴力威胁，人还有天然的崇古倾向。这使得大多数人想不到跳出如来佛的手掌心。中医据说起源于五千多年前黄帝时期，后来又和《易经》相伴相生。阴阳五行把世间万事万物囊括其中，于是也就没有什么新的知识可以发现了——一切道理先人已经知晓，后人只需去故纸堆里寻找解释就够了。</p>\n<p>有一个形象的比喻：不论现代科学取得了多大的成就——爱因斯坦发现没有绝对的时间，沃森克里克揭示遗传的本质，原子的裂变可以释放足以灭绝人类的能量，一只蝴蝶在巴西扇动翅膀，可以导致一个月后德克萨斯州的一场龙卷风……——神圣的祖先永远在山顶等着所有科学信徒的皈依——尽管这样一个山顶可能根本不存在，因为科学本身就有关于无尽的探索。</p>\n","site":{"data":{}},"excerpt":"<p>王小波说，有两种知识分子，一种想做一个如来佛，让别人永世跳不出他的手掌心；另一种是想倾其一生跳出如来佛的手掌心。几千年来，中国哲人们制定出的伦理纲常，都属于前一种——孔子想出来一套做人准则，招募弟子来受教，弟子学会了师父的这套理论，再推己及人。","more":"</p>\n<p>中国的古代哲学和宗教类似，都有一套不可亵渎的圣典，一个绝对的神圣实体。基督教有《圣经》、伊斯兰教有《古兰经》、印度教有《吠陀经》，且都有一个对应的神。孔子虽然是肉体凡胎，但中国人认为他是“圣人”。既然是圣人，就跟你我不同，圣人已经对所有的准则作出规定，后人只要照着圣人的思想行事就可以了。同样，宗教信徒们要对神绝对信仰，遵照神的旨意行事。于是，人的智慧也就被框在一本书里，对欧洲人是这一本，对中东人是那一本，中国人也有另一本。对于一个人来说，他生下来要怎么做，怎么想，早已经有本书替他规定好，只要照着做就行了。倘若不愿意，就面临着一些风险，在中国，容易背上破坏纲常的骂名，再坏一些时做不了人，只能被当作禽兽看待；在中世纪的欧洲，可能还会有生命危险。危险的大小很大程度与“神圣”的权力相关，当秦始皇罢黜百家，独尊儒术时、当政教合一时，反叛就意味着更大的风险。</p>\n<p>除了统治者的暴力威胁，人还有天然的崇古倾向。这使得大多数人想不到跳出如来佛的手掌心。中医据说起源于五千多年前黄帝时期，后来又和《易经》相伴相生。阴阳五行把世间万事万物囊括其中，于是也就没有什么新的知识可以发现了——一切道理先人已经知晓，后人只需去故纸堆里寻找解释就够了。</p>\n<p>有一个形象的比喻：不论现代科学取得了多大的成就——爱因斯坦发现没有绝对的时间，沃森克里克揭示遗传的本质，原子的裂变可以释放足以灭绝人类的能量，一只蝴蝶在巴西扇动翅膀，可以导致一个月后德克萨斯州的一场龙卷风……——神圣的祖先永远在山顶等着所有科学信徒的皈依——尽管这样一个山顶可能根本不存在，因为科学本身就有关于无尽的探索。</p>"},{"title":"集合论与函数","date":"2020-11-05T03:09:00.000Z","mathjax":true,"_content":"\n\n\n拉丁文（functio），含义为the action of performance。函数最重要的性质是**决定性**：同一输入只对应同一输出。<!--more-->\n\n### 一、函数的定义\n\n函数的现代定义（19世纪末，Dirichlet）：在集合论基础上，将函数视为关系的特例，特别之处就是决定性。设F为二元关系，F为函数是指\n$$\n(\\forall x,y,z)(xFy\\wedge xFz\\to y=z)\n$$\nF为函数等价于\n$$\nx \\in dom(F)\\to \\exists y!, \\ F(x)=y\n$$\n空关系也是函数。\n\n#### 函数的外延原则\n\n设F，G为函数，则\n$$\nF=G \\leftrightarrow [Dom(F)=Dom(G)\\wedge (\\forall x \\in Dom(F))(F(x)=G(x))]\n$$\n\n#### 函数的集合\n\n* ##### 定义域、值域、陪域\n\n  设A，B为集合，F为从A到B的函数（记为$F:A\\to B$），且$Dom(F)=A$，$Ran(F)\\subseteq B$。则称A为函数F的定义域，$Ran(F)$为函数F的值域，B为函数F的陪域（codomain）。\n  \n* ##### 函数的集合\n\n  记$B^A$为A到B的所有函数的集合，即$\\{F|F:A\\to B\\}$，读作”B上A“。\n\n* ##### 满射、单射与双射\n\n  满射（onto）：值域等于陪域，$Ran(F)=B$\n\n  单射（1-1）：$(\\forall x,y \\in A)(f(x)=f(y)\\to x=y)$\n\n  双射：(1-1 correspondence）：满射且单射。\n\n### 二、函数的性质\n\n### 三、函数的复合\n\n* #### 函数的复合\n\n  设F和G是函数，则$F\\circ G$也是函数，且满足\n  $$\n  (1) \\ Dom(F \\circ G)=\\{x|x \\in Dom(G) \\wedge G(x)\\in Dom(F)\\}\\\\\n  (2)\\ \\forall x \\in Dom(F\\circ G),有F\\circ G(X)=F(G(x))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n  $$\n  \n* #### 复合函数的性质\n\n  \n\n### 四、反函数\n\n#### 函数的逆关系\n\n函数的逆关系不一定是函数，可能只是普通的二元关系。\n\n对于单射函数$f:A\\to B$，逆关系$f^{-1}$是函数，是$Ran(f)\\to A$的双射函数，但不一定是$B\\to A$的双射函数；\n\n对于满射函数$f:A\\to B$，逆关系$f^{-1}$不是函数；\n\n对于双射函数$f:A\\to B$，逆关系$f^{-1}$是$B\\to A$的双射函数。\n\n#### 反函数\n\n对于$f$是$A\\to B$的双射函数的情况，我们称逆关系$f^{-1}$是$f$的反函数。\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*主要整理自吴楠老师《离散数学》课堂讲授*</small>","source":"_posts/function.md","raw":"---\ntitle: 集合论与函数 \ndate: 2020-11-05 11:09:00\ncategories:\n- 数理逻辑\ntags: \n- 数学\n- 集合论\nmathjax: true\n---\n\n\n\n拉丁文（functio），含义为the action of performance。函数最重要的性质是**决定性**：同一输入只对应同一输出。<!--more-->\n\n### 一、函数的定义\n\n函数的现代定义（19世纪末，Dirichlet）：在集合论基础上，将函数视为关系的特例，特别之处就是决定性。设F为二元关系，F为函数是指\n$$\n(\\forall x,y,z)(xFy\\wedge xFz\\to y=z)\n$$\nF为函数等价于\n$$\nx \\in dom(F)\\to \\exists y!, \\ F(x)=y\n$$\n空关系也是函数。\n\n#### 函数的外延原则\n\n设F，G为函数，则\n$$\nF=G \\leftrightarrow [Dom(F)=Dom(G)\\wedge (\\forall x \\in Dom(F))(F(x)=G(x))]\n$$\n\n#### 函数的集合\n\n* ##### 定义域、值域、陪域\n\n  设A，B为集合，F为从A到B的函数（记为$F:A\\to B$），且$Dom(F)=A$，$Ran(F)\\subseteq B$。则称A为函数F的定义域，$Ran(F)$为函数F的值域，B为函数F的陪域（codomain）。\n  \n* ##### 函数的集合\n\n  记$B^A$为A到B的所有函数的集合，即$\\{F|F:A\\to B\\}$，读作”B上A“。\n\n* ##### 满射、单射与双射\n\n  满射（onto）：值域等于陪域，$Ran(F)=B$\n\n  单射（1-1）：$(\\forall x,y \\in A)(f(x)=f(y)\\to x=y)$\n\n  双射：(1-1 correspondence）：满射且单射。\n\n### 二、函数的性质\n\n### 三、函数的复合\n\n* #### 函数的复合\n\n  设F和G是函数，则$F\\circ G$也是函数，且满足\n  $$\n  (1) \\ Dom(F \\circ G)=\\{x|x \\in Dom(G) \\wedge G(x)\\in Dom(F)\\}\\\\\n  (2)\\ \\forall x \\in Dom(F\\circ G),有F\\circ G(X)=F(G(x))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n  $$\n  \n* #### 复合函数的性质\n\n  \n\n### 四、反函数\n\n#### 函数的逆关系\n\n函数的逆关系不一定是函数，可能只是普通的二元关系。\n\n对于单射函数$f:A\\to B$，逆关系$f^{-1}$是函数，是$Ran(f)\\to A$的双射函数，但不一定是$B\\to A$的双射函数；\n\n对于满射函数$f:A\\to B$，逆关系$f^{-1}$不是函数；\n\n对于双射函数$f:A\\to B$，逆关系$f^{-1}$是$B\\to A$的双射函数。\n\n#### 反函数\n\n对于$f$是$A\\to B$的双射函数的情况，我们称逆关系$f^{-1}$是$f$的反函数。\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*主要整理自吴楠老师《离散数学》课堂讲授*</small>","slug":"function","published":1,"updated":"2020-11-19T03:07:41.558Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6d001i2sfy2egu11bo","content":"<p>拉丁文（functio），含义为the action of performance。函数最重要的性质是<strong>决定性</strong>：同一输入只对应同一输出。<a id=\"more\"></a></p>\n<h3 id=\"一、函数的定义\"><a href=\"#一、函数的定义\" class=\"headerlink\" title=\"一、函数的定义\"></a>一、函数的定义</h3><p>函数的现代定义（19世纪末，Dirichlet）：在集合论基础上，将函数视为关系的特例，特别之处就是决定性。设F为二元关系，F为函数是指</p>\n<script type=\"math/tex; mode=display\">\n(\\forall x,y,z)(xFy\\wedge xFz\\to y=z)</script><p>F为函数等价于</p>\n<script type=\"math/tex; mode=display\">\nx \\in dom(F)\\to \\exists y!, \\ F(x)=y</script><p>空关系也是函数。</p>\n<h4 id=\"函数的外延原则\"><a href=\"#函数的外延原则\" class=\"headerlink\" title=\"函数的外延原则\"></a>函数的外延原则</h4><p>设F，G为函数，则</p>\n<script type=\"math/tex; mode=display\">\nF=G \\leftrightarrow [Dom(F)=Dom(G)\\wedge (\\forall x \\in Dom(F))(F(x)=G(x))]</script><h4 id=\"函数的集合\"><a href=\"#函数的集合\" class=\"headerlink\" title=\"函数的集合\"></a>函数的集合</h4><ul>\n<li><h5 id=\"定义域、值域、陪域\"><a href=\"#定义域、值域、陪域\" class=\"headerlink\" title=\"定义域、值域、陪域\"></a>定义域、值域、陪域</h5><p>设A，B为集合，F为从A到B的函数（记为$F:A\\to B$），且$Dom(F)=A$，$Ran(F)\\subseteq B$。则称A为函数F的定义域，$Ran(F)$为函数F的值域，B为函数F的陪域（codomain）。</p>\n</li>\n<li><h5 id=\"函数的集合-1\"><a href=\"#函数的集合-1\" class=\"headerlink\" title=\"函数的集合\"></a>函数的集合</h5><p>记$B^A$为A到B的所有函数的集合，即$\\{F|F:A\\to B\\}$，读作”B上A“。</p>\n</li>\n<li><h5 id=\"满射、单射与双射\"><a href=\"#满射、单射与双射\" class=\"headerlink\" title=\"满射、单射与双射\"></a>满射、单射与双射</h5><p>满射（onto）：值域等于陪域，$Ran(F)=B$</p>\n<p>单射（1-1）：$(\\forall x,y \\in A)(f(x)=f(y)\\to x=y)$</p>\n<p>双射：(1-1 correspondence）：满射且单射。</p>\n</li>\n</ul>\n<h3 id=\"二、函数的性质\"><a href=\"#二、函数的性质\" class=\"headerlink\" title=\"二、函数的性质\"></a>二、函数的性质</h3><h3 id=\"三、函数的复合\"><a href=\"#三、函数的复合\" class=\"headerlink\" title=\"三、函数的复合\"></a>三、函数的复合</h3><ul>\n<li><h4 id=\"函数的复合\"><a href=\"#函数的复合\" class=\"headerlink\" title=\"函数的复合\"></a>函数的复合</h4><p>设F和G是函数，则$F\\circ G$也是函数，且满足</p>\n<script type=\"math/tex; mode=display\">\n(1) \\ Dom(F \\circ G)=\\{x|x \\in Dom(G) \\wedge G(x)\\in Dom(F)\\}\\\\\n(2)\\ \\forall x \\in Dom(F\\circ G),有F\\circ G(X)=F(G(x))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\</script></li>\n<li><h4 id=\"复合函数的性质\"><a href=\"#复合函数的性质\" class=\"headerlink\" title=\"复合函数的性质\"></a>复合函数的性质</h4></li>\n</ul>\n<h3 id=\"四、反函数\"><a href=\"#四、反函数\" class=\"headerlink\" title=\"四、反函数\"></a>四、反函数</h3><h4 id=\"函数的逆关系\"><a href=\"#函数的逆关系\" class=\"headerlink\" title=\"函数的逆关系\"></a>函数的逆关系</h4><p>函数的逆关系不一定是函数，可能只是普通的二元关系。</p>\n<p>对于单射函数$f:A\\to B$，逆关系$f^{-1}$是函数，是$Ran(f)\\to A$的双射函数，但不一定是$B\\to A$的双射函数；</p>\n<p>对于满射函数$f:A\\to B$，逆关系$f^{-1}$不是函数；</p>\n<p>对于双射函数$f:A\\to B$，逆关系$f^{-1}$是$B\\to A$的双射函数。</p>\n<h4 id=\"反函数\"><a href=\"#反函数\" class=\"headerlink\" title=\"反函数\"></a>反函数</h4><p>对于$f$是$A\\to B$的双射函数的情况，我们称逆关系$f^{-1}$是$f$的反函数。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>主要整理自吴楠老师《离散数学》课堂讲授</em></small></p>\n","site":{"data":{}},"excerpt":"<p>拉丁文（functio），含义为the action of performance。函数最重要的性质是<strong>决定性</strong>：同一输入只对应同一输出。","more":"</p>\n<h3 id=\"一、函数的定义\"><a href=\"#一、函数的定义\" class=\"headerlink\" title=\"一、函数的定义\"></a>一、函数的定义</h3><p>函数的现代定义（19世纪末，Dirichlet）：在集合论基础上，将函数视为关系的特例，特别之处就是决定性。设F为二元关系，F为函数是指</p>\n<script type=\"math/tex; mode=display\">\n(\\forall x,y,z)(xFy\\wedge xFz\\to y=z)</script><p>F为函数等价于</p>\n<script type=\"math/tex; mode=display\">\nx \\in dom(F)\\to \\exists y!, \\ F(x)=y</script><p>空关系也是函数。</p>\n<h4 id=\"函数的外延原则\"><a href=\"#函数的外延原则\" class=\"headerlink\" title=\"函数的外延原则\"></a>函数的外延原则</h4><p>设F，G为函数，则</p>\n<script type=\"math/tex; mode=display\">\nF=G \\leftrightarrow [Dom(F)=Dom(G)\\wedge (\\forall x \\in Dom(F))(F(x)=G(x))]</script><h4 id=\"函数的集合\"><a href=\"#函数的集合\" class=\"headerlink\" title=\"函数的集合\"></a>函数的集合</h4><ul>\n<li><h5 id=\"定义域、值域、陪域\"><a href=\"#定义域、值域、陪域\" class=\"headerlink\" title=\"定义域、值域、陪域\"></a>定义域、值域、陪域</h5><p>设A，B为集合，F为从A到B的函数（记为$F:A\\to B$），且$Dom(F)=A$，$Ran(F)\\subseteq B$。则称A为函数F的定义域，$Ran(F)$为函数F的值域，B为函数F的陪域（codomain）。</p>\n</li>\n<li><h5 id=\"函数的集合-1\"><a href=\"#函数的集合-1\" class=\"headerlink\" title=\"函数的集合\"></a>函数的集合</h5><p>记$B^A$为A到B的所有函数的集合，即$\\{F|F:A\\to B\\}$，读作”B上A“。</p>\n</li>\n<li><h5 id=\"满射、单射与双射\"><a href=\"#满射、单射与双射\" class=\"headerlink\" title=\"满射、单射与双射\"></a>满射、单射与双射</h5><p>满射（onto）：值域等于陪域，$Ran(F)=B$</p>\n<p>单射（1-1）：$(\\forall x,y \\in A)(f(x)=f(y)\\to x=y)$</p>\n<p>双射：(1-1 correspondence）：满射且单射。</p>\n</li>\n</ul>\n<h3 id=\"二、函数的性质\"><a href=\"#二、函数的性质\" class=\"headerlink\" title=\"二、函数的性质\"></a>二、函数的性质</h3><h3 id=\"三、函数的复合\"><a href=\"#三、函数的复合\" class=\"headerlink\" title=\"三、函数的复合\"></a>三、函数的复合</h3><ul>\n<li><h4 id=\"函数的复合\"><a href=\"#函数的复合\" class=\"headerlink\" title=\"函数的复合\"></a>函数的复合</h4><p>设F和G是函数，则$F\\circ G$也是函数，且满足</p>\n<script type=\"math/tex; mode=display\">\n(1) \\ Dom(F \\circ G)=\\{x|x \\in Dom(G) \\wedge G(x)\\in Dom(F)\\}\\\\\n(2)\\ \\forall x \\in Dom(F\\circ G),有F\\circ G(X)=F(G(x))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\</script></li>\n<li><h4 id=\"复合函数的性质\"><a href=\"#复合函数的性质\" class=\"headerlink\" title=\"复合函数的性质\"></a>复合函数的性质</h4></li>\n</ul>\n<h3 id=\"四、反函数\"><a href=\"#四、反函数\" class=\"headerlink\" title=\"四、反函数\"></a>四、反函数</h3><h4 id=\"函数的逆关系\"><a href=\"#函数的逆关系\" class=\"headerlink\" title=\"函数的逆关系\"></a>函数的逆关系</h4><p>函数的逆关系不一定是函数，可能只是普通的二元关系。</p>\n<p>对于单射函数$f:A\\to B$，逆关系$f^{-1}$是函数，是$Ran(f)\\to A$的双射函数，但不一定是$B\\to A$的双射函数；</p>\n<p>对于满射函数$f:A\\to B$，逆关系$f^{-1}$不是函数；</p>\n<p>对于双射函数$f:A\\to B$，逆关系$f^{-1}$是$B\\to A$的双射函数。</p>\n<h4 id=\"反函数\"><a href=\"#反函数\" class=\"headerlink\" title=\"反函数\"></a>反函数</h4><p>对于$f$是$A\\to B$的双射函数的情况，我们称逆关系$f^{-1}$是$f$的反函数。</p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em>主要整理自吴楠老师《离散数学》课堂讲授</em></small></p>"},{"title":"如何预报抛硬币","date":"2021-08-12T10:19:00.000Z","mathjax":true,"_content":"\n\n\n抛10次硬币，已经抛了9次结果都是正面朝上，下一次是正面还是反面？<!--more-->\n\n### 古典概率\n\n“下一次正面和反面的概率各是0.5。因为前面不论扔了多少次硬币，其结果不会对下一次扔硬币产生影响。所以第10次扔硬币正面的概率仍是一半。”\n\n这是很“标准”的一个答案。听起来很简单，却很有道理。这个答案背后的假设，正是古典概率的基本假设。法国数学家拉普拉斯（Laplace）提出：若一个随机试验可能的结果是有限的，且**每个结果发生的可能性均等**，这所有可能结果构成一个样本空间$S$，则事件$E$发生的概率为\n$$\nP(E)=\\frac{E包含的结果数}{S包含的结果数}\n$$\n对于抛硬币这样一个最简单的二元结果的事件，若只抛一次，样本空间$S=\\{1，0\\}$（以1代表正面向上，0代表反面向上）。那么正面朝上的概率$P(\\{1\\})=\\frac{1}{1+1}=\\frac{1}{2}$，同理反面朝上的概率$P(\\{0\\})=\\frac{1}{1+1}=\\frac{1}{2}$。\n\n回到最开始的问题，若抛10次硬币，已经抛了9次结果都是正面朝上，下一次抛正面向上的概率是多少？这是一个条件概率问题。记事件$T=t$为前$t$次都抛出了正面，事件$U$为下一次抛出正面，则所求事件的概率表示为$P(U|T=9)$，这里再引入一个假设：每次抛的结果都是**独立**的，才能得到\n$$\nP(U|T=9)=P(U)=\\frac{1}{2}\n$$\n现在换个问题，若抛10000次硬币，前9999次结果都是正面朝上，下一次正面的概率还会是0.5吗？要知道，连续抛9999次硬币都为正面的概率为$(\\frac{1}{2})^{9999}$，比1767年普莱斯用贝叶斯论文算出的基督复活的概率还要小（后者大于1/1600000的概率为0.535）。这时候，如果请你预测下一次抛硬币的结果，你是否会动摇？是哪里出了问题呢。\n\n### 贝叶斯学派\n\n为什么掷了9999次都是正面向上？是不是这枚硬币有什么问题？难道它两面都是正面？当你开始怀疑硬币正面朝上与反面朝上的概率是否相等时，就进入了贝叶斯学派的思考领域。\n\n贝叶斯学派认为，我们假设的“**硬币掷出正反面的可能性均等**”作为先验知识并不是确定的。一切**先验概率随时都可以因观测到新的事件结果而修正**，得到后验概率。这在现实中其实非常常见，比如描述大气运动的N-S方程在实际的预报模式中都会被简化，无法精确描述大气状态。方程是理想而美好的，但现实却很粗糙，所以需要不断调整对模式预报的置信度。相比较古典概率，贝叶斯学派更接地气，如果说古典概率是古希腊柏拉图式哲学一脉相承的产物，那么贝叶斯理论则属于实干家。\n\n记$H$是进行一次随机试验可能的结果，$E$是已经发生的事件，那么事件$H$的概率$P(H)$可以由观测$E$而更新：\n$$\nP(H|E) = \\frac{P(E|H)P(H)}{P(E)}\n$$\n其中$P(H)$是先验概率，$P(E|H)$是在先验概率假设下事件$E$发生的条件概率，$P(E)$是所有可能情况下$E$发生的概率。\n\n抛9999次硬币朝上后，下一次朝上的概率用贝叶斯公式计算为\n$$\nP(U|T=9999) = \\frac{P(T=9999|U)P(U)}{P(T=9999)}\n$$\n$P(U)$是假设的先验概率等于$\\frac{1}{2}$，在先验假设下前9999次正面朝上的概率$P(T=9999|U)=(\\frac{1}{2})^{9999}$，假如硬币真的是理想硬币，那$P(T=9999)$也等于$(\\frac{1}{2})^{9999}$，用贝叶斯公式算出来的后验概率$P(U|T=9999)=\\frac{1}{2}$，与古典概率一致；但很可能这枚硬币存在某种瑕疵，使得掷出正面向上的可能性远远大于反面，实际的$P(T=9999)$可能接近1（假设是0.8）的话，那么后验概率$P(U|T=9999)=\\frac{(\\frac{1}{2})^{9999}·\\frac{1}{2}}{0.8}$就接近0了。\n\n### 概率的概率：Deterministic vs Stochastic\n\n1767年普莱斯用贝叶斯论文（图1）中的方法计算了基督复活的可能性，用来反驳休谟的《论神迹》（休谟认为你可以因为神迹信宗教，但基督复活，算神话？），他给出的是“基督复活概率大于1/1600000的概率”，也就是说，概率的概率。\n\n<img src=\"/images/bayes.png\" alt=\"bayes\" style=\"zoom:60%;\" />\n\n<center><small>图1. 普莱斯选定的贝叶斯论文单行本标题页(Watson 2013)</small></center>\n\n这其实是另外一个重要观念的引入。之前计算硬币朝上事件发生的概率，我们给出了一个唯一的值$P(U)$，但如果这个概率本身就有不确定性呢？比如事先并不知道硬币正面朝上的概率，只知道硬币抛出正面概率可能为0.5，也可能为0.8，哪个可能性更大？如何描述这种可能性呢？事实上，事件$U$的概率$P(U)$可以看作一个特殊的参数，“概率的概率”可以推广到”任意未知参数的概率“，当我们无法确定一个参数的值时，就可以用概率分布描述它。这是从确定性（Deterministic）估计到随机性（Stochastic）估计的转变。\n\n记$\\theta = P(U)$，则同样的抛硬币问题用贝叶斯公式解为\n$$\nP(\\theta|T=9999) = \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\n$$\n此时先验概率$P(\\theta)$有一个假设的概率分布，对于抛硬币过程一般用$\\beta$分布来描述\n$$\n\\begin{align}\nBeta(\\theta;a,b)&=\\frac{1}{B(a,b)}\\theta ^{a-1}(1-\\theta)^{b-1}\\\\\nB(a,b)&=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\end{align}\n$$\n那么\n$$\n\\begin{align}\nP(\\theta|T=9999) &= \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\\\\\n &= \\frac{1}{B(a+T,b) }\\theta ^{a+T-1}(1-\\theta)^{\\beta -1} \\notag \\\\\n &= Beta(\\theta;a+T,b) \\notag \n \\end{align}\n$$\n后验概率也是一个概率分布。\n\n<br/><br/><br/>\n\n","source":"_posts/how_to_predict_a_coin_flip.md","raw":"---\ntitle: 如何预报抛硬币\ndate: 2021-08-12 18:19:00\ncategories:\n- 统计\ntags: \n- 数学\n- 统计\nmathjax: true\n\n---\n\n\n\n抛10次硬币，已经抛了9次结果都是正面朝上，下一次是正面还是反面？<!--more-->\n\n### 古典概率\n\n“下一次正面和反面的概率各是0.5。因为前面不论扔了多少次硬币，其结果不会对下一次扔硬币产生影响。所以第10次扔硬币正面的概率仍是一半。”\n\n这是很“标准”的一个答案。听起来很简单，却很有道理。这个答案背后的假设，正是古典概率的基本假设。法国数学家拉普拉斯（Laplace）提出：若一个随机试验可能的结果是有限的，且**每个结果发生的可能性均等**，这所有可能结果构成一个样本空间$S$，则事件$E$发生的概率为\n$$\nP(E)=\\frac{E包含的结果数}{S包含的结果数}\n$$\n对于抛硬币这样一个最简单的二元结果的事件，若只抛一次，样本空间$S=\\{1，0\\}$（以1代表正面向上，0代表反面向上）。那么正面朝上的概率$P(\\{1\\})=\\frac{1}{1+1}=\\frac{1}{2}$，同理反面朝上的概率$P(\\{0\\})=\\frac{1}{1+1}=\\frac{1}{2}$。\n\n回到最开始的问题，若抛10次硬币，已经抛了9次结果都是正面朝上，下一次抛正面向上的概率是多少？这是一个条件概率问题。记事件$T=t$为前$t$次都抛出了正面，事件$U$为下一次抛出正面，则所求事件的概率表示为$P(U|T=9)$，这里再引入一个假设：每次抛的结果都是**独立**的，才能得到\n$$\nP(U|T=9)=P(U)=\\frac{1}{2}\n$$\n现在换个问题，若抛10000次硬币，前9999次结果都是正面朝上，下一次正面的概率还会是0.5吗？要知道，连续抛9999次硬币都为正面的概率为$(\\frac{1}{2})^{9999}$，比1767年普莱斯用贝叶斯论文算出的基督复活的概率还要小（后者大于1/1600000的概率为0.535）。这时候，如果请你预测下一次抛硬币的结果，你是否会动摇？是哪里出了问题呢。\n\n### 贝叶斯学派\n\n为什么掷了9999次都是正面向上？是不是这枚硬币有什么问题？难道它两面都是正面？当你开始怀疑硬币正面朝上与反面朝上的概率是否相等时，就进入了贝叶斯学派的思考领域。\n\n贝叶斯学派认为，我们假设的“**硬币掷出正反面的可能性均等**”作为先验知识并不是确定的。一切**先验概率随时都可以因观测到新的事件结果而修正**，得到后验概率。这在现实中其实非常常见，比如描述大气运动的N-S方程在实际的预报模式中都会被简化，无法精确描述大气状态。方程是理想而美好的，但现实却很粗糙，所以需要不断调整对模式预报的置信度。相比较古典概率，贝叶斯学派更接地气，如果说古典概率是古希腊柏拉图式哲学一脉相承的产物，那么贝叶斯理论则属于实干家。\n\n记$H$是进行一次随机试验可能的结果，$E$是已经发生的事件，那么事件$H$的概率$P(H)$可以由观测$E$而更新：\n$$\nP(H|E) = \\frac{P(E|H)P(H)}{P(E)}\n$$\n其中$P(H)$是先验概率，$P(E|H)$是在先验概率假设下事件$E$发生的条件概率，$P(E)$是所有可能情况下$E$发生的概率。\n\n抛9999次硬币朝上后，下一次朝上的概率用贝叶斯公式计算为\n$$\nP(U|T=9999) = \\frac{P(T=9999|U)P(U)}{P(T=9999)}\n$$\n$P(U)$是假设的先验概率等于$\\frac{1}{2}$，在先验假设下前9999次正面朝上的概率$P(T=9999|U)=(\\frac{1}{2})^{9999}$，假如硬币真的是理想硬币，那$P(T=9999)$也等于$(\\frac{1}{2})^{9999}$，用贝叶斯公式算出来的后验概率$P(U|T=9999)=\\frac{1}{2}$，与古典概率一致；但很可能这枚硬币存在某种瑕疵，使得掷出正面向上的可能性远远大于反面，实际的$P(T=9999)$可能接近1（假设是0.8）的话，那么后验概率$P(U|T=9999)=\\frac{(\\frac{1}{2})^{9999}·\\frac{1}{2}}{0.8}$就接近0了。\n\n### 概率的概率：Deterministic vs Stochastic\n\n1767年普莱斯用贝叶斯论文（图1）中的方法计算了基督复活的可能性，用来反驳休谟的《论神迹》（休谟认为你可以因为神迹信宗教，但基督复活，算神话？），他给出的是“基督复活概率大于1/1600000的概率”，也就是说，概率的概率。\n\n<img src=\"/images/bayes.png\" alt=\"bayes\" style=\"zoom:60%;\" />\n\n<center><small>图1. 普莱斯选定的贝叶斯论文单行本标题页(Watson 2013)</small></center>\n\n这其实是另外一个重要观念的引入。之前计算硬币朝上事件发生的概率，我们给出了一个唯一的值$P(U)$，但如果这个概率本身就有不确定性呢？比如事先并不知道硬币正面朝上的概率，只知道硬币抛出正面概率可能为0.5，也可能为0.8，哪个可能性更大？如何描述这种可能性呢？事实上，事件$U$的概率$P(U)$可以看作一个特殊的参数，“概率的概率”可以推广到”任意未知参数的概率“，当我们无法确定一个参数的值时，就可以用概率分布描述它。这是从确定性（Deterministic）估计到随机性（Stochastic）估计的转变。\n\n记$\\theta = P(U)$，则同样的抛硬币问题用贝叶斯公式解为\n$$\nP(\\theta|T=9999) = \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\n$$\n此时先验概率$P(\\theta)$有一个假设的概率分布，对于抛硬币过程一般用$\\beta$分布来描述\n$$\n\\begin{align}\nBeta(\\theta;a,b)&=\\frac{1}{B(a,b)}\\theta ^{a-1}(1-\\theta)^{b-1}\\\\\nB(a,b)&=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\end{align}\n$$\n那么\n$$\n\\begin{align}\nP(\\theta|T=9999) &= \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\\\\\n &= \\frac{1}{B(a+T,b) }\\theta ^{a+T-1}(1-\\theta)^{\\beta -1} \\notag \\\\\n &= Beta(\\theta;a+T,b) \\notag \n \\end{align}\n$$\n后验概率也是一个概率分布。\n\n<br/><br/><br/>\n\n","slug":"how_to_predict_a_coin_flip","published":1,"updated":"2022-01-18T17:32:17.478Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6i001m2sfydei93sg4","content":"<p>抛10次硬币，已经抛了9次结果都是正面朝上，下一次是正面还是反面？<a id=\"more\"></a></p>\n<h3 id=\"古典概率\"><a href=\"#古典概率\" class=\"headerlink\" title=\"古典概率\"></a>古典概率</h3><p>“下一次正面和反面的概率各是0.5。因为前面不论扔了多少次硬币，其结果不会对下一次扔硬币产生影响。所以第10次扔硬币正面的概率仍是一半。”</p>\n<p>这是很“标准”的一个答案。听起来很简单，却很有道理。这个答案背后的假设，正是古典概率的基本假设。法国数学家拉普拉斯（Laplace）提出：若一个随机试验可能的结果是有限的，且<strong>每个结果发生的可能性均等</strong>，这所有可能结果构成一个样本空间$S$，则事件$E$发生的概率为</p>\n<script type=\"math/tex; mode=display\">\nP(E)=\\frac{E包含的结果数}{S包含的结果数}</script><p>对于抛硬币这样一个最简单的二元结果的事件，若只抛一次，样本空间$S=\\{1，0\\}$（以1代表正面向上，0代表反面向上）。那么正面朝上的概率$P(\\{1\\})=\\frac{1}{1+1}=\\frac{1}{2}$，同理反面朝上的概率$P(\\{0\\})=\\frac{1}{1+1}=\\frac{1}{2}$。</p>\n<p>回到最开始的问题，若抛10次硬币，已经抛了9次结果都是正面朝上，下一次抛正面向上的概率是多少？这是一个条件概率问题。记事件$T=t$为前$t$次都抛出了正面，事件$U$为下一次抛出正面，则所求事件的概率表示为$P(U|T=9)$，这里再引入一个假设：每次抛的结果都是<strong>独立</strong>的，才能得到</p>\n<script type=\"math/tex; mode=display\">\nP(U|T=9)=P(U)=\\frac{1}{2}</script><p>现在换个问题，若抛10000次硬币，前9999次结果都是正面朝上，下一次正面的概率还会是0.5吗？要知道，连续抛9999次硬币都为正面的概率为$(\\frac{1}{2})^{9999}$，比1767年普莱斯用贝叶斯论文算出的基督复活的概率还要小（后者大于1/1600000的概率为0.535）。这时候，如果请你预测下一次抛硬币的结果，你是否会动摇？是哪里出了问题呢。</p>\n<h3 id=\"贝叶斯学派\"><a href=\"#贝叶斯学派\" class=\"headerlink\" title=\"贝叶斯学派\"></a>贝叶斯学派</h3><p>为什么掷了9999次都是正面向上？是不是这枚硬币有什么问题？难道它两面都是正面？当你开始怀疑硬币正面朝上与反面朝上的概率是否相等时，就进入了贝叶斯学派的思考领域。</p>\n<p>贝叶斯学派认为，我们假设的“<strong>硬币掷出正反面的可能性均等</strong>”作为先验知识并不是确定的。一切<strong>先验概率随时都可以因观测到新的事件结果而修正</strong>，得到后验概率。这在现实中其实非常常见，比如描述大气运动的N-S方程在实际的预报模式中都会被简化，无法精确描述大气状态。方程是理想而美好的，但现实却很粗糙，所以需要不断调整对模式预报的置信度。相比较古典概率，贝叶斯学派更接地气，如果说古典概率是古希腊柏拉图式哲学一脉相承的产物，那么贝叶斯理论则属于实干家。</p>\n<p>记$H$是进行一次随机试验可能的结果，$E$是已经发生的事件，那么事件$H$的概率$P(H)$可以由观测$E$而更新：</p>\n<script type=\"math/tex; mode=display\">\nP(H|E) = \\frac{P(E|H)P(H)}{P(E)}</script><p>其中$P(H)$是先验概率，$P(E|H)$是在先验概率假设下事件$E$发生的条件概率，$P(E)$是所有可能情况下$E$发生的概率。</p>\n<p>抛9999次硬币朝上后，下一次朝上的概率用贝叶斯公式计算为</p>\n<script type=\"math/tex; mode=display\">\nP(U|T=9999) = \\frac{P(T=9999|U)P(U)}{P(T=9999)}</script><p>$P(U)$是假设的先验概率等于$\\frac{1}{2}$，在先验假设下前9999次正面朝上的概率$P(T=9999|U)=(\\frac{1}{2})^{9999}$，假如硬币真的是理想硬币，那$P(T=9999)$也等于$(\\frac{1}{2})^{9999}$，用贝叶斯公式算出来的后验概率$P(U|T=9999)=\\frac{1}{2}$，与古典概率一致；但很可能这枚硬币存在某种瑕疵，使得掷出正面向上的可能性远远大于反面，实际的$P(T=9999)$可能接近1（假设是0.8）的话，那么后验概率$P(U|T=9999)=\\frac{(\\frac{1}{2})^{9999}·\\frac{1}{2}}{0.8}$就接近0了。</p>\n<h3 id=\"概率的概率：Deterministic-vs-Stochastic\"><a href=\"#概率的概率：Deterministic-vs-Stochastic\" class=\"headerlink\" title=\"概率的概率：Deterministic vs Stochastic\"></a>概率的概率：Deterministic vs Stochastic</h3><p>1767年普莱斯用贝叶斯论文（图1）中的方法计算了基督复活的可能性，用来反驳休谟的《论神迹》（休谟认为你可以因为神迹信宗教，但基督复活，算神话？），他给出的是“基督复活概率大于1/1600000的概率”，也就是说，概率的概率。</p>\n<p><img src=\"/images/bayes.png\" alt=\"bayes\" style=\"zoom:60%;\" /></p>\n<center><small>图1. 普莱斯选定的贝叶斯论文单行本标题页(Watson 2013)</small></center>\n\n<p>这其实是另外一个重要观念的引入。之前计算硬币朝上事件发生的概率，我们给出了一个唯一的值$P(U)$，但如果这个概率本身就有不确定性呢？比如事先并不知道硬币正面朝上的概率，只知道硬币抛出正面概率可能为0.5，也可能为0.8，哪个可能性更大？如何描述这种可能性呢？事实上，事件$U$的概率$P(U)$可以看作一个特殊的参数，“概率的概率”可以推广到”任意未知参数的概率“，当我们无法确定一个参数的值时，就可以用概率分布描述它。这是从确定性（Deterministic）估计到随机性（Stochastic）估计的转变。</p>\n<p>记$\\theta = P(U)$，则同样的抛硬币问题用贝叶斯公式解为</p>\n<script type=\"math/tex; mode=display\">\nP(\\theta|T=9999) = \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }</script><p>此时先验概率$P(\\theta)$有一个假设的概率分布，对于抛硬币过程一般用$\\beta$分布来描述</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nBeta(\\theta;a,b)&=\\frac{1}{B(a,b)}\\theta ^{a-1}(1-\\theta)^{b-1}\\\\\nB(a,b)&=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\end{align}</script><p>那么</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nP(\\theta|T=9999) &= \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\\\\\n &= \\frac{1}{B(a+T,b) }\\theta ^{a+T-1}(1-\\theta)^{\\beta -1} \\notag \\\\\n &= Beta(\\theta;a+T,b) \\notag \n \\end{align}</script><p>后验概率也是一个概率分布。</p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>抛10次硬币，已经抛了9次结果都是正面朝上，下一次是正面还是反面？","more":"</p>\n<h3 id=\"古典概率\"><a href=\"#古典概率\" class=\"headerlink\" title=\"古典概率\"></a>古典概率</h3><p>“下一次正面和反面的概率各是0.5。因为前面不论扔了多少次硬币，其结果不会对下一次扔硬币产生影响。所以第10次扔硬币正面的概率仍是一半。”</p>\n<p>这是很“标准”的一个答案。听起来很简单，却很有道理。这个答案背后的假设，正是古典概率的基本假设。法国数学家拉普拉斯（Laplace）提出：若一个随机试验可能的结果是有限的，且<strong>每个结果发生的可能性均等</strong>，这所有可能结果构成一个样本空间$S$，则事件$E$发生的概率为</p>\n<script type=\"math/tex; mode=display\">\nP(E)=\\frac{E包含的结果数}{S包含的结果数}</script><p>对于抛硬币这样一个最简单的二元结果的事件，若只抛一次，样本空间$S=\\{1，0\\}$（以1代表正面向上，0代表反面向上）。那么正面朝上的概率$P(\\{1\\})=\\frac{1}{1+1}=\\frac{1}{2}$，同理反面朝上的概率$P(\\{0\\})=\\frac{1}{1+1}=\\frac{1}{2}$。</p>\n<p>回到最开始的问题，若抛10次硬币，已经抛了9次结果都是正面朝上，下一次抛正面向上的概率是多少？这是一个条件概率问题。记事件$T=t$为前$t$次都抛出了正面，事件$U$为下一次抛出正面，则所求事件的概率表示为$P(U|T=9)$，这里再引入一个假设：每次抛的结果都是<strong>独立</strong>的，才能得到</p>\n<script type=\"math/tex; mode=display\">\nP(U|T=9)=P(U)=\\frac{1}{2}</script><p>现在换个问题，若抛10000次硬币，前9999次结果都是正面朝上，下一次正面的概率还会是0.5吗？要知道，连续抛9999次硬币都为正面的概率为$(\\frac{1}{2})^{9999}$，比1767年普莱斯用贝叶斯论文算出的基督复活的概率还要小（后者大于1/1600000的概率为0.535）。这时候，如果请你预测下一次抛硬币的结果，你是否会动摇？是哪里出了问题呢。</p>\n<h3 id=\"贝叶斯学派\"><a href=\"#贝叶斯学派\" class=\"headerlink\" title=\"贝叶斯学派\"></a>贝叶斯学派</h3><p>为什么掷了9999次都是正面向上？是不是这枚硬币有什么问题？难道它两面都是正面？当你开始怀疑硬币正面朝上与反面朝上的概率是否相等时，就进入了贝叶斯学派的思考领域。</p>\n<p>贝叶斯学派认为，我们假设的“<strong>硬币掷出正反面的可能性均等</strong>”作为先验知识并不是确定的。一切<strong>先验概率随时都可以因观测到新的事件结果而修正</strong>，得到后验概率。这在现实中其实非常常见，比如描述大气运动的N-S方程在实际的预报模式中都会被简化，无法精确描述大气状态。方程是理想而美好的，但现实却很粗糙，所以需要不断调整对模式预报的置信度。相比较古典概率，贝叶斯学派更接地气，如果说古典概率是古希腊柏拉图式哲学一脉相承的产物，那么贝叶斯理论则属于实干家。</p>\n<p>记$H$是进行一次随机试验可能的结果，$E$是已经发生的事件，那么事件$H$的概率$P(H)$可以由观测$E$而更新：</p>\n<script type=\"math/tex; mode=display\">\nP(H|E) = \\frac{P(E|H)P(H)}{P(E)}</script><p>其中$P(H)$是先验概率，$P(E|H)$是在先验概率假设下事件$E$发生的条件概率，$P(E)$是所有可能情况下$E$发生的概率。</p>\n<p>抛9999次硬币朝上后，下一次朝上的概率用贝叶斯公式计算为</p>\n<script type=\"math/tex; mode=display\">\nP(U|T=9999) = \\frac{P(T=9999|U)P(U)}{P(T=9999)}</script><p>$P(U)$是假设的先验概率等于$\\frac{1}{2}$，在先验假设下前9999次正面朝上的概率$P(T=9999|U)=(\\frac{1}{2})^{9999}$，假如硬币真的是理想硬币，那$P(T=9999)$也等于$(\\frac{1}{2})^{9999}$，用贝叶斯公式算出来的后验概率$P(U|T=9999)=\\frac{1}{2}$，与古典概率一致；但很可能这枚硬币存在某种瑕疵，使得掷出正面向上的可能性远远大于反面，实际的$P(T=9999)$可能接近1（假设是0.8）的话，那么后验概率$P(U|T=9999)=\\frac{(\\frac{1}{2})^{9999}·\\frac{1}{2}}{0.8}$就接近0了。</p>\n<h3 id=\"概率的概率：Deterministic-vs-Stochastic\"><a href=\"#概率的概率：Deterministic-vs-Stochastic\" class=\"headerlink\" title=\"概率的概率：Deterministic vs Stochastic\"></a>概率的概率：Deterministic vs Stochastic</h3><p>1767年普莱斯用贝叶斯论文（图1）中的方法计算了基督复活的可能性，用来反驳休谟的《论神迹》（休谟认为你可以因为神迹信宗教，但基督复活，算神话？），他给出的是“基督复活概率大于1/1600000的概率”，也就是说，概率的概率。</p>\n<p><img src=\"/images/bayes.png\" alt=\"bayes\" style=\"zoom:60%;\" /></p>\n<center><small>图1. 普莱斯选定的贝叶斯论文单行本标题页(Watson 2013)</small></center>\n\n<p>这其实是另外一个重要观念的引入。之前计算硬币朝上事件发生的概率，我们给出了一个唯一的值$P(U)$，但如果这个概率本身就有不确定性呢？比如事先并不知道硬币正面朝上的概率，只知道硬币抛出正面概率可能为0.5，也可能为0.8，哪个可能性更大？如何描述这种可能性呢？事实上，事件$U$的概率$P(U)$可以看作一个特殊的参数，“概率的概率”可以推广到”任意未知参数的概率“，当我们无法确定一个参数的值时，就可以用概率分布描述它。这是从确定性（Deterministic）估计到随机性（Stochastic）估计的转变。</p>\n<p>记$\\theta = P(U)$，则同样的抛硬币问题用贝叶斯公式解为</p>\n<script type=\"math/tex; mode=display\">\nP(\\theta|T=9999) = \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }</script><p>此时先验概率$P(\\theta)$有一个假设的概率分布，对于抛硬币过程一般用$\\beta$分布来描述</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nBeta(\\theta;a,b)&=\\frac{1}{B(a,b)}\\theta ^{a-1}(1-\\theta)^{b-1}\\\\\nB(a,b)&=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\end{align}</script><p>那么</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nP(\\theta|T=9999) &= \\frac{P(T=9999|\\theta)P(\\theta)}{\\int_{\\theta \\in (0,1)}P(T=9999|\\theta)P(\\theta)d\\theta }\\\\\n &= \\frac{1}{B(a+T,b) }\\theta ^{a+T-1}(1-\\theta)^{\\beta -1} \\notag \\\\\n &= Beta(\\theta;a+T,b) \\notag \n \\end{align}</script><p>后验概率也是一个概率分布。</p>\n<p><br/><br/><br/></p>"},{"title":"相片与画（二）","date":"2022-01-21T14:26:00.000Z","mathjax":true,"_content":"\nGolden Sea, Coney Island<!--more-->\n\n![](/images/waterglow.jpg)\n\n<img src=\"/images/walking.jpg\"/>\n<img src=\"/images/20181126_213819.jpg\" width=\"700\"/> \n<img src=\"/images/20181126_214020.jpg\" width=\"520\"/> \n<img src=\"/images/purplewin.jpg\" width=\"700\"/> \n<img src=\"/images/chairs.jpg\" width=\"480\"/>\n\n<img src=\"/images/20190702_184004.jpg\"/>\n<img src=\"/images/20190626_235302.jpg\" width=\"700\"/> \n<img src=\"/images/20190626_235328.jpg\" width=\"700\"/>\n<img src=\"/images/20190626_235314.jpg\" width=\"700\"/>\n<img src=\"/images/20190129_224153.jpg\" width=\"520\"/> \n<img src=\"/images/20190712_151357.jpg\" width=\"700\"/>\n<img src=\"/images/20190202_162733.jpg\" width=\"700\"/> \n<img src=\"/images/IMG_0395.jpg\" width=\"700\"/>\n<img src=\"/images/IMG_1508.jpg\" width=\"700\"/>\n<img src=\"/images/DSC_5519.jpg\">\n<img src=\"/images/20190825_154120.jpg\" width=\"520\"/> \n<img src=\"/images/MID1072.jpg\"/>\n\n<img src=\"/images/IMG_4528.jpg\"/>\n<img src=\"/images/IMG_0340.jpg\"/>\n<img src=\"/images/IMG_0343.jpg\"/>\n<img src=\"/images/IMG_4393.jpg\" width=\"700\"/>\n<img src=\"/images/IMG_2906.jpg\" width=\"650\"/>\n<img src=\"/images/20181227_160908.jpg\"/>\n\n<br/><br/><br/>\n","source":"_posts/photopainting2.md","raw":"---\ntitle: 相片与画（二）\ndate: 2022-01-21 22:26:00\ncategories:\n- 相片与画\ntags: \n- 像\nmathjax: true\n---\n\nGolden Sea, Coney Island<!--more-->\n\n![](/images/waterglow.jpg)\n\n<img src=\"/images/walking.jpg\"/>\n<img src=\"/images/20181126_213819.jpg\" width=\"700\"/> \n<img src=\"/images/20181126_214020.jpg\" width=\"520\"/> \n<img src=\"/images/purplewin.jpg\" width=\"700\"/> \n<img src=\"/images/chairs.jpg\" width=\"480\"/>\n\n<img src=\"/images/20190702_184004.jpg\"/>\n<img src=\"/images/20190626_235302.jpg\" width=\"700\"/> \n<img src=\"/images/20190626_235328.jpg\" width=\"700\"/>\n<img src=\"/images/20190626_235314.jpg\" width=\"700\"/>\n<img src=\"/images/20190129_224153.jpg\" width=\"520\"/> \n<img src=\"/images/20190712_151357.jpg\" width=\"700\"/>\n<img src=\"/images/20190202_162733.jpg\" width=\"700\"/> \n<img src=\"/images/IMG_0395.jpg\" width=\"700\"/>\n<img src=\"/images/IMG_1508.jpg\" width=\"700\"/>\n<img src=\"/images/DSC_5519.jpg\">\n<img src=\"/images/20190825_154120.jpg\" width=\"520\"/> \n<img src=\"/images/MID1072.jpg\"/>\n\n<img src=\"/images/IMG_4528.jpg\"/>\n<img src=\"/images/IMG_0340.jpg\"/>\n<img src=\"/images/IMG_0343.jpg\"/>\n<img src=\"/images/IMG_4393.jpg\" width=\"700\"/>\n<img src=\"/images/IMG_2906.jpg\" width=\"650\"/>\n<img src=\"/images/20181227_160908.jpg\"/>\n\n<br/><br/><br/>\n","slug":"photopainting2","published":1,"updated":"2022-01-22T09:01:54.019Z","_id":"ckyoogd6k001q2sfyav1n8x8y","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Golden Sea, Coney Island<a id=\"more\"></a></p>\n<p><img src=\"/images/waterglow.jpg\" alt=\"\"></p>\n<p><img src=\"/images/walking.jpg\"/><br><img src=\"/images/20181126_213819.jpg\" width=\"700\"/><br><img src=\"/images/20181126_214020.jpg\" width=\"520\"/><br><img src=\"/images/purplewin.jpg\" width=\"700\"/><br><img src=\"/images/chairs.jpg\" width=\"480\"/></p>\n<p><img src=\"/images/20190702_184004.jpg\"/><br><img src=\"/images/20190626_235302.jpg\" width=\"700\"/><br><img src=\"/images/20190626_235328.jpg\" width=\"700\"/><br><img src=\"/images/20190626_235314.jpg\" width=\"700\"/><br><img src=\"/images/20190129_224153.jpg\" width=\"520\"/><br><img src=\"/images/20190712_151357.jpg\" width=\"700\"/><br><img src=\"/images/20190202_162733.jpg\" width=\"700\"/><br><img src=\"/images/IMG_0395.jpg\" width=\"700\"/><br><img src=\"/images/IMG_1508.jpg\" width=\"700\"/><br><img src=\"/images/DSC_5519.jpg\"><br><img src=\"/images/20190825_154120.jpg\" width=\"520\"/><br><img src=\"/images/MID1072.jpg\"/></p>\n<p><img src=\"/images/IMG_4528.jpg\"/><br><img src=\"/images/IMG_0340.jpg\"/><br><img src=\"/images/IMG_0343.jpg\"/><br><img src=\"/images/IMG_4393.jpg\" width=\"700\"/><br><img src=\"/images/IMG_2906.jpg\" width=\"650\"/><br><img src=\"/images/20181227_160908.jpg\"/></p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>Golden Sea, Coney Island","more":"</p>\n<p><img src=\"/images/waterglow.jpg\" alt=\"\"></p>\n<p><img src=\"/images/walking.jpg\"/><br><img src=\"/images/20181126_213819.jpg\" width=\"700\"/><br><img src=\"/images/20181126_214020.jpg\" width=\"520\"/><br><img src=\"/images/purplewin.jpg\" width=\"700\"/><br><img src=\"/images/chairs.jpg\" width=\"480\"/></p>\n<p><img src=\"/images/20190702_184004.jpg\"/><br><img src=\"/images/20190626_235302.jpg\" width=\"700\"/><br><img src=\"/images/20190626_235328.jpg\" width=\"700\"/><br><img src=\"/images/20190626_235314.jpg\" width=\"700\"/><br><img src=\"/images/20190129_224153.jpg\" width=\"520\"/><br><img src=\"/images/20190712_151357.jpg\" width=\"700\"/><br><img src=\"/images/20190202_162733.jpg\" width=\"700\"/><br><img src=\"/images/IMG_0395.jpg\" width=\"700\"/><br><img src=\"/images/IMG_1508.jpg\" width=\"700\"/><br><img src=\"/images/DSC_5519.jpg\"><br><img src=\"/images/20190825_154120.jpg\" width=\"520\"/><br><img src=\"/images/MID1072.jpg\"/></p>\n<p><img src=\"/images/IMG_4528.jpg\"/><br><img src=\"/images/IMG_0340.jpg\"/><br><img src=\"/images/IMG_0343.jpg\"/><br><img src=\"/images/IMG_4393.jpg\" width=\"700\"/><br><img src=\"/images/IMG_2906.jpg\" width=\"650\"/><br><img src=\"/images/20181227_160908.jpg\"/></p>\n<p><br/><br/><br/></p>"},{"title":"相片与画（三）","date":"2022-01-21T15:32:00.000Z","mathjax":true,"_content":"\n孩子，云和花儿<!--more-->\n\n<img src=\"/images/IMG_20200501_134812.jpg\" width=\"700\"/>\n\n<img src=\"/images/IMG_1829.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00429.jpg\" width=\"520\"/>\n\n<img src=\"/images/DSC00933.jpg\"/>\n<img src=\"/images/20180209_234842.jpg\"/>\n\n<img src=\"/images/DSC01381.jpg\"/>\n<img src=\"/images/mokey.jpg\"/>\n\n<img src=\"/images/20180109_221802.jpg\"/>\n<img src=\"/images/IMG_0927.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0632.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0215.jpg\" width=\"700\"/>\n\n<img src=\"/images/IMG_1071.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_1050.jpg\" width=\"520\"/>\n\n<img src=\"/images/20181227_1817202.jpg\"/>\n\n<img src=\"/images/IMG_4282.jpg\"/>\n<img src=\"/images/IMG_4302.jpg\"/>\n<img src=\"/images/IMG_4303.jpg\"/>\n<img src=\"/images/DSC01163.jpg\"/>\n<img src=\"/images/DSC01162.jpg\"/>\n<img src=\"/images/DSC01169.jpg\"/>\n<img src=\"/images/20181225_024529.jpg\"/>\n\n<img src=\"/images/DSC01395.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00338.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00374.jpg\" width=\"520\"/>\n<img src=\"/images/DSC01403.jpg\" width=\"600\"/>\n\n<img src=\"/images/MID1453.jpg\"/>\n\n<br/><br/><br/>\n","source":"_posts/photopainting3.md","raw":"---\ntitle: 相片与画（三）\ndate: 2022-01-21 23:32:00\ncategories:\n- 相片与画\ntags: \n- 像\n- 画\nmathjax: true\n---\n\n孩子，云和花儿<!--more-->\n\n<img src=\"/images/IMG_20200501_134812.jpg\" width=\"700\"/>\n\n<img src=\"/images/IMG_1829.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00429.jpg\" width=\"520\"/>\n\n<img src=\"/images/DSC00933.jpg\"/>\n<img src=\"/images/20180209_234842.jpg\"/>\n\n<img src=\"/images/DSC01381.jpg\"/>\n<img src=\"/images/mokey.jpg\"/>\n\n<img src=\"/images/20180109_221802.jpg\"/>\n<img src=\"/images/IMG_0927.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0632.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0215.jpg\" width=\"700\"/>\n\n<img src=\"/images/IMG_1071.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_1050.jpg\" width=\"520\"/>\n\n<img src=\"/images/20181227_1817202.jpg\"/>\n\n<img src=\"/images/IMG_4282.jpg\"/>\n<img src=\"/images/IMG_4302.jpg\"/>\n<img src=\"/images/IMG_4303.jpg\"/>\n<img src=\"/images/DSC01163.jpg\"/>\n<img src=\"/images/DSC01162.jpg\"/>\n<img src=\"/images/DSC01169.jpg\"/>\n<img src=\"/images/20181225_024529.jpg\"/>\n\n<img src=\"/images/DSC01395.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00338.jpg\" width=\"520\"/>\n<img src=\"/images/DSC00374.jpg\" width=\"520\"/>\n<img src=\"/images/DSC01403.jpg\" width=\"600\"/>\n\n<img src=\"/images/MID1453.jpg\"/>\n\n<br/><br/><br/>\n","slug":"photopainting3","published":1,"updated":"2022-01-21T19:04:13.207Z","_id":"ckyoogd6m001s2sfy1pk082hx","comments":1,"layout":"post","photos":[],"link":"","content":"<p>孩子，云和花儿<a id=\"more\"></a></p>\n<p><img src=\"/images/IMG_20200501_134812.jpg\" width=\"700\"/></p>\n<p><img src=\"/images/IMG_1829.jpg\" width=\"520\"/><br><img src=\"/images/DSC00429.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/DSC00933.jpg\"/><br><img src=\"/images/20180209_234842.jpg\"/></p>\n<p><img src=\"/images/DSC01381.jpg\"/><br><img src=\"/images/mokey.jpg\"/></p>\n<p><img src=\"/images/20180109_221802.jpg\"/><br><img src=\"/images/IMG_0927.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0632.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0215.jpg\" width=\"700\"/></p>\n<p><img src=\"/images/IMG_1071.jpg\" width=\"520\"/><br><img src=\"/images/IMG_1050.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/20181227_1817202.jpg\"/></p>\n<p><img src=\"/images/IMG_4282.jpg\"/><br><img src=\"/images/IMG_4302.jpg\"/><br><img src=\"/images/IMG_4303.jpg\"/><br><img src=\"/images/DSC01163.jpg\"/><br><img src=\"/images/DSC01162.jpg\"/><br><img src=\"/images/DSC01169.jpg\"/><br><img src=\"/images/20181225_024529.jpg\"/></p>\n<p><img src=\"/images/DSC01395.jpg\" width=\"520\"/><br><img src=\"/images/DSC00338.jpg\" width=\"520\"/><br><img src=\"/images/DSC00374.jpg\" width=\"520\"/><br><img src=\"/images/DSC01403.jpg\" width=\"600\"/></p>\n<p><img src=\"/images/MID1453.jpg\"/></p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>孩子，云和花儿","more":"</p>\n<p><img src=\"/images/IMG_20200501_134812.jpg\" width=\"700\"/></p>\n<p><img src=\"/images/IMG_1829.jpg\" width=\"520\"/><br><img src=\"/images/DSC00429.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/DSC00933.jpg\"/><br><img src=\"/images/20180209_234842.jpg\"/></p>\n<p><img src=\"/images/DSC01381.jpg\"/><br><img src=\"/images/mokey.jpg\"/></p>\n<p><img src=\"/images/20180109_221802.jpg\"/><br><img src=\"/images/IMG_0927.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0632.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0215.jpg\" width=\"700\"/></p>\n<p><img src=\"/images/IMG_1071.jpg\" width=\"520\"/><br><img src=\"/images/IMG_1050.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/20181227_1817202.jpg\"/></p>\n<p><img src=\"/images/IMG_4282.jpg\"/><br><img src=\"/images/IMG_4302.jpg\"/><br><img src=\"/images/IMG_4303.jpg\"/><br><img src=\"/images/DSC01163.jpg\"/><br><img src=\"/images/DSC01162.jpg\"/><br><img src=\"/images/DSC01169.jpg\"/><br><img src=\"/images/20181225_024529.jpg\"/></p>\n<p><img src=\"/images/DSC01395.jpg\" width=\"520\"/><br><img src=\"/images/DSC00338.jpg\" width=\"520\"/><br><img src=\"/images/DSC00374.jpg\" width=\"520\"/><br><img src=\"/images/DSC01403.jpg\" width=\"600\"/></p>\n<p><img src=\"/images/MID1453.jpg\"/></p>\n<p><br/><br/><br/></p>"},{"title":"相片与画（一）","date":"2022-01-21T11:22:00.000Z","mathjax":true,"_content":"\n时间，树，猫<!--more-->\n\n![](/images/IMG_20131031_131642.jpg)\n\n<img src=\"/images/DSC01365.jpg\"/>\n<img src=\"/images/MID0890.jpg\" width=\"480\"/> \n<img src=\"/images/MID0921.jpg\"/>\n<img src=\"/images/20190825_154330.jpg\" width=\"520\"/> \n<img src=\"/images/20181222_151042.jpg\"/>\n<img src=\"/images/20181222_162229.jpg\"/>\n<img src=\"/images/IMG_1282.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0720.jpg\" width=\"520\"/>\n<img src=\"/images/406812965.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_1460.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_5210.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_3344.jpg\"/>\n<img src=\"/images/20190717_234758.jpg\" width=\"700\"/>\n<img src=\"/images/20190708_230820.jpg\"/>\n<img src=\"/images/DSC00735.jpg\"/>\n<img src=\"/images/DSC00697.jpg\"/>\n<img src=\"/images/20181126_203334.jpg\" width=\"520\"/>\n<img src=\"/images/20181126_214514.jpg\" width=\"700\"/>\n<img src=\"/images/DSC_59052.jpg\"/>\n<img src=\"/images/DSC_50971.jpg\" width=\"520\"/>\n\n<img src=\"/images/DSC_5280.jpg\"/>\n<img src=\"/images/DSC_70031.jpg\"/>\n<img src=\"/images/DSC_7149.jpg\"/>\n<img src=\"/images/DSC_7181.jpg\"/>\n<img src=\"/images/DSC00598.jpg\"/>\n<img src=\"/images/DSC_7046.jpg\"/>\n<img src=\"/images/DSC_7047.jpg\"/>\n<img src=\"/images/IMG_0040.jpg\"/>\n![](/images/blackcat.jpg)\n<img src=\"/images/IMG_1550.jpg\" width=\"520\"/>\n\n<br/><br/><br/>\n","source":"_posts/photopainting.md","raw":"---\ntitle: 相片与画（一）\ndate: 2022-01-21 19:22:00\ncategories:\n- 相片与画\ntags: \n- 像\n- 画\nmathjax: true\n---\n\n时间，树，猫<!--more-->\n\n![](/images/IMG_20131031_131642.jpg)\n\n<img src=\"/images/DSC01365.jpg\"/>\n<img src=\"/images/MID0890.jpg\" width=\"480\"/> \n<img src=\"/images/MID0921.jpg\"/>\n<img src=\"/images/20190825_154330.jpg\" width=\"520\"/> \n<img src=\"/images/20181222_151042.jpg\"/>\n<img src=\"/images/20181222_162229.jpg\"/>\n<img src=\"/images/IMG_1282.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_0720.jpg\" width=\"520\"/>\n<img src=\"/images/406812965.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_1460.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_5210.jpg\" width=\"520\"/>\n<img src=\"/images/IMG_3344.jpg\"/>\n<img src=\"/images/20190717_234758.jpg\" width=\"700\"/>\n<img src=\"/images/20190708_230820.jpg\"/>\n<img src=\"/images/DSC00735.jpg\"/>\n<img src=\"/images/DSC00697.jpg\"/>\n<img src=\"/images/20181126_203334.jpg\" width=\"520\"/>\n<img src=\"/images/20181126_214514.jpg\" width=\"700\"/>\n<img src=\"/images/DSC_59052.jpg\"/>\n<img src=\"/images/DSC_50971.jpg\" width=\"520\"/>\n\n<img src=\"/images/DSC_5280.jpg\"/>\n<img src=\"/images/DSC_70031.jpg\"/>\n<img src=\"/images/DSC_7149.jpg\"/>\n<img src=\"/images/DSC_7181.jpg\"/>\n<img src=\"/images/DSC00598.jpg\"/>\n<img src=\"/images/DSC_7046.jpg\"/>\n<img src=\"/images/DSC_7047.jpg\"/>\n<img src=\"/images/IMG_0040.jpg\"/>\n![](/images/blackcat.jpg)\n<img src=\"/images/IMG_1550.jpg\" width=\"520\"/>\n\n<br/><br/><br/>\n","slug":"photopainting","published":1,"updated":"2022-09-22T13:20:19.656Z","_id":"ckyoogd6o001w2sfy6w2a1dqm","comments":1,"layout":"post","photos":[],"link":"","content":"<p>时间，树，猫<a id=\"more\"></a></p>\n<p><img src=\"/images/IMG_20131031_131642.jpg\" alt=\"\"></p>\n<p><img src=\"/images/DSC01365.jpg\"/><br><img src=\"/images/MID0890.jpg\" width=\"480\"/><br><img src=\"/images/MID0921.jpg\"/><br><img src=\"/images/20190825_154330.jpg\" width=\"520\"/><br><img src=\"/images/20181222_151042.jpg\"/><br><img src=\"/images/20181222_162229.jpg\"/><br><img src=\"/images/IMG_1282.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0720.jpg\" width=\"520\"/><br><img src=\"/images/406812965.jpg\" width=\"520\"/><br><img src=\"/images/IMG_1460.jpg\" width=\"520\"/><br><img src=\"/images/IMG_5210.jpg\" width=\"520\"/><br><img src=\"/images/IMG_3344.jpg\"/><br><img src=\"/images/20190717_234758.jpg\" width=\"700\"/><br><img src=\"/images/20190708_230820.jpg\"/><br><img src=\"/images/DSC00735.jpg\"/><br><img src=\"/images/DSC00697.jpg\"/><br><img src=\"/images/20181126_203334.jpg\" width=\"520\"/><br><img src=\"/images/20181126_214514.jpg\" width=\"700\"/><br><img src=\"/images/DSC_59052.jpg\"/><br><img src=\"/images/DSC_50971.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/DSC_5280.jpg\"/><br><img src=\"/images/DSC_70031.jpg\"/><br><img src=\"/images/DSC_7149.jpg\"/><br><img src=\"/images/DSC_7181.jpg\"/><br><img src=\"/images/DSC00598.jpg\"/><br><img src=\"/images/DSC_7046.jpg\"/><br><img src=\"/images/DSC_7047.jpg\"/><br><img src=\"/images/IMG_0040.jpg\"/><br><img src=\"/images/blackcat.jpg\" alt=\"\"><br><img src=\"/images/IMG_1550.jpg\" width=\"520\"/></p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>时间，树，猫","more":"</p>\n<p><img src=\"/images/IMG_20131031_131642.jpg\" alt=\"\"></p>\n<p><img src=\"/images/DSC01365.jpg\"/><br><img src=\"/images/MID0890.jpg\" width=\"480\"/><br><img src=\"/images/MID0921.jpg\"/><br><img src=\"/images/20190825_154330.jpg\" width=\"520\"/><br><img src=\"/images/20181222_151042.jpg\"/><br><img src=\"/images/20181222_162229.jpg\"/><br><img src=\"/images/IMG_1282.jpg\" width=\"520\"/><br><img src=\"/images/IMG_0720.jpg\" width=\"520\"/><br><img src=\"/images/406812965.jpg\" width=\"520\"/><br><img src=\"/images/IMG_1460.jpg\" width=\"520\"/><br><img src=\"/images/IMG_5210.jpg\" width=\"520\"/><br><img src=\"/images/IMG_3344.jpg\"/><br><img src=\"/images/20190717_234758.jpg\" width=\"700\"/><br><img src=\"/images/20190708_230820.jpg\"/><br><img src=\"/images/DSC00735.jpg\"/><br><img src=\"/images/DSC00697.jpg\"/><br><img src=\"/images/20181126_203334.jpg\" width=\"520\"/><br><img src=\"/images/20181126_214514.jpg\" width=\"700\"/><br><img src=\"/images/DSC_59052.jpg\"/><br><img src=\"/images/DSC_50971.jpg\" width=\"520\"/></p>\n<p><img src=\"/images/DSC_5280.jpg\"/><br><img src=\"/images/DSC_70031.jpg\"/><br><img src=\"/images/DSC_7149.jpg\"/><br><img src=\"/images/DSC_7181.jpg\"/><br><img src=\"/images/DSC00598.jpg\"/><br><img src=\"/images/DSC_7046.jpg\"/><br><img src=\"/images/DSC_7047.jpg\"/><br><img src=\"/images/IMG_0040.jpg\"/><br><img src=\"/images/blackcat.jpg\" alt=\"\"><br><img src=\"/images/IMG_1550.jpg\" width=\"520\"/></p>\n<p><br/><br/><br/></p>"},{"title":"读赵的《改革历程》","date":"2019-06-19T08:04:00.000Z","_content":"> ⼀直以来，邓⼩平被⼈们当作中国改⾰开放的总设计师。但事实上，由于政治力量的⼲预， ⼀位重要⼈物长久以来被⼈们忽视，消失于话语的世界。他在中国改⾰开放进程中起到如此重要的作⽤，以⾄于任何对他的无视或轻视，都是对他不公正的对待。他就是前中共中央总书记赵紫阳。从1980年出任中央财经领导⼩组组长开始，直到1989年6月结束政治生涯，赵紫阳对改⾰开放的思考和推动贯彻始终。本⽂主要依据由赵紫阳录⾳整理⽽成的《改⾰历程》⼀书，尝试对赵这⼗年的作为、之于改⾰开放的影响、在历史中的评价做⼀个简单的梳理。这样建⽴起的个⼈认识可能是偏狭的(录⾳毕竟是⼀家之言)，但未必是没有意义的，因为⼀切深刻的认识都从幼稚和曲折中发展而来。 <!--more-->\n\n1988年，赵紫阳在北京中南海接⻅美国著名经济学家弗里德曼。交流过后，弗里德曼称赵紫阳是他所⻅过的“社会主义国家里最好的经济学家”。这并不是无稽之谈。 \n\n早在1976年⽂⾰刚刚结束，还在四川任省委书记的赵紫阳就开始思考经济问题。1977-1978 两年，赵紫阳在四川30多个县市进⾏了农村调查，并开始逐步扩⼤农⺠⾃主权，制定了“农村经济政策⼗⼆条”。这是改⾰进⼊经济⽣活实质性的⼀步，也是赵紫阳改⾰之路的起点。从四川的经验中，赵紫阳得到⼀个认识:发展经济，效益不可忽视。因为他发现，建国以来，我国的经济⽣产总值增长了不少，而⼈均消费⽔平的增长却微乎其微。这说明国家经济总量的增⻓并没有给⼈民的⽣活⽔平带来相应程度的提升。为了改变这个情况，赵紫阳把经济效益摆在核心位置，这成了他日后深化改⾰的起点。1981年，刚刚上任国务院总理的赵紫阳向全国⼈⼤做了名为 《当前的经济形势和今后的经济政策》的报告，提出⼗条经济⽅针，核心就是把经济效益作为重点。更进⼀步，赵紫阳认识到经济效益问题根本上讲还是经济制度的问题。经济必须市场化，给予其充分的⾃由来⾃我调整，才有可能提高效益。 \n\n在农业上，赵紫阳继续扩⼤农⺠⾃主权。1980年任中央财经领导⼩组组长后，他提出首先在⼀亿⼈⼝的农村贫困社队实行包产到户，⽽后党内虽有反对的声⾳，但农村的联产承包责任制都逐渐得以实施。这背后少不了邓⼩平的⽀持，后面还会提到，邓⼩平对赵紫阳改⾰意⻅的⽀持起到了⾄关重要的作⽤。1978年，⼗⼀届三中全会确立的农业所有制还是⼈民公社所有制。从提出包产到户到全国范围推行，只⽤了短短三年时间。⼀⽅面得益于邓⼩平的⽀持，另⼀⽅⾯也是因为包产到户的成效很明显，实⾏包产到户后，农业生产恢复得很快，所以阻力很快化解。 \n\n在对外开放的问题上，赵紫阳与邓⼩平的意⻅保持⼀致，⽀持邓⼩平设⽴特区。对外开放的阻⼒主要来⾃于党内的保守派，陈云、邓力群等。陈云是党内资历深厚的⽼⼈，在经济问题上的意⻅举⾜轻重。他最早从第⼀个五年计划就开始主持经济工作，三⼤改造完成后，是他最早提出计划经济为主，市场调节为辅的“⻦笼经济”思想。赵紫阳对陈云很敬重。他曾提过:“对陈云同志，我调中央工作的头几年对他很尊重。我觉得在⽼⼀代领导⼈中， 陈云对经济工作有深刻的研 究，有⾃己的真知灼⻅。 ”但是，陈云的思想却停留在了“⻦笼经济”，以⾄于在改⾰开放初期成为了保守派⽼⼈，阻碍了改⾰开放的进程。他认为外国资本家追求的是超额利润，引进外资不可能得到实惠，所以他对引进外资、对外开放很警惕和怀疑。1982年1⽉在沿海地区开展的打击经济犯罪活动，就是经陈云批示后开展的。这场活动在保守派的推动下很快升级为经济领域的反资产阶级⾃由化运动，从单纯的打击经济犯罪上升到思想领域的阶级⽃争。经过这样的打击，⼈们对改⾰开放的态度陷入混乱，顾虑重重，改⾰开放⼀度陷入停顿。赵紫阳在录⾳中提到: \n\n*当时把经济上放宽搞活后难以完全避免的现象， 提高到“是在新的条件下阶级斗争的重要表现”，甚至还提出“是国内外阶级敌资本主义的腐朽思想对我们进行破坏、腐蚀的反映”。......这次提出打击经济犯罪活动，宣布经济特区也必须坚持以计划经济为主，市场调节为辅，这样来就没有什么特区了。还宣布要加强对外经济活动的统一管理。除国家规定的单位，按国家规定的原则和程序进行以外， 严禁任何单位和个体进行对外经济活动。这样就把原来在改革开放中，在建立特区时已经下放给特区的些权力取消。并且还规定要增加国家对农副产品的统购和派购，减少议价部分的比重。还提出把沿海工的奖金控制在略高于内地的水平。这样一来，打击沿海地区的经济犯罪活动，变成在经济上反对资产阶级由化，势必使整个改革开放的一些搞法被否定了，把已经下放的权力又收回来。* \n\n改⾰开放初期，赵紫阳结合欧洲考察和国内农村调查的观察，认识到对外贸易对于经济发展因地制宜、扬⻓避短的重要作⽤。后来又经过长期的摸索、思考，于1988年1月，赵紫阳提出了沿海发展战略。沿海发展战略是对外开放的进⼀步深化，目的是将国内经济推向世界经济，使之紧密结合。充分发􏰁国内劳动成本低的优势，发展进出⼝加工的劳动密集型产业，参与到世界经济的浪潮中。邓⼩平⾮常⽀持赵紫阳的沿海发展战略，虽然有来⾃包括陈云在内的各⽅不同意见，但最后还是通过并实施了。虽然89年之后不再提了，但其实还是按照这个路线来发展，事实证明沿海发展战略对于中国经济发展的意义是重⼤的。 \n\n之前提到，邓⼩平对赵紫阳在经济领域决策的⽀持对于改⾰开放的推进⾄关重要。赵紫阳是 从县委书记⼀步成为中央总书记的，他⼤部分政治⽣生涯在地⽅度过，1980年才调到北京。因此，赵在党内缺乏关系基础，他也说⾃己“渠道不多，消息⽐较闭塞。”所以，⾯面对党内保守派左派的冲击，邓⼩平这⼀元⽼的⽀持是赵紫阳坚实的后盾。 \n\n1987年胡耀邦下台后，党内出现了⼀股强烈的反⾃由化的声浪。为了防⽌止左倾⼒力量量将反⾃由化扩⼤，对改⾰造成伤害，赵紫阳从⼀开始就制定了《中共中央关于当前反对资产阶级⾃由化若⼲问题的通知》，对这场运动做出了严格的限制。但是，以邓力群、胡乔木等为代表的⼀些左派对这个限制很不满，他们制造舆论，极力突破⽂件的限制，企图将反⾃由化扩⼤到经济、科技、⽂艺各个领域。这使得改⾰开放⾯临严重威胁。邓⼒群等⼈利⽤左派控制下的刊物《红旗》、《⽂艺理论与批评》还有《光明⽇报》，组织舆论讨论，称反⾃由化是对⼗⼀届三中全会以来路线的第⼆次拨乱反正，想要否定三中全会以来的解放思想、改⾰开放路线。为了遏制反⾃由化扩⼤化的趋势，为即将到来的⼗三⼤进⼀步深化改⾰开放营造氛围，赵紫阳征得邓⼩平的同意后于1987年5⽉月13⽇发表了⼀次讲话。赵紫阳说: \n\n*我于 1987 5 月 13 日在宣传、理论、新闻、党校部会议上讲了一次话。 由于邓在这时期先后对外宾讲贫穷不是社会主义;中国搞社会主义左的错误是主要的;只有生产力发展才能说是真的搞社会主义等等这样的话，所以，我这次讲话就显得更加有力，在讨论我的讲话稿的书记处、政治局会议上，我除对反由化当中出现的以左反右企图冲破中央四号件规定的种种干扰进行严肃的批评外，还反复阐述以下几个观点:*\n\n*第一，经过几个月的努力，大的气候已经发生变化，自由化泛滥的情况已经制止。今后应强调改开放这一面，十三大必须关成改革开放的会议。从现在起就要 开好十三大做好舆论准备。*\n\n*第二，这次开展反由化的斗争，目的是要解决自由化泛滥的问题。......为了解决自由化泛滥问题，我们个时期主要排除右的干扰是对的，但从长期从根本方面讲，改革开放的障碍主要来左的方面。* \n\n*第三，四项基本原则是我们立国之本，改革开放是建设有中国特色社会主义的总方针、总政策。......*\n\n*第四，提出重视生产力标准的问题。......*\n\n*我的这篇讲话，受到绝大多数干部的拥护，整个空气在这篇讲话以后发生有利于改革开放的变化。这也为顺利地起草十三大报告创造了一个好的环境。*\n\n紧接着，由于1988年的物价改⾰的失误，市场发生􏰀挤兑抢购，为了稳定⼈心，中央提出了 “治理整顿”的⼝号。以李鹏、姚依林和⼀些⽼⼈为代表的左派借机抨击赵紫阳的经济改⾰措施。 1989年元旦前，李鹏、姚依林在政治局常委⽣活会上指责赵紫阳对经济过问太多，但其实当时赵紫阳已被他们架空，⼏乎失去了对经济改⾰的掌控。这次⽭头直指赵紫阳在物价改⾰中的责任问题。邓⼩平得知开会内容后，很不⾼兴，讲了⼀⼤段话说他⽀持改⾰。1989年春节前夕，赵紫阳到邓⼩平家中，两⼈进⾏了⼀次深⼊谈话。邓⼩平表示他想退隐，辞去军委主席，从⽽带动 ⼀批⽼⼈的退位，减少⽼⼈们的⼲预。但赵紫阳坚决劝邓⼩平留下，因为他知道邓如果在这个时候退出，不仅不会减少⽼⼈们的⼲预，反而会使他的改⾰进程陷入孤⽴无援的境地。 \n\n*从这段邓的谈话中，我觉察有人，也许不少老人向他施加压力，说我坏话， 而邓明确表示不受他们的影响，顶住他们这些压力。 1987年我担任总书记之后，逐渐聚集起来的一批反对我的保守力量，虽然他们非常嚣张地反对我，采用各种办法反对我，但由于没有得到邓的同意，始终未能得逞。这种情况直到 1989年发生那场政治风波后才改变。*\n\n回顾赵紫阳改⾰开放⼗年，我看到了⼀个朴实的政治家对于国家经济改⾰热忱、孜孜不倦的思考，也看到了身处官场，与各⽅⼒量周旋，实现⾃⼰想法的不易。邓⼩平被称为改⾰开放的总设计师⽽称颂于世，⽽赵紫阳却因为⼀场政治风波落得悲剧的结局，不⻅于当代的历史。但好在历史的时间维度是很⻓的，只要它真实而连续地存在，就不会消灭永恒的价值。 ","source":"_posts/reading gaigelicheng.md","raw":"---\ntitle: 读赵的《改革历程》\ndate: 2019-06-19 16:04:00\ncategories:\n- 杂想\ntags: \n- 近代史\n---\n> ⼀直以来，邓⼩平被⼈们当作中国改⾰开放的总设计师。但事实上，由于政治力量的⼲预， ⼀位重要⼈物长久以来被⼈们忽视，消失于话语的世界。他在中国改⾰开放进程中起到如此重要的作⽤，以⾄于任何对他的无视或轻视，都是对他不公正的对待。他就是前中共中央总书记赵紫阳。从1980年出任中央财经领导⼩组组长开始，直到1989年6月结束政治生涯，赵紫阳对改⾰开放的思考和推动贯彻始终。本⽂主要依据由赵紫阳录⾳整理⽽成的《改⾰历程》⼀书，尝试对赵这⼗年的作为、之于改⾰开放的影响、在历史中的评价做⼀个简单的梳理。这样建⽴起的个⼈认识可能是偏狭的(录⾳毕竟是⼀家之言)，但未必是没有意义的，因为⼀切深刻的认识都从幼稚和曲折中发展而来。 <!--more-->\n\n1988年，赵紫阳在北京中南海接⻅美国著名经济学家弗里德曼。交流过后，弗里德曼称赵紫阳是他所⻅过的“社会主义国家里最好的经济学家”。这并不是无稽之谈。 \n\n早在1976年⽂⾰刚刚结束，还在四川任省委书记的赵紫阳就开始思考经济问题。1977-1978 两年，赵紫阳在四川30多个县市进⾏了农村调查，并开始逐步扩⼤农⺠⾃主权，制定了“农村经济政策⼗⼆条”。这是改⾰进⼊经济⽣活实质性的⼀步，也是赵紫阳改⾰之路的起点。从四川的经验中，赵紫阳得到⼀个认识:发展经济，效益不可忽视。因为他发现，建国以来，我国的经济⽣产总值增长了不少，而⼈均消费⽔平的增长却微乎其微。这说明国家经济总量的增⻓并没有给⼈民的⽣活⽔平带来相应程度的提升。为了改变这个情况，赵紫阳把经济效益摆在核心位置，这成了他日后深化改⾰的起点。1981年，刚刚上任国务院总理的赵紫阳向全国⼈⼤做了名为 《当前的经济形势和今后的经济政策》的报告，提出⼗条经济⽅针，核心就是把经济效益作为重点。更进⼀步，赵紫阳认识到经济效益问题根本上讲还是经济制度的问题。经济必须市场化，给予其充分的⾃由来⾃我调整，才有可能提高效益。 \n\n在农业上，赵紫阳继续扩⼤农⺠⾃主权。1980年任中央财经领导⼩组组长后，他提出首先在⼀亿⼈⼝的农村贫困社队实行包产到户，⽽后党内虽有反对的声⾳，但农村的联产承包责任制都逐渐得以实施。这背后少不了邓⼩平的⽀持，后面还会提到，邓⼩平对赵紫阳改⾰意⻅的⽀持起到了⾄关重要的作⽤。1978年，⼗⼀届三中全会确立的农业所有制还是⼈民公社所有制。从提出包产到户到全国范围推行，只⽤了短短三年时间。⼀⽅面得益于邓⼩平的⽀持，另⼀⽅⾯也是因为包产到户的成效很明显，实⾏包产到户后，农业生产恢复得很快，所以阻力很快化解。 \n\n在对外开放的问题上，赵紫阳与邓⼩平的意⻅保持⼀致，⽀持邓⼩平设⽴特区。对外开放的阻⼒主要来⾃于党内的保守派，陈云、邓力群等。陈云是党内资历深厚的⽼⼈，在经济问题上的意⻅举⾜轻重。他最早从第⼀个五年计划就开始主持经济工作，三⼤改造完成后，是他最早提出计划经济为主，市场调节为辅的“⻦笼经济”思想。赵紫阳对陈云很敬重。他曾提过:“对陈云同志，我调中央工作的头几年对他很尊重。我觉得在⽼⼀代领导⼈中， 陈云对经济工作有深刻的研 究，有⾃己的真知灼⻅。 ”但是，陈云的思想却停留在了“⻦笼经济”，以⾄于在改⾰开放初期成为了保守派⽼⼈，阻碍了改⾰开放的进程。他认为外国资本家追求的是超额利润，引进外资不可能得到实惠，所以他对引进外资、对外开放很警惕和怀疑。1982年1⽉在沿海地区开展的打击经济犯罪活动，就是经陈云批示后开展的。这场活动在保守派的推动下很快升级为经济领域的反资产阶级⾃由化运动，从单纯的打击经济犯罪上升到思想领域的阶级⽃争。经过这样的打击，⼈们对改⾰开放的态度陷入混乱，顾虑重重，改⾰开放⼀度陷入停顿。赵紫阳在录⾳中提到: \n\n*当时把经济上放宽搞活后难以完全避免的现象， 提高到“是在新的条件下阶级斗争的重要表现”，甚至还提出“是国内外阶级敌资本主义的腐朽思想对我们进行破坏、腐蚀的反映”。......这次提出打击经济犯罪活动，宣布经济特区也必须坚持以计划经济为主，市场调节为辅，这样来就没有什么特区了。还宣布要加强对外经济活动的统一管理。除国家规定的单位，按国家规定的原则和程序进行以外， 严禁任何单位和个体进行对外经济活动。这样就把原来在改革开放中，在建立特区时已经下放给特区的些权力取消。并且还规定要增加国家对农副产品的统购和派购，减少议价部分的比重。还提出把沿海工的奖金控制在略高于内地的水平。这样一来，打击沿海地区的经济犯罪活动，变成在经济上反对资产阶级由化，势必使整个改革开放的一些搞法被否定了，把已经下放的权力又收回来。* \n\n改⾰开放初期，赵紫阳结合欧洲考察和国内农村调查的观察，认识到对外贸易对于经济发展因地制宜、扬⻓避短的重要作⽤。后来又经过长期的摸索、思考，于1988年1月，赵紫阳提出了沿海发展战略。沿海发展战略是对外开放的进⼀步深化，目的是将国内经济推向世界经济，使之紧密结合。充分发􏰁国内劳动成本低的优势，发展进出⼝加工的劳动密集型产业，参与到世界经济的浪潮中。邓⼩平⾮常⽀持赵紫阳的沿海发展战略，虽然有来⾃包括陈云在内的各⽅不同意见，但最后还是通过并实施了。虽然89年之后不再提了，但其实还是按照这个路线来发展，事实证明沿海发展战略对于中国经济发展的意义是重⼤的。 \n\n之前提到，邓⼩平对赵紫阳在经济领域决策的⽀持对于改⾰开放的推进⾄关重要。赵紫阳是 从县委书记⼀步成为中央总书记的，他⼤部分政治⽣生涯在地⽅度过，1980年才调到北京。因此，赵在党内缺乏关系基础，他也说⾃己“渠道不多，消息⽐较闭塞。”所以，⾯面对党内保守派左派的冲击，邓⼩平这⼀元⽼的⽀持是赵紫阳坚实的后盾。 \n\n1987年胡耀邦下台后，党内出现了⼀股强烈的反⾃由化的声浪。为了防⽌止左倾⼒力量量将反⾃由化扩⼤，对改⾰造成伤害，赵紫阳从⼀开始就制定了《中共中央关于当前反对资产阶级⾃由化若⼲问题的通知》，对这场运动做出了严格的限制。但是，以邓力群、胡乔木等为代表的⼀些左派对这个限制很不满，他们制造舆论，极力突破⽂件的限制，企图将反⾃由化扩⼤到经济、科技、⽂艺各个领域。这使得改⾰开放⾯临严重威胁。邓⼒群等⼈利⽤左派控制下的刊物《红旗》、《⽂艺理论与批评》还有《光明⽇报》，组织舆论讨论，称反⾃由化是对⼗⼀届三中全会以来路线的第⼆次拨乱反正，想要否定三中全会以来的解放思想、改⾰开放路线。为了遏制反⾃由化扩⼤化的趋势，为即将到来的⼗三⼤进⼀步深化改⾰开放营造氛围，赵紫阳征得邓⼩平的同意后于1987年5⽉月13⽇发表了⼀次讲话。赵紫阳说: \n\n*我于 1987 5 月 13 日在宣传、理论、新闻、党校部会议上讲了一次话。 由于邓在这时期先后对外宾讲贫穷不是社会主义;中国搞社会主义左的错误是主要的;只有生产力发展才能说是真的搞社会主义等等这样的话，所以，我这次讲话就显得更加有力，在讨论我的讲话稿的书记处、政治局会议上，我除对反由化当中出现的以左反右企图冲破中央四号件规定的种种干扰进行严肃的批评外，还反复阐述以下几个观点:*\n\n*第一，经过几个月的努力，大的气候已经发生变化，自由化泛滥的情况已经制止。今后应强调改开放这一面，十三大必须关成改革开放的会议。从现在起就要 开好十三大做好舆论准备。*\n\n*第二，这次开展反由化的斗争，目的是要解决自由化泛滥的问题。......为了解决自由化泛滥问题，我们个时期主要排除右的干扰是对的，但从长期从根本方面讲，改革开放的障碍主要来左的方面。* \n\n*第三，四项基本原则是我们立国之本，改革开放是建设有中国特色社会主义的总方针、总政策。......*\n\n*第四，提出重视生产力标准的问题。......*\n\n*我的这篇讲话，受到绝大多数干部的拥护，整个空气在这篇讲话以后发生有利于改革开放的变化。这也为顺利地起草十三大报告创造了一个好的环境。*\n\n紧接着，由于1988年的物价改⾰的失误，市场发生􏰀挤兑抢购，为了稳定⼈心，中央提出了 “治理整顿”的⼝号。以李鹏、姚依林和⼀些⽼⼈为代表的左派借机抨击赵紫阳的经济改⾰措施。 1989年元旦前，李鹏、姚依林在政治局常委⽣活会上指责赵紫阳对经济过问太多，但其实当时赵紫阳已被他们架空，⼏乎失去了对经济改⾰的掌控。这次⽭头直指赵紫阳在物价改⾰中的责任问题。邓⼩平得知开会内容后，很不⾼兴，讲了⼀⼤段话说他⽀持改⾰。1989年春节前夕，赵紫阳到邓⼩平家中，两⼈进⾏了⼀次深⼊谈话。邓⼩平表示他想退隐，辞去军委主席，从⽽带动 ⼀批⽼⼈的退位，减少⽼⼈们的⼲预。但赵紫阳坚决劝邓⼩平留下，因为他知道邓如果在这个时候退出，不仅不会减少⽼⼈们的⼲预，反而会使他的改⾰进程陷入孤⽴无援的境地。 \n\n*从这段邓的谈话中，我觉察有人，也许不少老人向他施加压力，说我坏话， 而邓明确表示不受他们的影响，顶住他们这些压力。 1987年我担任总书记之后，逐渐聚集起来的一批反对我的保守力量，虽然他们非常嚣张地反对我，采用各种办法反对我，但由于没有得到邓的同意，始终未能得逞。这种情况直到 1989年发生那场政治风波后才改变。*\n\n回顾赵紫阳改⾰开放⼗年，我看到了⼀个朴实的政治家对于国家经济改⾰热忱、孜孜不倦的思考，也看到了身处官场，与各⽅⼒量周旋，实现⾃⼰想法的不易。邓⼩平被称为改⾰开放的总设计师⽽称颂于世，⽽赵紫阳却因为⼀场政治风波落得悲剧的结局，不⻅于当代的历史。但好在历史的时间维度是很⻓的，只要它真实而连续地存在，就不会消灭永恒的价值。 ","slug":"reading gaigelicheng","published":1,"updated":"2021-08-26T13:13:42.724Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6p001y2sfyfsufh2zi","content":"<blockquote>\n<p>⼀直以来，邓⼩平被⼈们当作中国改⾰开放的总设计师。但事实上，由于政治力量的⼲预， ⼀位重要⼈物长久以来被⼈们忽视，消失于话语的世界。他在中国改⾰开放进程中起到如此重要的作⽤，以⾄于任何对他的无视或轻视，都是对他不公正的对待。他就是前中共中央总书记赵紫阳。从1980年出任中央财经领导⼩组组长开始，直到1989年6月结束政治生涯，赵紫阳对改⾰开放的思考和推动贯彻始终。本⽂主要依据由赵紫阳录⾳整理⽽成的《改⾰历程》⼀书，尝试对赵这⼗年的作为、之于改⾰开放的影响、在历史中的评价做⼀个简单的梳理。这样建⽴起的个⼈认识可能是偏狭的(录⾳毕竟是⼀家之言)，但未必是没有意义的，因为⼀切深刻的认识都从幼稚和曲折中发展而来。 <a id=\"more\"></a></p>\n</blockquote>\n<p>1988年，赵紫阳在北京中南海接⻅美国著名经济学家弗里德曼。交流过后，弗里德曼称赵紫阳是他所⻅过的“社会主义国家里最好的经济学家”。这并不是无稽之谈。 </p>\n<p>早在1976年⽂⾰刚刚结束，还在四川任省委书记的赵紫阳就开始思考经济问题。1977-1978 两年，赵紫阳在四川30多个县市进⾏了农村调查，并开始逐步扩⼤农⺠⾃主权，制定了“农村经济政策⼗⼆条”。这是改⾰进⼊经济⽣活实质性的⼀步，也是赵紫阳改⾰之路的起点。从四川的经验中，赵紫阳得到⼀个认识:发展经济，效益不可忽视。因为他发现，建国以来，我国的经济⽣产总值增长了不少，而⼈均消费⽔平的增长却微乎其微。这说明国家经济总量的增⻓并没有给⼈民的⽣活⽔平带来相应程度的提升。为了改变这个情况，赵紫阳把经济效益摆在核心位置，这成了他日后深化改⾰的起点。1981年，刚刚上任国务院总理的赵紫阳向全国⼈⼤做了名为 《当前的经济形势和今后的经济政策》的报告，提出⼗条经济⽅针，核心就是把经济效益作为重点。更进⼀步，赵紫阳认识到经济效益问题根本上讲还是经济制度的问题。经济必须市场化，给予其充分的⾃由来⾃我调整，才有可能提高效益。 </p>\n<p>在农业上，赵紫阳继续扩⼤农⺠⾃主权。1980年任中央财经领导⼩组组长后，他提出首先在⼀亿⼈⼝的农村贫困社队实行包产到户，⽽后党内虽有反对的声⾳，但农村的联产承包责任制都逐渐得以实施。这背后少不了邓⼩平的⽀持，后面还会提到，邓⼩平对赵紫阳改⾰意⻅的⽀持起到了⾄关重要的作⽤。1978年，⼗⼀届三中全会确立的农业所有制还是⼈民公社所有制。从提出包产到户到全国范围推行，只⽤了短短三年时间。⼀⽅面得益于邓⼩平的⽀持，另⼀⽅⾯也是因为包产到户的成效很明显，实⾏包产到户后，农业生产恢复得很快，所以阻力很快化解。 </p>\n<p>在对外开放的问题上，赵紫阳与邓⼩平的意⻅保持⼀致，⽀持邓⼩平设⽴特区。对外开放的阻⼒主要来⾃于党内的保守派，陈云、邓力群等。陈云是党内资历深厚的⽼⼈，在经济问题上的意⻅举⾜轻重。他最早从第⼀个五年计划就开始主持经济工作，三⼤改造完成后，是他最早提出计划经济为主，市场调节为辅的“⻦笼经济”思想。赵紫阳对陈云很敬重。他曾提过:“对陈云同志，我调中央工作的头几年对他很尊重。我觉得在⽼⼀代领导⼈中， 陈云对经济工作有深刻的研 究，有⾃己的真知灼⻅。 ”但是，陈云的思想却停留在了“⻦笼经济”，以⾄于在改⾰开放初期成为了保守派⽼⼈，阻碍了改⾰开放的进程。他认为外国资本家追求的是超额利润，引进外资不可能得到实惠，所以他对引进外资、对外开放很警惕和怀疑。1982年1⽉在沿海地区开展的打击经济犯罪活动，就是经陈云批示后开展的。这场活动在保守派的推动下很快升级为经济领域的反资产阶级⾃由化运动，从单纯的打击经济犯罪上升到思想领域的阶级⽃争。经过这样的打击，⼈们对改⾰开放的态度陷入混乱，顾虑重重，改⾰开放⼀度陷入停顿。赵紫阳在录⾳中提到: </p>\n<p><em>当时把经济上放宽搞活后难以完全避免的现象， 提高到“是在新的条件下阶级斗争的重要表现”，甚至还提出“是国内外阶级敌资本主义的腐朽思想对我们进行破坏、腐蚀的反映”。……这次提出打击经济犯罪活动，宣布经济特区也必须坚持以计划经济为主，市场调节为辅，这样来就没有什么特区了。还宣布要加强对外经济活动的统一管理。除国家规定的单位，按国家规定的原则和程序进行以外， 严禁任何单位和个体进行对外经济活动。这样就把原来在改革开放中，在建立特区时已经下放给特区的些权力取消。并且还规定要增加国家对农副产品的统购和派购，减少议价部分的比重。还提出把沿海工的奖金控制在略高于内地的水平。这样一来，打击沿海地区的经济犯罪活动，变成在经济上反对资产阶级由化，势必使整个改革开放的一些搞法被否定了，把已经下放的权力又收回来。</em> </p>\n<p>改⾰开放初期，赵紫阳结合欧洲考察和国内农村调查的观察，认识到对外贸易对于经济发展因地制宜、扬⻓避短的重要作⽤。后来又经过长期的摸索、思考，于1988年1月，赵紫阳提出了沿海发展战略。沿海发展战略是对外开放的进⼀步深化，目的是将国内经济推向世界经济，使之紧密结合。充分发􏰁国内劳动成本低的优势，发展进出⼝加工的劳动密集型产业，参与到世界经济的浪潮中。邓⼩平⾮常⽀持赵紫阳的沿海发展战略，虽然有来⾃包括陈云在内的各⽅不同意见，但最后还是通过并实施了。虽然89年之后不再提了，但其实还是按照这个路线来发展，事实证明沿海发展战略对于中国经济发展的意义是重⼤的。 </p>\n<p>之前提到，邓⼩平对赵紫阳在经济领域决策的⽀持对于改⾰开放的推进⾄关重要。赵紫阳是 从县委书记⼀步成为中央总书记的，他⼤部分政治⽣生涯在地⽅度过，1980年才调到北京。因此，赵在党内缺乏关系基础，他也说⾃己“渠道不多，消息⽐较闭塞。”所以，⾯面对党内保守派左派的冲击，邓⼩平这⼀元⽼的⽀持是赵紫阳坚实的后盾。 </p>\n<p>1987年胡耀邦下台后，党内出现了⼀股强烈的反⾃由化的声浪。为了防⽌止左倾⼒力量量将反⾃由化扩⼤，对改⾰造成伤害，赵紫阳从⼀开始就制定了《中共中央关于当前反对资产阶级⾃由化若⼲问题的通知》，对这场运动做出了严格的限制。但是，以邓力群、胡乔木等为代表的⼀些左派对这个限制很不满，他们制造舆论，极力突破⽂件的限制，企图将反⾃由化扩⼤到经济、科技、⽂艺各个领域。这使得改⾰开放⾯临严重威胁。邓⼒群等⼈利⽤左派控制下的刊物《红旗》、《⽂艺理论与批评》还有《光明⽇报》，组织舆论讨论，称反⾃由化是对⼗⼀届三中全会以来路线的第⼆次拨乱反正，想要否定三中全会以来的解放思想、改⾰开放路线。为了遏制反⾃由化扩⼤化的趋势，为即将到来的⼗三⼤进⼀步深化改⾰开放营造氛围，赵紫阳征得邓⼩平的同意后于1987年5⽉月13⽇发表了⼀次讲话。赵紫阳说: </p>\n<p><em>我于 1987 5 月 13 日在宣传、理论、新闻、党校部会议上讲了一次话。 由于邓在这时期先后对外宾讲贫穷不是社会主义;中国搞社会主义左的错误是主要的;只有生产力发展才能说是真的搞社会主义等等这样的话，所以，我这次讲话就显得更加有力，在讨论我的讲话稿的书记处、政治局会议上，我除对反由化当中出现的以左反右企图冲破中央四号件规定的种种干扰进行严肃的批评外，还反复阐述以下几个观点:</em></p>\n<p><em>第一，经过几个月的努力，大的气候已经发生变化，自由化泛滥的情况已经制止。今后应强调改开放这一面，十三大必须关成改革开放的会议。从现在起就要 开好十三大做好舆论准备。</em></p>\n<p><em>第二，这次开展反由化的斗争，目的是要解决自由化泛滥的问题。……为了解决自由化泛滥问题，我们个时期主要排除右的干扰是对的，但从长期从根本方面讲，改革开放的障碍主要来左的方面。</em> </p>\n<p><em>第三，四项基本原则是我们立国之本，改革开放是建设有中国特色社会主义的总方针、总政策。……</em></p>\n<p><em>第四，提出重视生产力标准的问题。……</em></p>\n<p><em>我的这篇讲话，受到绝大多数干部的拥护，整个空气在这篇讲话以后发生有利于改革开放的变化。这也为顺利地起草十三大报告创造了一个好的环境。</em></p>\n<p>紧接着，由于1988年的物价改⾰的失误，市场发生􏰀挤兑抢购，为了稳定⼈心，中央提出了 “治理整顿”的⼝号。以李鹏、姚依林和⼀些⽼⼈为代表的左派借机抨击赵紫阳的经济改⾰措施。 1989年元旦前，李鹏、姚依林在政治局常委⽣活会上指责赵紫阳对经济过问太多，但其实当时赵紫阳已被他们架空，⼏乎失去了对经济改⾰的掌控。这次⽭头直指赵紫阳在物价改⾰中的责任问题。邓⼩平得知开会内容后，很不⾼兴，讲了⼀⼤段话说他⽀持改⾰。1989年春节前夕，赵紫阳到邓⼩平家中，两⼈进⾏了⼀次深⼊谈话。邓⼩平表示他想退隐，辞去军委主席，从⽽带动 ⼀批⽼⼈的退位，减少⽼⼈们的⼲预。但赵紫阳坚决劝邓⼩平留下，因为他知道邓如果在这个时候退出，不仅不会减少⽼⼈们的⼲预，反而会使他的改⾰进程陷入孤⽴无援的境地。 </p>\n<p><em>从这段邓的谈话中，我觉察有人，也许不少老人向他施加压力，说我坏话， 而邓明确表示不受他们的影响，顶住他们这些压力。 1987年我担任总书记之后，逐渐聚集起来的一批反对我的保守力量，虽然他们非常嚣张地反对我，采用各种办法反对我，但由于没有得到邓的同意，始终未能得逞。这种情况直到 1989年发生那场政治风波后才改变。</em></p>\n<p>回顾赵紫阳改⾰开放⼗年，我看到了⼀个朴实的政治家对于国家经济改⾰热忱、孜孜不倦的思考，也看到了身处官场，与各⽅⼒量周旋，实现⾃⼰想法的不易。邓⼩平被称为改⾰开放的总设计师⽽称颂于世，⽽赵紫阳却因为⼀场政治风波落得悲剧的结局，不⻅于当代的历史。但好在历史的时间维度是很⻓的，只要它真实而连续地存在，就不会消灭永恒的价值。 </p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>⼀直以来，邓⼩平被⼈们当作中国改⾰开放的总设计师。但事实上，由于政治力量的⼲预， ⼀位重要⼈物长久以来被⼈们忽视，消失于话语的世界。他在中国改⾰开放进程中起到如此重要的作⽤，以⾄于任何对他的无视或轻视，都是对他不公正的对待。他就是前中共中央总书记赵紫阳。从1980年出任中央财经领导⼩组组长开始，直到1989年6月结束政治生涯，赵紫阳对改⾰开放的思考和推动贯彻始终。本⽂主要依据由赵紫阳录⾳整理⽽成的《改⾰历程》⼀书，尝试对赵这⼗年的作为、之于改⾰开放的影响、在历史中的评价做⼀个简单的梳理。这样建⽴起的个⼈认识可能是偏狭的(录⾳毕竟是⼀家之言)，但未必是没有意义的，因为⼀切深刻的认识都从幼稚和曲折中发展而来。","more":"</p>\n</blockquote>\n<p>1988年，赵紫阳在北京中南海接⻅美国著名经济学家弗里德曼。交流过后，弗里德曼称赵紫阳是他所⻅过的“社会主义国家里最好的经济学家”。这并不是无稽之谈。 </p>\n<p>早在1976年⽂⾰刚刚结束，还在四川任省委书记的赵紫阳就开始思考经济问题。1977-1978 两年，赵紫阳在四川30多个县市进⾏了农村调查，并开始逐步扩⼤农⺠⾃主权，制定了“农村经济政策⼗⼆条”。这是改⾰进⼊经济⽣活实质性的⼀步，也是赵紫阳改⾰之路的起点。从四川的经验中，赵紫阳得到⼀个认识:发展经济，效益不可忽视。因为他发现，建国以来，我国的经济⽣产总值增长了不少，而⼈均消费⽔平的增长却微乎其微。这说明国家经济总量的增⻓并没有给⼈民的⽣活⽔平带来相应程度的提升。为了改变这个情况，赵紫阳把经济效益摆在核心位置，这成了他日后深化改⾰的起点。1981年，刚刚上任国务院总理的赵紫阳向全国⼈⼤做了名为 《当前的经济形势和今后的经济政策》的报告，提出⼗条经济⽅针，核心就是把经济效益作为重点。更进⼀步，赵紫阳认识到经济效益问题根本上讲还是经济制度的问题。经济必须市场化，给予其充分的⾃由来⾃我调整，才有可能提高效益。 </p>\n<p>在农业上，赵紫阳继续扩⼤农⺠⾃主权。1980年任中央财经领导⼩组组长后，他提出首先在⼀亿⼈⼝的农村贫困社队实行包产到户，⽽后党内虽有反对的声⾳，但农村的联产承包责任制都逐渐得以实施。这背后少不了邓⼩平的⽀持，后面还会提到，邓⼩平对赵紫阳改⾰意⻅的⽀持起到了⾄关重要的作⽤。1978年，⼗⼀届三中全会确立的农业所有制还是⼈民公社所有制。从提出包产到户到全国范围推行，只⽤了短短三年时间。⼀⽅面得益于邓⼩平的⽀持，另⼀⽅⾯也是因为包产到户的成效很明显，实⾏包产到户后，农业生产恢复得很快，所以阻力很快化解。 </p>\n<p>在对外开放的问题上，赵紫阳与邓⼩平的意⻅保持⼀致，⽀持邓⼩平设⽴特区。对外开放的阻⼒主要来⾃于党内的保守派，陈云、邓力群等。陈云是党内资历深厚的⽼⼈，在经济问题上的意⻅举⾜轻重。他最早从第⼀个五年计划就开始主持经济工作，三⼤改造完成后，是他最早提出计划经济为主，市场调节为辅的“⻦笼经济”思想。赵紫阳对陈云很敬重。他曾提过:“对陈云同志，我调中央工作的头几年对他很尊重。我觉得在⽼⼀代领导⼈中， 陈云对经济工作有深刻的研 究，有⾃己的真知灼⻅。 ”但是，陈云的思想却停留在了“⻦笼经济”，以⾄于在改⾰开放初期成为了保守派⽼⼈，阻碍了改⾰开放的进程。他认为外国资本家追求的是超额利润，引进外资不可能得到实惠，所以他对引进外资、对外开放很警惕和怀疑。1982年1⽉在沿海地区开展的打击经济犯罪活动，就是经陈云批示后开展的。这场活动在保守派的推动下很快升级为经济领域的反资产阶级⾃由化运动，从单纯的打击经济犯罪上升到思想领域的阶级⽃争。经过这样的打击，⼈们对改⾰开放的态度陷入混乱，顾虑重重，改⾰开放⼀度陷入停顿。赵紫阳在录⾳中提到: </p>\n<p><em>当时把经济上放宽搞活后难以完全避免的现象， 提高到“是在新的条件下阶级斗争的重要表现”，甚至还提出“是国内外阶级敌资本主义的腐朽思想对我们进行破坏、腐蚀的反映”。……这次提出打击经济犯罪活动，宣布经济特区也必须坚持以计划经济为主，市场调节为辅，这样来就没有什么特区了。还宣布要加强对外经济活动的统一管理。除国家规定的单位，按国家规定的原则和程序进行以外， 严禁任何单位和个体进行对外经济活动。这样就把原来在改革开放中，在建立特区时已经下放给特区的些权力取消。并且还规定要增加国家对农副产品的统购和派购，减少议价部分的比重。还提出把沿海工的奖金控制在略高于内地的水平。这样一来，打击沿海地区的经济犯罪活动，变成在经济上反对资产阶级由化，势必使整个改革开放的一些搞法被否定了，把已经下放的权力又收回来。</em> </p>\n<p>改⾰开放初期，赵紫阳结合欧洲考察和国内农村调查的观察，认识到对外贸易对于经济发展因地制宜、扬⻓避短的重要作⽤。后来又经过长期的摸索、思考，于1988年1月，赵紫阳提出了沿海发展战略。沿海发展战略是对外开放的进⼀步深化，目的是将国内经济推向世界经济，使之紧密结合。充分发􏰁国内劳动成本低的优势，发展进出⼝加工的劳动密集型产业，参与到世界经济的浪潮中。邓⼩平⾮常⽀持赵紫阳的沿海发展战略，虽然有来⾃包括陈云在内的各⽅不同意见，但最后还是通过并实施了。虽然89年之后不再提了，但其实还是按照这个路线来发展，事实证明沿海发展战略对于中国经济发展的意义是重⼤的。 </p>\n<p>之前提到，邓⼩平对赵紫阳在经济领域决策的⽀持对于改⾰开放的推进⾄关重要。赵紫阳是 从县委书记⼀步成为中央总书记的，他⼤部分政治⽣生涯在地⽅度过，1980年才调到北京。因此，赵在党内缺乏关系基础，他也说⾃己“渠道不多，消息⽐较闭塞。”所以，⾯面对党内保守派左派的冲击，邓⼩平这⼀元⽼的⽀持是赵紫阳坚实的后盾。 </p>\n<p>1987年胡耀邦下台后，党内出现了⼀股强烈的反⾃由化的声浪。为了防⽌止左倾⼒力量量将反⾃由化扩⼤，对改⾰造成伤害，赵紫阳从⼀开始就制定了《中共中央关于当前反对资产阶级⾃由化若⼲问题的通知》，对这场运动做出了严格的限制。但是，以邓力群、胡乔木等为代表的⼀些左派对这个限制很不满，他们制造舆论，极力突破⽂件的限制，企图将反⾃由化扩⼤到经济、科技、⽂艺各个领域。这使得改⾰开放⾯临严重威胁。邓⼒群等⼈利⽤左派控制下的刊物《红旗》、《⽂艺理论与批评》还有《光明⽇报》，组织舆论讨论，称反⾃由化是对⼗⼀届三中全会以来路线的第⼆次拨乱反正，想要否定三中全会以来的解放思想、改⾰开放路线。为了遏制反⾃由化扩⼤化的趋势，为即将到来的⼗三⼤进⼀步深化改⾰开放营造氛围，赵紫阳征得邓⼩平的同意后于1987年5⽉月13⽇发表了⼀次讲话。赵紫阳说: </p>\n<p><em>我于 1987 5 月 13 日在宣传、理论、新闻、党校部会议上讲了一次话。 由于邓在这时期先后对外宾讲贫穷不是社会主义;中国搞社会主义左的错误是主要的;只有生产力发展才能说是真的搞社会主义等等这样的话，所以，我这次讲话就显得更加有力，在讨论我的讲话稿的书记处、政治局会议上，我除对反由化当中出现的以左反右企图冲破中央四号件规定的种种干扰进行严肃的批评外，还反复阐述以下几个观点:</em></p>\n<p><em>第一，经过几个月的努力，大的气候已经发生变化，自由化泛滥的情况已经制止。今后应强调改开放这一面，十三大必须关成改革开放的会议。从现在起就要 开好十三大做好舆论准备。</em></p>\n<p><em>第二，这次开展反由化的斗争，目的是要解决自由化泛滥的问题。……为了解决自由化泛滥问题，我们个时期主要排除右的干扰是对的，但从长期从根本方面讲，改革开放的障碍主要来左的方面。</em> </p>\n<p><em>第三，四项基本原则是我们立国之本，改革开放是建设有中国特色社会主义的总方针、总政策。……</em></p>\n<p><em>第四，提出重视生产力标准的问题。……</em></p>\n<p><em>我的这篇讲话，受到绝大多数干部的拥护，整个空气在这篇讲话以后发生有利于改革开放的变化。这也为顺利地起草十三大报告创造了一个好的环境。</em></p>\n<p>紧接着，由于1988年的物价改⾰的失误，市场发生􏰀挤兑抢购，为了稳定⼈心，中央提出了 “治理整顿”的⼝号。以李鹏、姚依林和⼀些⽼⼈为代表的左派借机抨击赵紫阳的经济改⾰措施。 1989年元旦前，李鹏、姚依林在政治局常委⽣活会上指责赵紫阳对经济过问太多，但其实当时赵紫阳已被他们架空，⼏乎失去了对经济改⾰的掌控。这次⽭头直指赵紫阳在物价改⾰中的责任问题。邓⼩平得知开会内容后，很不⾼兴，讲了⼀⼤段话说他⽀持改⾰。1989年春节前夕，赵紫阳到邓⼩平家中，两⼈进⾏了⼀次深⼊谈话。邓⼩平表示他想退隐，辞去军委主席，从⽽带动 ⼀批⽼⼈的退位，减少⽼⼈们的⼲预。但赵紫阳坚决劝邓⼩平留下，因为他知道邓如果在这个时候退出，不仅不会减少⽼⼈们的⼲预，反而会使他的改⾰进程陷入孤⽴无援的境地。 </p>\n<p><em>从这段邓的谈话中，我觉察有人，也许不少老人向他施加压力，说我坏话， 而邓明确表示不受他们的影响，顶住他们这些压力。 1987年我担任总书记之后，逐渐聚集起来的一批反对我的保守力量，虽然他们非常嚣张地反对我，采用各种办法反对我，但由于没有得到邓的同意，始终未能得逞。这种情况直到 1989年发生那场政治风波后才改变。</em></p>\n<p>回顾赵紫阳改⾰开放⼗年，我看到了⼀个朴实的政治家对于国家经济改⾰热忱、孜孜不倦的思考，也看到了身处官场，与各⽅⼒量周旋，实现⾃⼰想法的不易。邓⼩平被称为改⾰开放的总设计师⽽称颂于世，⽽赵紫阳却因为⼀场政治风波落得悲剧的结局，不⻅于当代的历史。但好在历史的时间维度是很⻓的，只要它真实而连续地存在，就不会消灭永恒的价值。 </p>"},{"title":"Python环境（重）配置","date":"2020-05-27T18:00:00.000Z","_content":"\n\n\n今天使用conda更新python pkg的时候出现了[问题](https://github.com/conda/conda/issues/9038),导致jupyter notebook、之前写过的python程序，在import module的时候报出各种各样的错误，所以打算重装miniconda，重建一遍环境。<!--more-->\n\n### 安装miniconda3\n\nMiniconda是anaconda的精简版本，只包含python和conda，需要用到什么包时再安装。PC机上装miniconda非常轻便灵活。\n\n重装前把之前的卸载干净，卸载方法见[miniconda官网](https://conda.io/projects/conda/en/latest/user-guide/install/macos.html)\n\n到[miniconda官网](https://docs.conda.io/en/latest/miniconda.html)或国内的镜像网站（[清华镜像](https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/)）下载对应的Mac pkg，目的宗卷选择Macintosh HD\n\n在终端输入`conda -V`检查是否安装成功。\n\n<br/>\n\n### 创建新的python虚拟环境\n\n在终端输入`conda create -n condapy3 python`，创建一个名为condapy3的虚拟环境，miniconda3默认创建python3.8.3版本\n\n进入虚拟环境`conda activate condapy3`\n\n退出虚拟环境`conda deactivate`\n\n查看虚拟环境列表`\tconda env list`\n\n删除虚拟环境`conda remove -n my_env --all`\n\n<br/>\n\n### 安装Jupiter notebook\n\n选择在根环境下安装jupyter，安装指令`conda install jupyter notebook`\n\n为了使Jupyter notebook可以使用不同的Conda虚拟环境，需要在notebook所在的环境中安装`nb_conda_kernels`包，并在其他需要用到的环境下安装`ipykernel`包。\n\n```bash\nconda install nb_conda_kernels\nconda activate condapy3\nconda install ipykernel\n```\n\n重启jupyter notebook，就可以在服务-改变服务里找到目标虚拟环境。\n\n<br/>\n\n### 安装各种包\n\n这次打算把各种python pkg都装在虚拟环境里，逐渐习惯使用虚拟环境\n\n安装列表：\n\n- 科学计算（numpy, pandas, scipy）\n\n- 绘图（matplotlib, seaborn）\n\n- 网页爬取（requests, beautifulsoup4)\n\n- 语词分析（jieba, wordcloud) 不能直接从conda官方库安装，安装方法见[CSDN](https://blog.csdn.net/zhaohaibo_/article/details/79253740)\n\n`conda list`查看已安装的包列表\n\n<br/>\n\n### 使用镜像源(可选)\n\n使用anaconda官网的源时经常出现下载慢、中断的情况，解决办法是为conda添加国内的镜像源，如添加清华的镜像源：\n\n`conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/`\n\n设置搜索时显示通道地址(生效)\n\n`conda config --set show_channel_urls yes`\n\n检查是否添加成功\n\n`conda config --show-sources`\n\n移除源\n\n`conda config --remove channels xxxx`\n\n<br/>\n\n### 在Pycharm中使用conda虚拟环境\n\n打开Pycharm的preference-project intepreter-conda env，设置python解释器为\n\n![](/images/image-20200528015900321.png)\n\n<br/>\n\n### 在python中使用Matlab\n\n[Matlab官网](https://ww2.mathworks.cn/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)写的很详细，还有[知乎](https://zhuanlan.zhihu.com/p/47655091)这篇。\n\n<br/>\n\n<br/>","source":"_posts/python_environment.md","raw":"---\ntitle: Python环境（重）配置\ndate: 2020-05-28 02:00:00\ncategories:\n- 计算机科学\ntags: \n- python\n\n---\n\n\n\n今天使用conda更新python pkg的时候出现了[问题](https://github.com/conda/conda/issues/9038),导致jupyter notebook、之前写过的python程序，在import module的时候报出各种各样的错误，所以打算重装miniconda，重建一遍环境。<!--more-->\n\n### 安装miniconda3\n\nMiniconda是anaconda的精简版本，只包含python和conda，需要用到什么包时再安装。PC机上装miniconda非常轻便灵活。\n\n重装前把之前的卸载干净，卸载方法见[miniconda官网](https://conda.io/projects/conda/en/latest/user-guide/install/macos.html)\n\n到[miniconda官网](https://docs.conda.io/en/latest/miniconda.html)或国内的镜像网站（[清华镜像](https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/)）下载对应的Mac pkg，目的宗卷选择Macintosh HD\n\n在终端输入`conda -V`检查是否安装成功。\n\n<br/>\n\n### 创建新的python虚拟环境\n\n在终端输入`conda create -n condapy3 python`，创建一个名为condapy3的虚拟环境，miniconda3默认创建python3.8.3版本\n\n进入虚拟环境`conda activate condapy3`\n\n退出虚拟环境`conda deactivate`\n\n查看虚拟环境列表`\tconda env list`\n\n删除虚拟环境`conda remove -n my_env --all`\n\n<br/>\n\n### 安装Jupiter notebook\n\n选择在根环境下安装jupyter，安装指令`conda install jupyter notebook`\n\n为了使Jupyter notebook可以使用不同的Conda虚拟环境，需要在notebook所在的环境中安装`nb_conda_kernels`包，并在其他需要用到的环境下安装`ipykernel`包。\n\n```bash\nconda install nb_conda_kernels\nconda activate condapy3\nconda install ipykernel\n```\n\n重启jupyter notebook，就可以在服务-改变服务里找到目标虚拟环境。\n\n<br/>\n\n### 安装各种包\n\n这次打算把各种python pkg都装在虚拟环境里，逐渐习惯使用虚拟环境\n\n安装列表：\n\n- 科学计算（numpy, pandas, scipy）\n\n- 绘图（matplotlib, seaborn）\n\n- 网页爬取（requests, beautifulsoup4)\n\n- 语词分析（jieba, wordcloud) 不能直接从conda官方库安装，安装方法见[CSDN](https://blog.csdn.net/zhaohaibo_/article/details/79253740)\n\n`conda list`查看已安装的包列表\n\n<br/>\n\n### 使用镜像源(可选)\n\n使用anaconda官网的源时经常出现下载慢、中断的情况，解决办法是为conda添加国内的镜像源，如添加清华的镜像源：\n\n`conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/`\n\n设置搜索时显示通道地址(生效)\n\n`conda config --set show_channel_urls yes`\n\n检查是否添加成功\n\n`conda config --show-sources`\n\n移除源\n\n`conda config --remove channels xxxx`\n\n<br/>\n\n### 在Pycharm中使用conda虚拟环境\n\n打开Pycharm的preference-project intepreter-conda env，设置python解释器为\n\n![](/images/image-20200528015900321.png)\n\n<br/>\n\n### 在python中使用Matlab\n\n[Matlab官网](https://ww2.mathworks.cn/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)写的很详细，还有[知乎](https://zhuanlan.zhihu.com/p/47655091)这篇。\n\n<br/>\n\n<br/>","slug":"python_environment","published":1,"updated":"2020-09-17T15:36:23.830Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6r00222sfy042724an","content":"<p>今天使用conda更新python pkg的时候出现了<a href=\"https://github.com/conda/conda/issues/9038\" target=\"_blank\" rel=\"noopener\">问题</a>,导致jupyter notebook、之前写过的python程序，在import module的时候报出各种各样的错误，所以打算重装miniconda，重建一遍环境。<a id=\"more\"></a></p>\n<h3 id=\"安装miniconda3\"><a href=\"#安装miniconda3\" class=\"headerlink\" title=\"安装miniconda3\"></a>安装miniconda3</h3><p>Miniconda是anaconda的精简版本，只包含python和conda，需要用到什么包时再安装。PC机上装miniconda非常轻便灵活。</p>\n<p>重装前把之前的卸载干净，卸载方法见<a href=\"https://conda.io/projects/conda/en/latest/user-guide/install/macos.html\" target=\"_blank\" rel=\"noopener\">miniconda官网</a></p>\n<p>到<a href=\"https://docs.conda.io/en/latest/miniconda.html\" target=\"_blank\" rel=\"noopener\">miniconda官网</a>或国内的镜像网站（<a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/\" target=\"_blank\" rel=\"noopener\">清华镜像</a>）下载对应的Mac pkg，目的宗卷选择Macintosh HD</p>\n<p>在终端输入<code>conda -V</code>检查是否安装成功。</p>\n<p><br/></p>\n<h3 id=\"创建新的python虚拟环境\"><a href=\"#创建新的python虚拟环境\" class=\"headerlink\" title=\"创建新的python虚拟环境\"></a>创建新的python虚拟环境</h3><p>在终端输入<code>conda create -n condapy3 python</code>，创建一个名为condapy3的虚拟环境，miniconda3默认创建python3.8.3版本</p>\n<p>进入虚拟环境<code>conda activate condapy3</code></p>\n<p>退出虚拟环境<code>conda deactivate</code></p>\n<p>查看虚拟环境列表<code>conda env list</code></p>\n<p>删除虚拟环境<code>conda remove -n my_env --all</code></p>\n<p><br/></p>\n<h3 id=\"安装Jupiter-notebook\"><a href=\"#安装Jupiter-notebook\" class=\"headerlink\" title=\"安装Jupiter notebook\"></a>安装Jupiter notebook</h3><p>选择在根环境下安装jupyter，安装指令<code>conda install jupyter notebook</code></p>\n<p>为了使Jupyter notebook可以使用不同的Conda虚拟环境，需要在notebook所在的环境中安装<code>nb_conda_kernels</code>包，并在其他需要用到的环境下安装<code>ipykernel</code>包。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda_kernels</span><br><span class=\"line\">conda activate condapy3</span><br><span class=\"line\">conda install ipykernel</span><br></pre></td></tr></table></figure>\n<p>重启jupyter notebook，就可以在服务-改变服务里找到目标虚拟环境。</p>\n<p><br/></p>\n<h3 id=\"安装各种包\"><a href=\"#安装各种包\" class=\"headerlink\" title=\"安装各种包\"></a>安装各种包</h3><p>这次打算把各种python pkg都装在虚拟环境里，逐渐习惯使用虚拟环境</p>\n<p>安装列表：</p>\n<ul>\n<li><p>科学计算（numpy, pandas, scipy）</p>\n</li>\n<li><p>绘图（matplotlib, seaborn）</p>\n</li>\n<li><p>网页爬取（requests, beautifulsoup4)</p>\n</li>\n<li><p>语词分析（jieba, wordcloud) 不能直接从conda官方库安装，安装方法见<a href=\"https://blog.csdn.net/zhaohaibo_/article/details/79253740\" target=\"_blank\" rel=\"noopener\">CSDN</a></p>\n</li>\n</ul>\n<p><code>conda list</code>查看已安装的包列表</p>\n<p><br/></p>\n<h3 id=\"使用镜像源-可选\"><a href=\"#使用镜像源-可选\" class=\"headerlink\" title=\"使用镜像源(可选)\"></a>使用镜像源(可选)</h3><p>使用anaconda官网的源时经常出现下载慢、中断的情况，解决办法是为conda添加国内的镜像源，如添加清华的镜像源：</p>\n<p><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</code></p>\n<p>设置搜索时显示通道地址(生效)</p>\n<p><code>conda config --set show_channel_urls yes</code></p>\n<p>检查是否添加成功</p>\n<p><code>conda config --show-sources</code></p>\n<p>移除源</p>\n<p><code>conda config --remove channels xxxx</code></p>\n<p><br/></p>\n<h3 id=\"在Pycharm中使用conda虚拟环境\"><a href=\"#在Pycharm中使用conda虚拟环境\" class=\"headerlink\" title=\"在Pycharm中使用conda虚拟环境\"></a>在Pycharm中使用conda虚拟环境</h3><p>打开Pycharm的preference-project intepreter-conda env，设置python解释器为</p>\n<p><img src=\"/images/image-20200528015900321.png\" alt=\"\"></p>\n<p><br/></p>\n<h3 id=\"在python中使用Matlab\"><a href=\"#在python中使用Matlab\" class=\"headerlink\" title=\"在python中使用Matlab\"></a>在python中使用Matlab</h3><p><a href=\"https://ww2.mathworks.cn/help/matlab/matlab_external/install-the-matlab-engine-for-python.html\" target=\"_blank\" rel=\"noopener\">Matlab官网</a>写的很详细，还有<a href=\"https://zhuanlan.zhihu.com/p/47655091\" target=\"_blank\" rel=\"noopener\">知乎</a>这篇。</p>\n<p><br/></p>\n<p><br/></p>\n","site":{"data":{}},"excerpt":"<p>今天使用conda更新python pkg的时候出现了<a href=\"https://github.com/conda/conda/issues/9038\" target=\"_blank\" rel=\"noopener\">问题</a>,导致jupyter notebook、之前写过的python程序，在import module的时候报出各种各样的错误，所以打算重装miniconda，重建一遍环境。","more":"</p>\n<h3 id=\"安装miniconda3\"><a href=\"#安装miniconda3\" class=\"headerlink\" title=\"安装miniconda3\"></a>安装miniconda3</h3><p>Miniconda是anaconda的精简版本，只包含python和conda，需要用到什么包时再安装。PC机上装miniconda非常轻便灵活。</p>\n<p>重装前把之前的卸载干净，卸载方法见<a href=\"https://conda.io/projects/conda/en/latest/user-guide/install/macos.html\" target=\"_blank\" rel=\"noopener\">miniconda官网</a></p>\n<p>到<a href=\"https://docs.conda.io/en/latest/miniconda.html\" target=\"_blank\" rel=\"noopener\">miniconda官网</a>或国内的镜像网站（<a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/\" target=\"_blank\" rel=\"noopener\">清华镜像</a>）下载对应的Mac pkg，目的宗卷选择Macintosh HD</p>\n<p>在终端输入<code>conda -V</code>检查是否安装成功。</p>\n<p><br/></p>\n<h3 id=\"创建新的python虚拟环境\"><a href=\"#创建新的python虚拟环境\" class=\"headerlink\" title=\"创建新的python虚拟环境\"></a>创建新的python虚拟环境</h3><p>在终端输入<code>conda create -n condapy3 python</code>，创建一个名为condapy3的虚拟环境，miniconda3默认创建python3.8.3版本</p>\n<p>进入虚拟环境<code>conda activate condapy3</code></p>\n<p>退出虚拟环境<code>conda deactivate</code></p>\n<p>查看虚拟环境列表<code>conda env list</code></p>\n<p>删除虚拟环境<code>conda remove -n my_env --all</code></p>\n<p><br/></p>\n<h3 id=\"安装Jupiter-notebook\"><a href=\"#安装Jupiter-notebook\" class=\"headerlink\" title=\"安装Jupiter notebook\"></a>安装Jupiter notebook</h3><p>选择在根环境下安装jupyter，安装指令<code>conda install jupyter notebook</code></p>\n<p>为了使Jupyter notebook可以使用不同的Conda虚拟环境，需要在notebook所在的环境中安装<code>nb_conda_kernels</code>包，并在其他需要用到的环境下安装<code>ipykernel</code>包。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda_kernels</span><br><span class=\"line\">conda activate condapy3</span><br><span class=\"line\">conda install ipykernel</span><br></pre></td></tr></table></figure>\n<p>重启jupyter notebook，就可以在服务-改变服务里找到目标虚拟环境。</p>\n<p><br/></p>\n<h3 id=\"安装各种包\"><a href=\"#安装各种包\" class=\"headerlink\" title=\"安装各种包\"></a>安装各种包</h3><p>这次打算把各种python pkg都装在虚拟环境里，逐渐习惯使用虚拟环境</p>\n<p>安装列表：</p>\n<ul>\n<li><p>科学计算（numpy, pandas, scipy）</p>\n</li>\n<li><p>绘图（matplotlib, seaborn）</p>\n</li>\n<li><p>网页爬取（requests, beautifulsoup4)</p>\n</li>\n<li><p>语词分析（jieba, wordcloud) 不能直接从conda官方库安装，安装方法见<a href=\"https://blog.csdn.net/zhaohaibo_/article/details/79253740\" target=\"_blank\" rel=\"noopener\">CSDN</a></p>\n</li>\n</ul>\n<p><code>conda list</code>查看已安装的包列表</p>\n<p><br/></p>\n<h3 id=\"使用镜像源-可选\"><a href=\"#使用镜像源-可选\" class=\"headerlink\" title=\"使用镜像源(可选)\"></a>使用镜像源(可选)</h3><p>使用anaconda官网的源时经常出现下载慢、中断的情况，解决办法是为conda添加国内的镜像源，如添加清华的镜像源：</p>\n<p><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</code></p>\n<p>设置搜索时显示通道地址(生效)</p>\n<p><code>conda config --set show_channel_urls yes</code></p>\n<p>检查是否添加成功</p>\n<p><code>conda config --show-sources</code></p>\n<p>移除源</p>\n<p><code>conda config --remove channels xxxx</code></p>\n<p><br/></p>\n<h3 id=\"在Pycharm中使用conda虚拟环境\"><a href=\"#在Pycharm中使用conda虚拟环境\" class=\"headerlink\" title=\"在Pycharm中使用conda虚拟环境\"></a>在Pycharm中使用conda虚拟环境</h3><p>打开Pycharm的preference-project intepreter-conda env，设置python解释器为</p>\n<p><img src=\"/images/image-20200528015900321.png\" alt=\"\"></p>\n<p><br/></p>\n<h3 id=\"在python中使用Matlab\"><a href=\"#在python中使用Matlab\" class=\"headerlink\" title=\"在python中使用Matlab\"></a>在python中使用Matlab</h3><p><a href=\"https://ww2.mathworks.cn/help/matlab/matlab_external/install-the-matlab-engine-for-python.html\" target=\"_blank\" rel=\"noopener\">Matlab官网</a>写的很详细，还有<a href=\"https://zhuanlan.zhihu.com/p/47655091\" target=\"_blank\" rel=\"noopener\">知乎</a>这篇。</p>\n<p><br/></p>\n<p><br/></p>"},{"title":"读柯朗《什么是数学》","date":"2018-07-07T16:02:47.000Z","mathjax":true,"_content":"身为一个理科生，从小学一年级开始，数学一直是我的必修科目。从自然数四则运算到线性代数、偏微分方程，数学的学习逐步深入，但是如果有人问我，到底什么是数学，恐怕我还真的讲不清楚。\n<!--more-->\n\n小学的时候有同学问过老师，学数学有什么用，出门买菜又用不到勾股定理，一元二次方程都绝少会用到吧。老师说，数学不仅是算术，还是对思维能力的锻炼，是思维的“体操”。在这种说法的基础上，我们稍加延伸，就可以做出一个类比，来说明数学和其他科学的分别与联系:假设世界由许多岛屿组成，人只有两种交通工具，分别是飞机和汽车。想要到达岛屿内的某个目的地，人可以选择走路或搭乘汽车；而想要到达其他岛屿，则只能乘坐飞机。那么，数学就是岛上的汽车，飞机就相当于其他的自然科学或社会科学知识。\n\n这个类比的潜在逻辑是，数学的本质是演绎法，任何数学知识都可在脑中通过思维得出，就像走路和电车都能到达岛屿任何地方，只是速度快慢的差别；而科学除了演绎法还必须能够归纳检验，必须具有外部经验性。就像电车不能代替飞机跨越海洋一样，电磁感应定律也不能仅凭思维实验得出。既然数学可以仅凭思维创造，那它是不是可以脱离现实，完全成为数学家们思维演绎的游戏呢？这就牵涉到数学的根源问题，是什么把数学同天马行空的推理游戏区别开来，使其成为今天的数学。\n\n回到最基本的数域来，这个问题或许能得到解答。跟随柯朗，我重新回顾了再熟悉不过、在我眼里早已是理所当然的自然数数域，如何扩大到复数数域的过程。\n\n自然数是如何产生的？在几千年前，东方的古巴比伦就已经有了数的概念，并发展出了简单的数的计算规则。自然数是随着度量有限已知量的需要应运而生的，简而言之，是为了计数。比如，为了度量袋子里苹果的量，因为苹果是分立且有限的，所以可以定义自然数1，2，3，4......来度量。这是自然而然的过程，似乎没有什么阻碍。然而，数域的下 一步扩大，即从自然数扩大到零和负数，则是漫长而曲折的过程。\n\n上面提到，自然数的产生是与现实的需要紧密相关的。而零和负数的产生，则主要依靠推动数学发展的另一典型推动力：突破限制并且自洽的追求。实现这一思维的跨越，花费了人类几个世纪的时间。创造出自然数之后，人们定义了一套自然数运算的规则，加法运算如1+1=2，乘法运算如1x2=2，这当然也适应现实计数的需要。不过，对于它们的逆运算:减法和除法，是有限制条件的。表现在:减法运算a-b，必须要求a>b，否则计算结果无法表示。如果定义新的数，能够表示a-a , a-b(a<b)的值，这个问题就可以解决。所以，人 们用0=a-a ; -(b-a)=a-b来表示它们，零和负数也就产生了。除了简单的加法和乘法基本运算规则外，自然数运算还有一些规律，如\n\n- 加法交换律 $a+b=b+a$\n- 加法结合律 $a+(b+c)=(a+b)+c$ \n- 乘法交换律 $a·b=b·a$ \n- 乘法结合律 $a(b·c)=(a·b)·c$\n- 乘法分配律 $a·(b+c)=ab+ac$\n\n为了使负数仍然满足上述定律，定义负数的乘法(-1)*(-1)=1，这样就使得扩大后的数域仍然满足原有的规则和规律，实现自洽。\n\n此后整数域到有理数域的扩大，也是受着外部现实度量需要和内部突破算术限制需要两方面的推动。整数可以描述某一度量单位下的数量，但对于可以无限细分的量，如时间、 面积，则不方便描述。很简单的例子，一个商人可以轻易表达两个苹果的概念，因为没人想把苹果切成60块来计量，但想要描述时间的话，可以用天，小时，分，秒，秒又可以继 续拆分60份成毫秒，毫秒又可以拆成微秒......当面对时间这样一个连续的，可以无限细分的量时，整数就不够用了——如何用整数表达1min=1/60h这样的概念呢？由此我们可以看 出引入“分数”的必要了。同样，从数学内部的突破限制角度来看，当除法运算a/b，a不是b 的整数倍时，计算结果如何表达呢？为了突破除法运算的限制，引入分数的概念，同时定义其运算规则:\n$$\n\\begin{eqnarray}\n\\frac{a}{b}+\\frac{c}{d}=\\frac{ad+bc}{bd}\\\\\n\\frac{a}{b}·\\frac{c}{d}=\\frac{ac}{bd}\\\\\n\\frac{a}{a}=1\\\\\n\\frac{ac}{bc}=\\frac{a}{b}\n\\end{eqnarray}\n$$\n这样，数域就扩大到有理数，扩大后的数域仍然适用整数的运算规律，实现自洽。这时，再回过头试图回答最初的那个问题，是什么使数学与一般的演绎区别开来，成为今天深刻改变了世界的数学。从数域扩大的历程中可以窥见，数学的发展是与现实需要的驱动分不开的。数学的定义的创造，公理体系的建立，除了满足基本的内部逻辑自洽之外，从来都是与现实世界相关联照应的。数学史上经历过希腊欧多克斯、欧几里得沉迷于公理演绎的时代，尽管这种公理化一定程度上能够帮助我们更深刻地认识数学规律和事实，但事实证明，脱离了物理现实的这种倾向是危险的——希腊几何在较早的认识到“不可度量”之后，便沉溺到公理演绎中去了，使得本应是必然的数的概念和运算推迟了两千年才出现——这是科学史上一个重大而奇怪的曲折。假如数学真的脱离现实，其定义和功理可以任意创造，只需满足逻辑自洽，那么它将失去一切动力和目标，退化为演绎法的游戏。","source":"_posts/reading Whatismath.md","raw":"---\ntitle: 读柯朗《什么是数学》\ndate: 2018-07-08 00:02:47\nmathjax: true\ncategories:\n- 数理逻辑\ntags: \n- 数学\n---\n身为一个理科生，从小学一年级开始，数学一直是我的必修科目。从自然数四则运算到线性代数、偏微分方程，数学的学习逐步深入，但是如果有人问我，到底什么是数学，恐怕我还真的讲不清楚。\n<!--more-->\n\n小学的时候有同学问过老师，学数学有什么用，出门买菜又用不到勾股定理，一元二次方程都绝少会用到吧。老师说，数学不仅是算术，还是对思维能力的锻炼，是思维的“体操”。在这种说法的基础上，我们稍加延伸，就可以做出一个类比，来说明数学和其他科学的分别与联系:假设世界由许多岛屿组成，人只有两种交通工具，分别是飞机和汽车。想要到达岛屿内的某个目的地，人可以选择走路或搭乘汽车；而想要到达其他岛屿，则只能乘坐飞机。那么，数学就是岛上的汽车，飞机就相当于其他的自然科学或社会科学知识。\n\n这个类比的潜在逻辑是，数学的本质是演绎法，任何数学知识都可在脑中通过思维得出，就像走路和电车都能到达岛屿任何地方，只是速度快慢的差别；而科学除了演绎法还必须能够归纳检验，必须具有外部经验性。就像电车不能代替飞机跨越海洋一样，电磁感应定律也不能仅凭思维实验得出。既然数学可以仅凭思维创造，那它是不是可以脱离现实，完全成为数学家们思维演绎的游戏呢？这就牵涉到数学的根源问题，是什么把数学同天马行空的推理游戏区别开来，使其成为今天的数学。\n\n回到最基本的数域来，这个问题或许能得到解答。跟随柯朗，我重新回顾了再熟悉不过、在我眼里早已是理所当然的自然数数域，如何扩大到复数数域的过程。\n\n自然数是如何产生的？在几千年前，东方的古巴比伦就已经有了数的概念，并发展出了简单的数的计算规则。自然数是随着度量有限已知量的需要应运而生的，简而言之，是为了计数。比如，为了度量袋子里苹果的量，因为苹果是分立且有限的，所以可以定义自然数1，2，3，4......来度量。这是自然而然的过程，似乎没有什么阻碍。然而，数域的下 一步扩大，即从自然数扩大到零和负数，则是漫长而曲折的过程。\n\n上面提到，自然数的产生是与现实的需要紧密相关的。而零和负数的产生，则主要依靠推动数学发展的另一典型推动力：突破限制并且自洽的追求。实现这一思维的跨越，花费了人类几个世纪的时间。创造出自然数之后，人们定义了一套自然数运算的规则，加法运算如1+1=2，乘法运算如1x2=2，这当然也适应现实计数的需要。不过，对于它们的逆运算:减法和除法，是有限制条件的。表现在:减法运算a-b，必须要求a>b，否则计算结果无法表示。如果定义新的数，能够表示a-a , a-b(a<b)的值，这个问题就可以解决。所以，人 们用0=a-a ; -(b-a)=a-b来表示它们，零和负数也就产生了。除了简单的加法和乘法基本运算规则外，自然数运算还有一些规律，如\n\n- 加法交换律 $a+b=b+a$\n- 加法结合律 $a+(b+c)=(a+b)+c$ \n- 乘法交换律 $a·b=b·a$ \n- 乘法结合律 $a(b·c)=(a·b)·c$\n- 乘法分配律 $a·(b+c)=ab+ac$\n\n为了使负数仍然满足上述定律，定义负数的乘法(-1)*(-1)=1，这样就使得扩大后的数域仍然满足原有的规则和规律，实现自洽。\n\n此后整数域到有理数域的扩大，也是受着外部现实度量需要和内部突破算术限制需要两方面的推动。整数可以描述某一度量单位下的数量，但对于可以无限细分的量，如时间、 面积，则不方便描述。很简单的例子，一个商人可以轻易表达两个苹果的概念，因为没人想把苹果切成60块来计量，但想要描述时间的话，可以用天，小时，分，秒，秒又可以继 续拆分60份成毫秒，毫秒又可以拆成微秒......当面对时间这样一个连续的，可以无限细分的量时，整数就不够用了——如何用整数表达1min=1/60h这样的概念呢？由此我们可以看 出引入“分数”的必要了。同样，从数学内部的突破限制角度来看，当除法运算a/b，a不是b 的整数倍时，计算结果如何表达呢？为了突破除法运算的限制，引入分数的概念，同时定义其运算规则:\n$$\n\\begin{eqnarray}\n\\frac{a}{b}+\\frac{c}{d}=\\frac{ad+bc}{bd}\\\\\n\\frac{a}{b}·\\frac{c}{d}=\\frac{ac}{bd}\\\\\n\\frac{a}{a}=1\\\\\n\\frac{ac}{bc}=\\frac{a}{b}\n\\end{eqnarray}\n$$\n这样，数域就扩大到有理数，扩大后的数域仍然适用整数的运算规律，实现自洽。这时，再回过头试图回答最初的那个问题，是什么使数学与一般的演绎区别开来，成为今天深刻改变了世界的数学。从数域扩大的历程中可以窥见，数学的发展是与现实需要的驱动分不开的。数学的定义的创造，公理体系的建立，除了满足基本的内部逻辑自洽之外，从来都是与现实世界相关联照应的。数学史上经历过希腊欧多克斯、欧几里得沉迷于公理演绎的时代，尽管这种公理化一定程度上能够帮助我们更深刻地认识数学规律和事实，但事实证明，脱离了物理现实的这种倾向是危险的——希腊几何在较早的认识到“不可度量”之后，便沉溺到公理演绎中去了，使得本应是必然的数的概念和运算推迟了两千年才出现——这是科学史上一个重大而奇怪的曲折。假如数学真的脱离现实，其定义和功理可以任意创造，只需满足逻辑自洽，那么它将失去一切动力和目标，退化为演绎法的游戏。","slug":"reading Whatismath","published":1,"updated":"2020-03-23T05:41:43.200Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6s00252sfy1xv7eq92","content":"<p>身为一个理科生，从小学一年级开始，数学一直是我的必修科目。从自然数四则运算到线性代数、偏微分方程，数学的学习逐步深入，但是如果有人问我，到底什么是数学，恐怕我还真的讲不清楚。<br><a id=\"more\"></a></p>\n<p>小学的时候有同学问过老师，学数学有什么用，出门买菜又用不到勾股定理，一元二次方程都绝少会用到吧。老师说，数学不仅是算术，还是对思维能力的锻炼，是思维的“体操”。在这种说法的基础上，我们稍加延伸，就可以做出一个类比，来说明数学和其他科学的分别与联系:假设世界由许多岛屿组成，人只有两种交通工具，分别是飞机和汽车。想要到达岛屿内的某个目的地，人可以选择走路或搭乘汽车；而想要到达其他岛屿，则只能乘坐飞机。那么，数学就是岛上的汽车，飞机就相当于其他的自然科学或社会科学知识。</p>\n<p>这个类比的潜在逻辑是，数学的本质是演绎法，任何数学知识都可在脑中通过思维得出，就像走路和电车都能到达岛屿任何地方，只是速度快慢的差别；而科学除了演绎法还必须能够归纳检验，必须具有外部经验性。就像电车不能代替飞机跨越海洋一样，电磁感应定律也不能仅凭思维实验得出。既然数学可以仅凭思维创造，那它是不是可以脱离现实，完全成为数学家们思维演绎的游戏呢？这就牵涉到数学的根源问题，是什么把数学同天马行空的推理游戏区别开来，使其成为今天的数学。</p>\n<p>回到最基本的数域来，这个问题或许能得到解答。跟随柯朗，我重新回顾了再熟悉不过、在我眼里早已是理所当然的自然数数域，如何扩大到复数数域的过程。</p>\n<p>自然数是如何产生的？在几千年前，东方的古巴比伦就已经有了数的概念，并发展出了简单的数的计算规则。自然数是随着度量有限已知量的需要应运而生的，简而言之，是为了计数。比如，为了度量袋子里苹果的量，因为苹果是分立且有限的，所以可以定义自然数1，2，3，4……来度量。这是自然而然的过程，似乎没有什么阻碍。然而，数域的下 一步扩大，即从自然数扩大到零和负数，则是漫长而曲折的过程。</p>\n<p>上面提到，自然数的产生是与现实的需要紧密相关的。而零和负数的产生，则主要依靠推动数学发展的另一典型推动力：突破限制并且自洽的追求。实现这一思维的跨越，花费了人类几个世纪的时间。创造出自然数之后，人们定义了一套自然数运算的规则，加法运算如1+1=2，乘法运算如1x2=2，这当然也适应现实计数的需要。不过，对于它们的逆运算:减法和除法，是有限制条件的。表现在:减法运算a-b，必须要求a&gt;b，否则计算结果无法表示。如果定义新的数，能够表示a-a , a-b(a&lt;b)的值，这个问题就可以解决。所以，人 们用0=a-a ; -(b-a)=a-b来表示它们，零和负数也就产生了。除了简单的加法和乘法基本运算规则外，自然数运算还有一些规律，如</p>\n<ul>\n<li>加法交换律 $a+b=b+a$</li>\n<li>加法结合律 $a+(b+c)=(a+b)+c$ </li>\n<li>乘法交换律 $a·b=b·a$ </li>\n<li>乘法结合律 $a(b·c)=(a·b)·c$</li>\n<li>乘法分配律 $a·(b+c)=ab+ac$</li>\n</ul>\n<p>为了使负数仍然满足上述定律，定义负数的乘法(-1)*(-1)=1，这样就使得扩大后的数域仍然满足原有的规则和规律，实现自洽。</p>\n<p>此后整数域到有理数域的扩大，也是受着外部现实度量需要和内部突破算术限制需要两方面的推动。整数可以描述某一度量单位下的数量，但对于可以无限细分的量，如时间、 面积，则不方便描述。很简单的例子，一个商人可以轻易表达两个苹果的概念，因为没人想把苹果切成60块来计量，但想要描述时间的话，可以用天，小时，分，秒，秒又可以继 续拆分60份成毫秒，毫秒又可以拆成微秒……当面对时间这样一个连续的，可以无限细分的量时，整数就不够用了——如何用整数表达1min=1/60h这样的概念呢？由此我们可以看 出引入“分数”的必要了。同样，从数学内部的突破限制角度来看，当除法运算a/b，a不是b 的整数倍时，计算结果如何表达呢？为了突破除法运算的限制，引入分数的概念，同时定义其运算规则:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{eqnarray}\n\\frac{a}{b}+\\frac{c}{d}=\\frac{ad+bc}{bd}\\\\\n\\frac{a}{b}·\\frac{c}{d}=\\frac{ac}{bd}\\\\\n\\frac{a}{a}=1\\\\\n\\frac{ac}{bc}=\\frac{a}{b}\n\\end{eqnarray}</script><p>这样，数域就扩大到有理数，扩大后的数域仍然适用整数的运算规律，实现自洽。这时，再回过头试图回答最初的那个问题，是什么使数学与一般的演绎区别开来，成为今天深刻改变了世界的数学。从数域扩大的历程中可以窥见，数学的发展是与现实需要的驱动分不开的。数学的定义的创造，公理体系的建立，除了满足基本的内部逻辑自洽之外，从来都是与现实世界相关联照应的。数学史上经历过希腊欧多克斯、欧几里得沉迷于公理演绎的时代，尽管这种公理化一定程度上能够帮助我们更深刻地认识数学规律和事实，但事实证明，脱离了物理现实的这种倾向是危险的——希腊几何在较早的认识到“不可度量”之后，便沉溺到公理演绎中去了，使得本应是必然的数的概念和运算推迟了两千年才出现——这是科学史上一个重大而奇怪的曲折。假如数学真的脱离现实，其定义和功理可以任意创造，只需满足逻辑自洽，那么它将失去一切动力和目标，退化为演绎法的游戏。</p>\n","site":{"data":{}},"excerpt":"<p>身为一个理科生，从小学一年级开始，数学一直是我的必修科目。从自然数四则运算到线性代数、偏微分方程，数学的学习逐步深入，但是如果有人问我，到底什么是数学，恐怕我还真的讲不清楚。<br>","more":"</p>\n<p>小学的时候有同学问过老师，学数学有什么用，出门买菜又用不到勾股定理，一元二次方程都绝少会用到吧。老师说，数学不仅是算术，还是对思维能力的锻炼，是思维的“体操”。在这种说法的基础上，我们稍加延伸，就可以做出一个类比，来说明数学和其他科学的分别与联系:假设世界由许多岛屿组成，人只有两种交通工具，分别是飞机和汽车。想要到达岛屿内的某个目的地，人可以选择走路或搭乘汽车；而想要到达其他岛屿，则只能乘坐飞机。那么，数学就是岛上的汽车，飞机就相当于其他的自然科学或社会科学知识。</p>\n<p>这个类比的潜在逻辑是，数学的本质是演绎法，任何数学知识都可在脑中通过思维得出，就像走路和电车都能到达岛屿任何地方，只是速度快慢的差别；而科学除了演绎法还必须能够归纳检验，必须具有外部经验性。就像电车不能代替飞机跨越海洋一样，电磁感应定律也不能仅凭思维实验得出。既然数学可以仅凭思维创造，那它是不是可以脱离现实，完全成为数学家们思维演绎的游戏呢？这就牵涉到数学的根源问题，是什么把数学同天马行空的推理游戏区别开来，使其成为今天的数学。</p>\n<p>回到最基本的数域来，这个问题或许能得到解答。跟随柯朗，我重新回顾了再熟悉不过、在我眼里早已是理所当然的自然数数域，如何扩大到复数数域的过程。</p>\n<p>自然数是如何产生的？在几千年前，东方的古巴比伦就已经有了数的概念，并发展出了简单的数的计算规则。自然数是随着度量有限已知量的需要应运而生的，简而言之，是为了计数。比如，为了度量袋子里苹果的量，因为苹果是分立且有限的，所以可以定义自然数1，2，3，4……来度量。这是自然而然的过程，似乎没有什么阻碍。然而，数域的下 一步扩大，即从自然数扩大到零和负数，则是漫长而曲折的过程。</p>\n<p>上面提到，自然数的产生是与现实的需要紧密相关的。而零和负数的产生，则主要依靠推动数学发展的另一典型推动力：突破限制并且自洽的追求。实现这一思维的跨越，花费了人类几个世纪的时间。创造出自然数之后，人们定义了一套自然数运算的规则，加法运算如1+1=2，乘法运算如1x2=2，这当然也适应现实计数的需要。不过，对于它们的逆运算:减法和除法，是有限制条件的。表现在:减法运算a-b，必须要求a&gt;b，否则计算结果无法表示。如果定义新的数，能够表示a-a , a-b(a&lt;b)的值，这个问题就可以解决。所以，人 们用0=a-a ; -(b-a)=a-b来表示它们，零和负数也就产生了。除了简单的加法和乘法基本运算规则外，自然数运算还有一些规律，如</p>\n<ul>\n<li>加法交换律 $a+b=b+a$</li>\n<li>加法结合律 $a+(b+c)=(a+b)+c$ </li>\n<li>乘法交换律 $a·b=b·a$ </li>\n<li>乘法结合律 $a(b·c)=(a·b)·c$</li>\n<li>乘法分配律 $a·(b+c)=ab+ac$</li>\n</ul>\n<p>为了使负数仍然满足上述定律，定义负数的乘法(-1)*(-1)=1，这样就使得扩大后的数域仍然满足原有的规则和规律，实现自洽。</p>\n<p>此后整数域到有理数域的扩大，也是受着外部现实度量需要和内部突破算术限制需要两方面的推动。整数可以描述某一度量单位下的数量，但对于可以无限细分的量，如时间、 面积，则不方便描述。很简单的例子，一个商人可以轻易表达两个苹果的概念，因为没人想把苹果切成60块来计量，但想要描述时间的话，可以用天，小时，分，秒，秒又可以继 续拆分60份成毫秒，毫秒又可以拆成微秒……当面对时间这样一个连续的，可以无限细分的量时，整数就不够用了——如何用整数表达1min=1/60h这样的概念呢？由此我们可以看 出引入“分数”的必要了。同样，从数学内部的突破限制角度来看，当除法运算a/b，a不是b 的整数倍时，计算结果如何表达呢？为了突破除法运算的限制，引入分数的概念，同时定义其运算规则:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{eqnarray}\n\\frac{a}{b}+\\frac{c}{d}=\\frac{ad+bc}{bd}\\\\\n\\frac{a}{b}·\\frac{c}{d}=\\frac{ac}{bd}\\\\\n\\frac{a}{a}=1\\\\\n\\frac{ac}{bc}=\\frac{a}{b}\n\\end{eqnarray}</script><p>这样，数域就扩大到有理数，扩大后的数域仍然适用整数的运算规律，实现自洽。这时，再回过头试图回答最初的那个问题，是什么使数学与一般的演绎区别开来，成为今天深刻改变了世界的数学。从数域扩大的历程中可以窥见，数学的发展是与现实需要的驱动分不开的。数学的定义的创造，公理体系的建立，除了满足基本的内部逻辑自洽之外，从来都是与现实世界相关联照应的。数学史上经历过希腊欧多克斯、欧几里得沉迷于公理演绎的时代，尽管这种公理化一定程度上能够帮助我们更深刻地认识数学规律和事实，但事实证明，脱离了物理现实的这种倾向是危险的——希腊几何在较早的认识到“不可度量”之后，便沉溺到公理演绎中去了，使得本应是必然的数的概念和运算推迟了两千年才出现——这是科学史上一个重大而奇怪的曲折。假如数学真的脱离现实，其定义和功理可以任意创造，只需满足逻辑自洽，那么它将失去一切动力和目标，退化为演绎法的游戏。</p>"},{"title":"读韦伯《新教伦理与资本主义精神》","date":"2019-01-20T16:02:00.000Z","_content":"#### 序\n\n我第一次读到马克思·韦伯的《新教伦理与资本主义精神》是在去年，当时正对资本主义社会感兴趣。韦伯从人的思想观念来阐述资本主义具有的一种普遍精神，让我感到思路很是新颖大胆。作为一个不读社会学系的外行人，我从这本书中获益的主要倒不是他花大力气想要论证的，宗教精神对资本主义发展具有何种影响，以及这种影响的确定性和信度，而是他在导言和前两章中对资本主义、资本主义精神的阐释。\n<!--more-->\n\n韦伯认为，资本主义的经济行为应是：“*依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。*”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。造成这种独特性的原因包括技术、经济、社会结构、社会精神等方面的影响。\n\n对于西方来说，由于宗教是本土的，其社会思想的变革也是连续，有迹可循的；而对东方的我们来说，现代资本主义精神是舶来品，我们没有基督教基础，也没有世俗理性主义思潮的影响，谈资本主义就成了无源之水。如何认识现代资本主义在中国的发展，关系到我们如何看待自身的经济生活。现代经济社会像是一个独立的宇宙，所有置身其中的人都要遵守它的规则，接受它的改造，否则就会被淘汰，无法在其中生存。例如一个世代遵循小农经济的中国传统农民，遇到一个现代的、大规模集约生产、具有非常成熟的产业线的农业主，前者如果不快速做出改变，不久就会因为成本高，利润低，或是产品质量差而被资本市场淘汰。\n\n事实上，中国经济现代化的过程就是被动和主动接受资本主义改造的过程。在这个过程中，新的经济生产方式、精神观念，必定会与传统的东西发生冲突。直到现在，不论是乡村还是一线城市，都仍然能看到传统的影子，只不过经济现代化程度高的地方传统的力量更弱一些。例如，中国的“关系”社会，费孝通先生提过的中国传统社会的差序格局、人际圈子，都在某种程度上阻碍着现代化资本主义经济的发展。缺少宗教观念孕育的资本主义精神，正是现代资本主义在中国推广遇到的社会思想障碍。\n\n资本主义市场自身具有巨大的改造力量，不幸的是，如今已经没有人能脱离它而遗世独立，所有人都要在与他人发生经济联系的情况下才能生存，所以，认识资本主义也就显得格外重要，《新教伦理》这本书在今天仍旧值得一读。\n\n\n\n---\n\n#### 正文\n\n马克思·韦伯的《新教伦理与资本主义精神》是他探讨社会精神气质与经济发展之间的关系系列著作的其中一部分，旨在论证：新教的伦理对资本主义在西方的发展起到了重要的作用。\n\n在讨论资本主义精神之前，韦伯首先想要明晰“资本主义”的概念。有人误解了资本主义，认为对财富的贪欲就是资本主义，其实不然。对财富的贪欲根本不等同于资本主义，更不是资本主义的精神。韦伯认为，资本主义的经济行为应是：“依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。是什么催生出了这种理性的现代资本主义？韦伯从四个方面来讨论：即技术、经济、社会结构、社会精神气质。技术是理智性产生的根本推动力；经济因素激励技术发展；法律和行政的理性结构保障了个人固定资本具有确定核算的理性企业能稳定存在；除此之外，“采取某些类型的实际的理性行动却要取决于人的能力和气质”，宗教观念作为一个方面，就会对社会的精神气质产生影响。这本书前两章即是探讨近代经济生活的精神与新教伦理观念之间的关系问题。\n\n韦伯从文化或者说社会心态方面考虑其对资本主义起源的影响，某种程度上弥补了历史唯物主义论的不足。但他并没有陷入绝对的唯心主义。在衡量社会心态所起到的作用时，韦伯做了充分谨慎细致的考量。“考虑到物质基础、社会政治组织形式和宗教改革时期流行的观念之间相互影响的极其混乱的状态，我们只能从研究宗教信仰形式和实际伦理道德观念之间是否存在和哪些方面存在相互关联开始。同时，我们将尽可能详细说明，宗教运动通过这些关系影响物质文化发展的方式和总的方向。只有当我们合理准确地阐明了这一点，才有可能试图对现代文化的发展在何种程度上应归因于这些宗教力量，在何种程度上应归因于其他因素作出估计。”\n\n从一个有趣的现象出发，韦伯提出了经济发展与宗教观念可能具有的关系。他发现社会分层与宗教派别具有某种关联：在任何一个宗教成分混杂，资本主义充分发展的国家，资本占有者、经营管理者、现代工商业高级工人中，新教徒所占的比例要高于其他宗教信徒。造成这种现象的原因或许不能排除历史因素，即十六世纪，古老帝国中一些经济最发达的富庶城镇大多转向新教，以至于新教徒在后来的经济斗争中处于优势地位。但也有不能用历史原因解释的现象：在涉及下一代的教育问题上，新教徒表现出独特的倾向。如在巴登，巴伐利亚，匈牙利，天主教徒父母同新教徒父母为子女选择的高等教育种类大不相同：天主教徒更愿意选择人文教育，新教徒更愿意让子女学技术和工商业。这只能归结于一种共同的精神特质，使新教徒作出了这样的选择。是“家庭宗教氛围首肯的教育类型决定了职业选择”，韦伯认为。孟德斯鸠曾说：英国人“在世上所有民族中取得了三项最长足的进步，即虔诚、贸易和自由。”韦伯想探讨的就是英国人贸易上的优势和对自由政治的顺应是否以某种方式和宗教虔诚发生关联。\n\n什么是资本主义精神？韦伯认为本杰明·富兰克林的伦理观即是资本主义精神的典型代表：认为个人有增加自己资本的责任，而增加资本本身就是目的。“在现代经济制度下挣钱，只要挣得合法，就是长于某种天职的表现”——一个人对天职负有责任——是资产阶级社会伦理中最具代表性的东西，某种意义上可以说是资产阶级文化的基础。 路德翻译的《圣经》里，发展了“职业”的思想。“个人道德活动所能采取的最高形式，应是对其履行世俗事务的义务进行评价。正是这一点使日常的世俗活动具有了宗教意义。并引出了新教核心教理：上帝应许的唯一生存方式，不是要人们以苦修的禁欲主义超越世俗道德，而是要人完成个人在现世里所处地位赋予它的责任和义务。这是他的天职。”\n\n这就把资本主义精神和功利主义区别开来，因为工作挣钱本身即成为了目的。富兰克林这归因于一种神的启示，他的自传中引用圣经：“你看见办事殷勤的人么，他必站在君王面前”；同样，资本主义精神也不同于享乐主义，因为这种伦理是和凭本能冲动享受生活相抵触的。资本主义经济根本特征之一：以理性化的核算、远见和小心谨慎来追求经济成功，这与追求勉强糊口的生活态度是相反的；当然，把这种精神简单等同于对金钱的贪欲的看法更是谬之千里了。有充分的证据表明，资本主义与前资本主义精神之间的区别不在赚钱欲望的发展程度上。因为自从有了人，就有了对黄金的贪欲。不论是中国的清朝官员，还是古罗马贵族，或者是现代的商人，他们的贪欲并没有太大的区别。区别在于赚钱的方式。清朝官员倾向于利用各种政治机会来获利，属于非理性的投机活动，这属于传统主义的范畴。正是资本主义精神中“一个人对天职负有责任”这样一种从功利角度来看完全先验和非理性的思想，能够抑制人贪图享受的本能冲动，冲破传统主义对创新和进步的阻碍，超越世俗的对金钱的贪欲，造就了一种将工作本身当作目的的全新的伦理观。\n\n韦伯从社会精神气质出发，探讨的是社会心态或着说文化史方面的问题。但有历史学者认为，从心态史的观点来看，韦伯的认识有些局限在一个”短时段“时间尺度内，只注重到宗教改革这样一个”短时间“事件的影响，而“社会心态是人类历史发展过程中变化速度最为缓慢的层次，因此人们通常只有经过数十年，甚至百年的’长时段’观察才能发现并理解这方面的变化。”\n\n以资本主义精神中的时间观念的产生为例，向荣认为“韦伯为了突出宗教思想在资本主义产生过程中的决定作用，极力贬低世俗理性主义思潮的影响，而事实上这种影响是不可忽视的”。法国人勒高夫曾对这种世俗理性主义的影响作出过分析。中世纪只有以教堂钟声为标准的“教会时间”，是13、14世纪“城市运动的成功和由商人和企业主构成的市民阶层的成长”，才推动了一种新的时间划分方式产生——适应于工作的“商人时间”，这种新的时间划分随着不久后机械钟表的问世，极大推动了精确、理性、快节奏生活方式的产生。随后，15、16世纪人文主义者思想的传播也对增进时间观念也起到了重要的作用，如意大利人文主义者阿尔伯蒂提醒人们“注视时间，根据时间安排……工作，然后顺序地去做，一个小时也不要浪费”；北欧基督教人文主义者维韦斯建议，强迫沉迷于赌场和酒馆的纨绔子弟“像面对自己的父亲一样，向地方官员交代他们是怎样打发时光的”。\n\n除此之外，向荣认为韦伯还忽视了与宗教改革同时代的其他社会经济变化的作用。1500-1650年欧洲进行了一场名为“习俗改革”的运动。“习俗改革”运动提倡简朴，反对挥霍，打击流浪汉，宣扬的是一种“正派、勤劳、严肃、朴实、守纪、有远见、理智、自制、冷静和节俭”的伦理，类似“人世禁欲主义”。而这其实是一场跨越宗教界限的运动，参与者既有人文主义者，也有新教改革家和天主教改革家，它更像是“一些受过教育的人为了改变人口中其他成员的生活态度和价值观念所做的系统努力”。这种努力是有社会根源可以解释的。15、16世纪农奴制瓦解，资本积累和贫富分化的加剧，使得经济个人主义抬头；而同时文艺复兴使得希腊公益思想重新流行，富人和精英在追求个人利益的同时，不得不扶持贫民。历史学家特雷弗·罗伯把16、17世纪称为伟大的”集体主义“和”慈善捐赠“的时代。这一时期英国的慈善捐款达到了历史上前所未有的高峰。政府方面，从16世纪上半叶开始，颁布了一系列济贫法令，并最后以1601年的《贫穷法》确定下来。所以，富人和处于上升状态的工匠或农民“一方面不得不履行传统道德赋予他们的社会责任，另一方面又因履行这些责任所造成的损失而不平。这种矛盾心理使他们产生出对社会依附阶层，尤其是需要帮助的穷人的潜意识或有意识的仇恨情绪。他们将贫穷归因于穷人本身的懒惰、不知节俭和缺乏远见。他们主张改造穷人，去掉他们身上的不良生活习惯。”因此，习俗改革不仅是文化精英对大众文化的改造，也是社会精英对普通民众的改造。这种改革不可能不对社会精神和经济发展产生影响。\n\n因此，韦伯的《新教伦理和资本主义精神》从精神因素考虑，分析了思想观念的变革对资本主义发展的重要影响，但他的论证似乎有些过于强调教派神学中国先验成分的影响，而忽视了世俗理性主义和社会经济因素的影响。\n\n对于我们来说，围绕资本主义的经济组织方式、精神文化都可以说是西方的舶来品，是西方的坚船利炮冲击封建主义的同时带来了资本主义的发展模式以及文化。基督教不是东方的宗教文明。对西方而言，宗教精神对资本主义经济的影响是完整连续的，而对东方来说，则是断层的。缺少文化根基的土地上，本土的现代资本主义是如何产生和发展的，这是一个很有意思的话题。因此，韦伯的《新教伦理与资本主义精神》不仅对于弄清楚西方资本主义的起源问题很重要，对于我国进行中的市场经济实践，以及个人理解身边的经济生活，也都有借鉴意义。\n\n\n\n\n\n<small>*参考*</small>\n\n<small>*向荣：《文化变革与西方资本主义的兴起——读韦伯〈新教伦理与资本主义精神〉》*</small>\n\n<small>*张椿年:《从信仰到理性——意大利人文主义研究》，浙江人民出版社1993年版，第112页*</small>\n\n<small>*马戈·托德：《基督教人文主义与清教社会秩序》剑桥大学出版社1987年版，第125页*</small>\n\n<small>*W.K.乔丹：《英国的慈善事业.1480—1660年》伦敦1960年版*</small>\n\n<small>*伯克：《早期近代欧洲的大众文化》第213页*</small>\n\n","source":"_posts/reading MaxWebber.md","raw":"---\ntitle: 读韦伯《新教伦理与资本主义精神》\ndate: 2019-01-21 00:02:00\ncategories:\n- 社会科学\ntags: \n- 宗教\n- 资本主义\n---\n#### 序\n\n我第一次读到马克思·韦伯的《新教伦理与资本主义精神》是在去年，当时正对资本主义社会感兴趣。韦伯从人的思想观念来阐述资本主义具有的一种普遍精神，让我感到思路很是新颖大胆。作为一个不读社会学系的外行人，我从这本书中获益的主要倒不是他花大力气想要论证的，宗教精神对资本主义发展具有何种影响，以及这种影响的确定性和信度，而是他在导言和前两章中对资本主义、资本主义精神的阐释。\n<!--more-->\n\n韦伯认为，资本主义的经济行为应是：“*依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。*”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。造成这种独特性的原因包括技术、经济、社会结构、社会精神等方面的影响。\n\n对于西方来说，由于宗教是本土的，其社会思想的变革也是连续，有迹可循的；而对东方的我们来说，现代资本主义精神是舶来品，我们没有基督教基础，也没有世俗理性主义思潮的影响，谈资本主义就成了无源之水。如何认识现代资本主义在中国的发展，关系到我们如何看待自身的经济生活。现代经济社会像是一个独立的宇宙，所有置身其中的人都要遵守它的规则，接受它的改造，否则就会被淘汰，无法在其中生存。例如一个世代遵循小农经济的中国传统农民，遇到一个现代的、大规模集约生产、具有非常成熟的产业线的农业主，前者如果不快速做出改变，不久就会因为成本高，利润低，或是产品质量差而被资本市场淘汰。\n\n事实上，中国经济现代化的过程就是被动和主动接受资本主义改造的过程。在这个过程中，新的经济生产方式、精神观念，必定会与传统的东西发生冲突。直到现在，不论是乡村还是一线城市，都仍然能看到传统的影子，只不过经济现代化程度高的地方传统的力量更弱一些。例如，中国的“关系”社会，费孝通先生提过的中国传统社会的差序格局、人际圈子，都在某种程度上阻碍着现代化资本主义经济的发展。缺少宗教观念孕育的资本主义精神，正是现代资本主义在中国推广遇到的社会思想障碍。\n\n资本主义市场自身具有巨大的改造力量，不幸的是，如今已经没有人能脱离它而遗世独立，所有人都要在与他人发生经济联系的情况下才能生存，所以，认识资本主义也就显得格外重要，《新教伦理》这本书在今天仍旧值得一读。\n\n\n\n---\n\n#### 正文\n\n马克思·韦伯的《新教伦理与资本主义精神》是他探讨社会精神气质与经济发展之间的关系系列著作的其中一部分，旨在论证：新教的伦理对资本主义在西方的发展起到了重要的作用。\n\n在讨论资本主义精神之前，韦伯首先想要明晰“资本主义”的概念。有人误解了资本主义，认为对财富的贪欲就是资本主义，其实不然。对财富的贪欲根本不等同于资本主义，更不是资本主义的精神。韦伯认为，资本主义的经济行为应是：“依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。是什么催生出了这种理性的现代资本主义？韦伯从四个方面来讨论：即技术、经济、社会结构、社会精神气质。技术是理智性产生的根本推动力；经济因素激励技术发展；法律和行政的理性结构保障了个人固定资本具有确定核算的理性企业能稳定存在；除此之外，“采取某些类型的实际的理性行动却要取决于人的能力和气质”，宗教观念作为一个方面，就会对社会的精神气质产生影响。这本书前两章即是探讨近代经济生活的精神与新教伦理观念之间的关系问题。\n\n韦伯从文化或者说社会心态方面考虑其对资本主义起源的影响，某种程度上弥补了历史唯物主义论的不足。但他并没有陷入绝对的唯心主义。在衡量社会心态所起到的作用时，韦伯做了充分谨慎细致的考量。“考虑到物质基础、社会政治组织形式和宗教改革时期流行的观念之间相互影响的极其混乱的状态，我们只能从研究宗教信仰形式和实际伦理道德观念之间是否存在和哪些方面存在相互关联开始。同时，我们将尽可能详细说明，宗教运动通过这些关系影响物质文化发展的方式和总的方向。只有当我们合理准确地阐明了这一点，才有可能试图对现代文化的发展在何种程度上应归因于这些宗教力量，在何种程度上应归因于其他因素作出估计。”\n\n从一个有趣的现象出发，韦伯提出了经济发展与宗教观念可能具有的关系。他发现社会分层与宗教派别具有某种关联：在任何一个宗教成分混杂，资本主义充分发展的国家，资本占有者、经营管理者、现代工商业高级工人中，新教徒所占的比例要高于其他宗教信徒。造成这种现象的原因或许不能排除历史因素，即十六世纪，古老帝国中一些经济最发达的富庶城镇大多转向新教，以至于新教徒在后来的经济斗争中处于优势地位。但也有不能用历史原因解释的现象：在涉及下一代的教育问题上，新教徒表现出独特的倾向。如在巴登，巴伐利亚，匈牙利，天主教徒父母同新教徒父母为子女选择的高等教育种类大不相同：天主教徒更愿意选择人文教育，新教徒更愿意让子女学技术和工商业。这只能归结于一种共同的精神特质，使新教徒作出了这样的选择。是“家庭宗教氛围首肯的教育类型决定了职业选择”，韦伯认为。孟德斯鸠曾说：英国人“在世上所有民族中取得了三项最长足的进步，即虔诚、贸易和自由。”韦伯想探讨的就是英国人贸易上的优势和对自由政治的顺应是否以某种方式和宗教虔诚发生关联。\n\n什么是资本主义精神？韦伯认为本杰明·富兰克林的伦理观即是资本主义精神的典型代表：认为个人有增加自己资本的责任，而增加资本本身就是目的。“在现代经济制度下挣钱，只要挣得合法，就是长于某种天职的表现”——一个人对天职负有责任——是资产阶级社会伦理中最具代表性的东西，某种意义上可以说是资产阶级文化的基础。 路德翻译的《圣经》里，发展了“职业”的思想。“个人道德活动所能采取的最高形式，应是对其履行世俗事务的义务进行评价。正是这一点使日常的世俗活动具有了宗教意义。并引出了新教核心教理：上帝应许的唯一生存方式，不是要人们以苦修的禁欲主义超越世俗道德，而是要人完成个人在现世里所处地位赋予它的责任和义务。这是他的天职。”\n\n这就把资本主义精神和功利主义区别开来，因为工作挣钱本身即成为了目的。富兰克林这归因于一种神的启示，他的自传中引用圣经：“你看见办事殷勤的人么，他必站在君王面前”；同样，资本主义精神也不同于享乐主义，因为这种伦理是和凭本能冲动享受生活相抵触的。资本主义经济根本特征之一：以理性化的核算、远见和小心谨慎来追求经济成功，这与追求勉强糊口的生活态度是相反的；当然，把这种精神简单等同于对金钱的贪欲的看法更是谬之千里了。有充分的证据表明，资本主义与前资本主义精神之间的区别不在赚钱欲望的发展程度上。因为自从有了人，就有了对黄金的贪欲。不论是中国的清朝官员，还是古罗马贵族，或者是现代的商人，他们的贪欲并没有太大的区别。区别在于赚钱的方式。清朝官员倾向于利用各种政治机会来获利，属于非理性的投机活动，这属于传统主义的范畴。正是资本主义精神中“一个人对天职负有责任”这样一种从功利角度来看完全先验和非理性的思想，能够抑制人贪图享受的本能冲动，冲破传统主义对创新和进步的阻碍，超越世俗的对金钱的贪欲，造就了一种将工作本身当作目的的全新的伦理观。\n\n韦伯从社会精神气质出发，探讨的是社会心态或着说文化史方面的问题。但有历史学者认为，从心态史的观点来看，韦伯的认识有些局限在一个”短时段“时间尺度内，只注重到宗教改革这样一个”短时间“事件的影响，而“社会心态是人类历史发展过程中变化速度最为缓慢的层次，因此人们通常只有经过数十年，甚至百年的’长时段’观察才能发现并理解这方面的变化。”\n\n以资本主义精神中的时间观念的产生为例，向荣认为“韦伯为了突出宗教思想在资本主义产生过程中的决定作用，极力贬低世俗理性主义思潮的影响，而事实上这种影响是不可忽视的”。法国人勒高夫曾对这种世俗理性主义的影响作出过分析。中世纪只有以教堂钟声为标准的“教会时间”，是13、14世纪“城市运动的成功和由商人和企业主构成的市民阶层的成长”，才推动了一种新的时间划分方式产生——适应于工作的“商人时间”，这种新的时间划分随着不久后机械钟表的问世，极大推动了精确、理性、快节奏生活方式的产生。随后，15、16世纪人文主义者思想的传播也对增进时间观念也起到了重要的作用，如意大利人文主义者阿尔伯蒂提醒人们“注视时间，根据时间安排……工作，然后顺序地去做，一个小时也不要浪费”；北欧基督教人文主义者维韦斯建议，强迫沉迷于赌场和酒馆的纨绔子弟“像面对自己的父亲一样，向地方官员交代他们是怎样打发时光的”。\n\n除此之外，向荣认为韦伯还忽视了与宗教改革同时代的其他社会经济变化的作用。1500-1650年欧洲进行了一场名为“习俗改革”的运动。“习俗改革”运动提倡简朴，反对挥霍，打击流浪汉，宣扬的是一种“正派、勤劳、严肃、朴实、守纪、有远见、理智、自制、冷静和节俭”的伦理，类似“人世禁欲主义”。而这其实是一场跨越宗教界限的运动，参与者既有人文主义者，也有新教改革家和天主教改革家，它更像是“一些受过教育的人为了改变人口中其他成员的生活态度和价值观念所做的系统努力”。这种努力是有社会根源可以解释的。15、16世纪农奴制瓦解，资本积累和贫富分化的加剧，使得经济个人主义抬头；而同时文艺复兴使得希腊公益思想重新流行，富人和精英在追求个人利益的同时，不得不扶持贫民。历史学家特雷弗·罗伯把16、17世纪称为伟大的”集体主义“和”慈善捐赠“的时代。这一时期英国的慈善捐款达到了历史上前所未有的高峰。政府方面，从16世纪上半叶开始，颁布了一系列济贫法令，并最后以1601年的《贫穷法》确定下来。所以，富人和处于上升状态的工匠或农民“一方面不得不履行传统道德赋予他们的社会责任，另一方面又因履行这些责任所造成的损失而不平。这种矛盾心理使他们产生出对社会依附阶层，尤其是需要帮助的穷人的潜意识或有意识的仇恨情绪。他们将贫穷归因于穷人本身的懒惰、不知节俭和缺乏远见。他们主张改造穷人，去掉他们身上的不良生活习惯。”因此，习俗改革不仅是文化精英对大众文化的改造，也是社会精英对普通民众的改造。这种改革不可能不对社会精神和经济发展产生影响。\n\n因此，韦伯的《新教伦理和资本主义精神》从精神因素考虑，分析了思想观念的变革对资本主义发展的重要影响，但他的论证似乎有些过于强调教派神学中国先验成分的影响，而忽视了世俗理性主义和社会经济因素的影响。\n\n对于我们来说，围绕资本主义的经济组织方式、精神文化都可以说是西方的舶来品，是西方的坚船利炮冲击封建主义的同时带来了资本主义的发展模式以及文化。基督教不是东方的宗教文明。对西方而言，宗教精神对资本主义经济的影响是完整连续的，而对东方来说，则是断层的。缺少文化根基的土地上，本土的现代资本主义是如何产生和发展的，这是一个很有意思的话题。因此，韦伯的《新教伦理与资本主义精神》不仅对于弄清楚西方资本主义的起源问题很重要，对于我国进行中的市场经济实践，以及个人理解身边的经济生活，也都有借鉴意义。\n\n\n\n\n\n<small>*参考*</small>\n\n<small>*向荣：《文化变革与西方资本主义的兴起——读韦伯〈新教伦理与资本主义精神〉》*</small>\n\n<small>*张椿年:《从信仰到理性——意大利人文主义研究》，浙江人民出版社1993年版，第112页*</small>\n\n<small>*马戈·托德：《基督教人文主义与清教社会秩序》剑桥大学出版社1987年版，第125页*</small>\n\n<small>*W.K.乔丹：《英国的慈善事业.1480—1660年》伦敦1960年版*</small>\n\n<small>*伯克：《早期近代欧洲的大众文化》第213页*</small>\n\n","slug":"reading MaxWebber","published":1,"updated":"2020-02-12T07:45:11.748Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6u002a2sfy7aty26ur","content":"<h4 id=\"序\"><a href=\"#序\" class=\"headerlink\" title=\"序\"></a>序</h4><p>我第一次读到马克思·韦伯的《新教伦理与资本主义精神》是在去年，当时正对资本主义社会感兴趣。韦伯从人的思想观念来阐述资本主义具有的一种普遍精神，让我感到思路很是新颖大胆。作为一个不读社会学系的外行人，我从这本书中获益的主要倒不是他花大力气想要论证的，宗教精神对资本主义发展具有何种影响，以及这种影响的确定性和信度，而是他在导言和前两章中对资本主义、资本主义精神的阐释。<br><a id=\"more\"></a></p>\n<p>韦伯认为，资本主义的经济行为应是：“<em>依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。</em>”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。造成这种独特性的原因包括技术、经济、社会结构、社会精神等方面的影响。</p>\n<p>对于西方来说，由于宗教是本土的，其社会思想的变革也是连续，有迹可循的；而对东方的我们来说，现代资本主义精神是舶来品，我们没有基督教基础，也没有世俗理性主义思潮的影响，谈资本主义就成了无源之水。如何认识现代资本主义在中国的发展，关系到我们如何看待自身的经济生活。现代经济社会像是一个独立的宇宙，所有置身其中的人都要遵守它的规则，接受它的改造，否则就会被淘汰，无法在其中生存。例如一个世代遵循小农经济的中国传统农民，遇到一个现代的、大规模集约生产、具有非常成熟的产业线的农业主，前者如果不快速做出改变，不久就会因为成本高，利润低，或是产品质量差而被资本市场淘汰。</p>\n<p>事实上，中国经济现代化的过程就是被动和主动接受资本主义改造的过程。在这个过程中，新的经济生产方式、精神观念，必定会与传统的东西发生冲突。直到现在，不论是乡村还是一线城市，都仍然能看到传统的影子，只不过经济现代化程度高的地方传统的力量更弱一些。例如，中国的“关系”社会，费孝通先生提过的中国传统社会的差序格局、人际圈子，都在某种程度上阻碍着现代化资本主义经济的发展。缺少宗教观念孕育的资本主义精神，正是现代资本主义在中国推广遇到的社会思想障碍。</p>\n<p>资本主义市场自身具有巨大的改造力量，不幸的是，如今已经没有人能脱离它而遗世独立，所有人都要在与他人发生经济联系的情况下才能生存，所以，认识资本主义也就显得格外重要，《新教伦理》这本书在今天仍旧值得一读。</p>\n<hr>\n<h4 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h4><p>马克思·韦伯的《新教伦理与资本主义精神》是他探讨社会精神气质与经济发展之间的关系系列著作的其中一部分，旨在论证：新教的伦理对资本主义在西方的发展起到了重要的作用。</p>\n<p>在讨论资本主义精神之前，韦伯首先想要明晰“资本主义”的概念。有人误解了资本主义，认为对财富的贪欲就是资本主义，其实不然。对财富的贪欲根本不等同于资本主义，更不是资本主义的精神。韦伯认为，资本主义的经济行为应是：“依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。是什么催生出了这种理性的现代资本主义？韦伯从四个方面来讨论：即技术、经济、社会结构、社会精神气质。技术是理智性产生的根本推动力；经济因素激励技术发展；法律和行政的理性结构保障了个人固定资本具有确定核算的理性企业能稳定存在；除此之外，“采取某些类型的实际的理性行动却要取决于人的能力和气质”，宗教观念作为一个方面，就会对社会的精神气质产生影响。这本书前两章即是探讨近代经济生活的精神与新教伦理观念之间的关系问题。</p>\n<p>韦伯从文化或者说社会心态方面考虑其对资本主义起源的影响，某种程度上弥补了历史唯物主义论的不足。但他并没有陷入绝对的唯心主义。在衡量社会心态所起到的作用时，韦伯做了充分谨慎细致的考量。“考虑到物质基础、社会政治组织形式和宗教改革时期流行的观念之间相互影响的极其混乱的状态，我们只能从研究宗教信仰形式和实际伦理道德观念之间是否存在和哪些方面存在相互关联开始。同时，我们将尽可能详细说明，宗教运动通过这些关系影响物质文化发展的方式和总的方向。只有当我们合理准确地阐明了这一点，才有可能试图对现代文化的发展在何种程度上应归因于这些宗教力量，在何种程度上应归因于其他因素作出估计。”</p>\n<p>从一个有趣的现象出发，韦伯提出了经济发展与宗教观念可能具有的关系。他发现社会分层与宗教派别具有某种关联：在任何一个宗教成分混杂，资本主义充分发展的国家，资本占有者、经营管理者、现代工商业高级工人中，新教徒所占的比例要高于其他宗教信徒。造成这种现象的原因或许不能排除历史因素，即十六世纪，古老帝国中一些经济最发达的富庶城镇大多转向新教，以至于新教徒在后来的经济斗争中处于优势地位。但也有不能用历史原因解释的现象：在涉及下一代的教育问题上，新教徒表现出独特的倾向。如在巴登，巴伐利亚，匈牙利，天主教徒父母同新教徒父母为子女选择的高等教育种类大不相同：天主教徒更愿意选择人文教育，新教徒更愿意让子女学技术和工商业。这只能归结于一种共同的精神特质，使新教徒作出了这样的选择。是“家庭宗教氛围首肯的教育类型决定了职业选择”，韦伯认为。孟德斯鸠曾说：英国人“在世上所有民族中取得了三项最长足的进步，即虔诚、贸易和自由。”韦伯想探讨的就是英国人贸易上的优势和对自由政治的顺应是否以某种方式和宗教虔诚发生关联。</p>\n<p>什么是资本主义精神？韦伯认为本杰明·富兰克林的伦理观即是资本主义精神的典型代表：认为个人有增加自己资本的责任，而增加资本本身就是目的。“在现代经济制度下挣钱，只要挣得合法，就是长于某种天职的表现”——一个人对天职负有责任——是资产阶级社会伦理中最具代表性的东西，某种意义上可以说是资产阶级文化的基础。 路德翻译的《圣经》里，发展了“职业”的思想。“个人道德活动所能采取的最高形式，应是对其履行世俗事务的义务进行评价。正是这一点使日常的世俗活动具有了宗教意义。并引出了新教核心教理：上帝应许的唯一生存方式，不是要人们以苦修的禁欲主义超越世俗道德，而是要人完成个人在现世里所处地位赋予它的责任和义务。这是他的天职。”</p>\n<p>这就把资本主义精神和功利主义区别开来，因为工作挣钱本身即成为了目的。富兰克林这归因于一种神的启示，他的自传中引用圣经：“你看见办事殷勤的人么，他必站在君王面前”；同样，资本主义精神也不同于享乐主义，因为这种伦理是和凭本能冲动享受生活相抵触的。资本主义经济根本特征之一：以理性化的核算、远见和小心谨慎来追求经济成功，这与追求勉强糊口的生活态度是相反的；当然，把这种精神简单等同于对金钱的贪欲的看法更是谬之千里了。有充分的证据表明，资本主义与前资本主义精神之间的区别不在赚钱欲望的发展程度上。因为自从有了人，就有了对黄金的贪欲。不论是中国的清朝官员，还是古罗马贵族，或者是现代的商人，他们的贪欲并没有太大的区别。区别在于赚钱的方式。清朝官员倾向于利用各种政治机会来获利，属于非理性的投机活动，这属于传统主义的范畴。正是资本主义精神中“一个人对天职负有责任”这样一种从功利角度来看完全先验和非理性的思想，能够抑制人贪图享受的本能冲动，冲破传统主义对创新和进步的阻碍，超越世俗的对金钱的贪欲，造就了一种将工作本身当作目的的全新的伦理观。</p>\n<p>韦伯从社会精神气质出发，探讨的是社会心态或着说文化史方面的问题。但有历史学者认为，从心态史的观点来看，韦伯的认识有些局限在一个”短时段“时间尺度内，只注重到宗教改革这样一个”短时间“事件的影响，而“社会心态是人类历史发展过程中变化速度最为缓慢的层次，因此人们通常只有经过数十年，甚至百年的’长时段’观察才能发现并理解这方面的变化。”</p>\n<p>以资本主义精神中的时间观念的产生为例，向荣认为“韦伯为了突出宗教思想在资本主义产生过程中的决定作用，极力贬低世俗理性主义思潮的影响，而事实上这种影响是不可忽视的”。法国人勒高夫曾对这种世俗理性主义的影响作出过分析。中世纪只有以教堂钟声为标准的“教会时间”，是13、14世纪“城市运动的成功和由商人和企业主构成的市民阶层的成长”，才推动了一种新的时间划分方式产生——适应于工作的“商人时间”，这种新的时间划分随着不久后机械钟表的问世，极大推动了精确、理性、快节奏生活方式的产生。随后，15、16世纪人文主义者思想的传播也对增进时间观念也起到了重要的作用，如意大利人文主义者阿尔伯蒂提醒人们“注视时间，根据时间安排……工作，然后顺序地去做，一个小时也不要浪费”；北欧基督教人文主义者维韦斯建议，强迫沉迷于赌场和酒馆的纨绔子弟“像面对自己的父亲一样，向地方官员交代他们是怎样打发时光的”。</p>\n<p>除此之外，向荣认为韦伯还忽视了与宗教改革同时代的其他社会经济变化的作用。1500-1650年欧洲进行了一场名为“习俗改革”的运动。“习俗改革”运动提倡简朴，反对挥霍，打击流浪汉，宣扬的是一种“正派、勤劳、严肃、朴实、守纪、有远见、理智、自制、冷静和节俭”的伦理，类似“人世禁欲主义”。而这其实是一场跨越宗教界限的运动，参与者既有人文主义者，也有新教改革家和天主教改革家，它更像是“一些受过教育的人为了改变人口中其他成员的生活态度和价值观念所做的系统努力”。这种努力是有社会根源可以解释的。15、16世纪农奴制瓦解，资本积累和贫富分化的加剧，使得经济个人主义抬头；而同时文艺复兴使得希腊公益思想重新流行，富人和精英在追求个人利益的同时，不得不扶持贫民。历史学家特雷弗·罗伯把16、17世纪称为伟大的”集体主义“和”慈善捐赠“的时代。这一时期英国的慈善捐款达到了历史上前所未有的高峰。政府方面，从16世纪上半叶开始，颁布了一系列济贫法令，并最后以1601年的《贫穷法》确定下来。所以，富人和处于上升状态的工匠或农民“一方面不得不履行传统道德赋予他们的社会责任，另一方面又因履行这些责任所造成的损失而不平。这种矛盾心理使他们产生出对社会依附阶层，尤其是需要帮助的穷人的潜意识或有意识的仇恨情绪。他们将贫穷归因于穷人本身的懒惰、不知节俭和缺乏远见。他们主张改造穷人，去掉他们身上的不良生活习惯。”因此，习俗改革不仅是文化精英对大众文化的改造，也是社会精英对普通民众的改造。这种改革不可能不对社会精神和经济发展产生影响。</p>\n<p>因此，韦伯的《新教伦理和资本主义精神》从精神因素考虑，分析了思想观念的变革对资本主义发展的重要影响，但他的论证似乎有些过于强调教派神学中国先验成分的影响，而忽视了世俗理性主义和社会经济因素的影响。</p>\n<p>对于我们来说，围绕资本主义的经济组织方式、精神文化都可以说是西方的舶来品，是西方的坚船利炮冲击封建主义的同时带来了资本主义的发展模式以及文化。基督教不是东方的宗教文明。对西方而言，宗教精神对资本主义经济的影响是完整连续的，而对东方来说，则是断层的。缺少文化根基的土地上，本土的现代资本主义是如何产生和发展的，这是一个很有意思的话题。因此，韦伯的《新教伦理与资本主义精神》不仅对于弄清楚西方资本主义的起源问题很重要，对于我国进行中的市场经济实践，以及个人理解身边的经济生活，也都有借鉴意义。</p>\n<p><small><em>参考</em></small></p>\n<p><small><em>向荣：《文化变革与西方资本主义的兴起——读韦伯〈新教伦理与资本主义精神〉》</em></small></p>\n<p><small><em>张椿年:《从信仰到理性——意大利人文主义研究》，浙江人民出版社1993年版，第112页</em></small></p>\n<p><small><em>马戈·托德：《基督教人文主义与清教社会秩序》剑桥大学出版社1987年版，第125页</em></small></p>\n<p><small><em>W.K.乔丹：《英国的慈善事业.1480—1660年》伦敦1960年版</em></small></p>\n<p><small><em>伯克：《早期近代欧洲的大众文化》第213页</em></small></p>\n","site":{"data":{}},"excerpt":"<h4 id=\"序\"><a href=\"#序\" class=\"headerlink\" title=\"序\"></a>序</h4><p>我第一次读到马克思·韦伯的《新教伦理与资本主义精神》是在去年，当时正对资本主义社会感兴趣。韦伯从人的思想观念来阐述资本主义具有的一种普遍精神，让我感到思路很是新颖大胆。作为一个不读社会学系的外行人，我从这本书中获益的主要倒不是他花大力气想要论证的，宗教精神对资本主义发展具有何种影响，以及这种影响的确定性和信度，而是他在导言和前两章中对资本主义、资本主义精神的阐释。<br>","more":"</p>\n<p>韦伯认为，资本主义的经济行为应是：“<em>依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。</em>”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。造成这种独特性的原因包括技术、经济、社会结构、社会精神等方面的影响。</p>\n<p>对于西方来说，由于宗教是本土的，其社会思想的变革也是连续，有迹可循的；而对东方的我们来说，现代资本主义精神是舶来品，我们没有基督教基础，也没有世俗理性主义思潮的影响，谈资本主义就成了无源之水。如何认识现代资本主义在中国的发展，关系到我们如何看待自身的经济生活。现代经济社会像是一个独立的宇宙，所有置身其中的人都要遵守它的规则，接受它的改造，否则就会被淘汰，无法在其中生存。例如一个世代遵循小农经济的中国传统农民，遇到一个现代的、大规模集约生产、具有非常成熟的产业线的农业主，前者如果不快速做出改变，不久就会因为成本高，利润低，或是产品质量差而被资本市场淘汰。</p>\n<p>事实上，中国经济现代化的过程就是被动和主动接受资本主义改造的过程。在这个过程中，新的经济生产方式、精神观念，必定会与传统的东西发生冲突。直到现在，不论是乡村还是一线城市，都仍然能看到传统的影子，只不过经济现代化程度高的地方传统的力量更弱一些。例如，中国的“关系”社会，费孝通先生提过的中国传统社会的差序格局、人际圈子，都在某种程度上阻碍着现代化资本主义经济的发展。缺少宗教观念孕育的资本主义精神，正是现代资本主义在中国推广遇到的社会思想障碍。</p>\n<p>资本主义市场自身具有巨大的改造力量，不幸的是，如今已经没有人能脱离它而遗世独立，所有人都要在与他人发生经济联系的情况下才能生存，所以，认识资本主义也就显得格外重要，《新教伦理》这本书在今天仍旧值得一读。</p>\n<hr>\n<h4 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h4><p>马克思·韦伯的《新教伦理与资本主义精神》是他探讨社会精神气质与经济发展之间的关系系列著作的其中一部分，旨在论证：新教的伦理对资本主义在西方的发展起到了重要的作用。</p>\n<p>在讨论资本主义精神之前，韦伯首先想要明晰“资本主义”的概念。有人误解了资本主义，认为对财富的贪欲就是资本主义，其实不然。对财富的贪欲根本不等同于资本主义，更不是资本主义的精神。韦伯认为，资本主义的经济行为应是：“依赖于利用交换机会来谋取利润的行为，亦即是依赖于（在形式上）和平的获取机会的行为。”原始的资本主义其实在世界各地都出现过，但“以自由劳动的理性组织方式”为特征的资本主义，却只在西方产生了。是什么催生出了这种理性的现代资本主义？韦伯从四个方面来讨论：即技术、经济、社会结构、社会精神气质。技术是理智性产生的根本推动力；经济因素激励技术发展；法律和行政的理性结构保障了个人固定资本具有确定核算的理性企业能稳定存在；除此之外，“采取某些类型的实际的理性行动却要取决于人的能力和气质”，宗教观念作为一个方面，就会对社会的精神气质产生影响。这本书前两章即是探讨近代经济生活的精神与新教伦理观念之间的关系问题。</p>\n<p>韦伯从文化或者说社会心态方面考虑其对资本主义起源的影响，某种程度上弥补了历史唯物主义论的不足。但他并没有陷入绝对的唯心主义。在衡量社会心态所起到的作用时，韦伯做了充分谨慎细致的考量。“考虑到物质基础、社会政治组织形式和宗教改革时期流行的观念之间相互影响的极其混乱的状态，我们只能从研究宗教信仰形式和实际伦理道德观念之间是否存在和哪些方面存在相互关联开始。同时，我们将尽可能详细说明，宗教运动通过这些关系影响物质文化发展的方式和总的方向。只有当我们合理准确地阐明了这一点，才有可能试图对现代文化的发展在何种程度上应归因于这些宗教力量，在何种程度上应归因于其他因素作出估计。”</p>\n<p>从一个有趣的现象出发，韦伯提出了经济发展与宗教观念可能具有的关系。他发现社会分层与宗教派别具有某种关联：在任何一个宗教成分混杂，资本主义充分发展的国家，资本占有者、经营管理者、现代工商业高级工人中，新教徒所占的比例要高于其他宗教信徒。造成这种现象的原因或许不能排除历史因素，即十六世纪，古老帝国中一些经济最发达的富庶城镇大多转向新教，以至于新教徒在后来的经济斗争中处于优势地位。但也有不能用历史原因解释的现象：在涉及下一代的教育问题上，新教徒表现出独特的倾向。如在巴登，巴伐利亚，匈牙利，天主教徒父母同新教徒父母为子女选择的高等教育种类大不相同：天主教徒更愿意选择人文教育，新教徒更愿意让子女学技术和工商业。这只能归结于一种共同的精神特质，使新教徒作出了这样的选择。是“家庭宗教氛围首肯的教育类型决定了职业选择”，韦伯认为。孟德斯鸠曾说：英国人“在世上所有民族中取得了三项最长足的进步，即虔诚、贸易和自由。”韦伯想探讨的就是英国人贸易上的优势和对自由政治的顺应是否以某种方式和宗教虔诚发生关联。</p>\n<p>什么是资本主义精神？韦伯认为本杰明·富兰克林的伦理观即是资本主义精神的典型代表：认为个人有增加自己资本的责任，而增加资本本身就是目的。“在现代经济制度下挣钱，只要挣得合法，就是长于某种天职的表现”——一个人对天职负有责任——是资产阶级社会伦理中最具代表性的东西，某种意义上可以说是资产阶级文化的基础。 路德翻译的《圣经》里，发展了“职业”的思想。“个人道德活动所能采取的最高形式，应是对其履行世俗事务的义务进行评价。正是这一点使日常的世俗活动具有了宗教意义。并引出了新教核心教理：上帝应许的唯一生存方式，不是要人们以苦修的禁欲主义超越世俗道德，而是要人完成个人在现世里所处地位赋予它的责任和义务。这是他的天职。”</p>\n<p>这就把资本主义精神和功利主义区别开来，因为工作挣钱本身即成为了目的。富兰克林这归因于一种神的启示，他的自传中引用圣经：“你看见办事殷勤的人么，他必站在君王面前”；同样，资本主义精神也不同于享乐主义，因为这种伦理是和凭本能冲动享受生活相抵触的。资本主义经济根本特征之一：以理性化的核算、远见和小心谨慎来追求经济成功，这与追求勉强糊口的生活态度是相反的；当然，把这种精神简单等同于对金钱的贪欲的看法更是谬之千里了。有充分的证据表明，资本主义与前资本主义精神之间的区别不在赚钱欲望的发展程度上。因为自从有了人，就有了对黄金的贪欲。不论是中国的清朝官员，还是古罗马贵族，或者是现代的商人，他们的贪欲并没有太大的区别。区别在于赚钱的方式。清朝官员倾向于利用各种政治机会来获利，属于非理性的投机活动，这属于传统主义的范畴。正是资本主义精神中“一个人对天职负有责任”这样一种从功利角度来看完全先验和非理性的思想，能够抑制人贪图享受的本能冲动，冲破传统主义对创新和进步的阻碍，超越世俗的对金钱的贪欲，造就了一种将工作本身当作目的的全新的伦理观。</p>\n<p>韦伯从社会精神气质出发，探讨的是社会心态或着说文化史方面的问题。但有历史学者认为，从心态史的观点来看，韦伯的认识有些局限在一个”短时段“时间尺度内，只注重到宗教改革这样一个”短时间“事件的影响，而“社会心态是人类历史发展过程中变化速度最为缓慢的层次，因此人们通常只有经过数十年，甚至百年的’长时段’观察才能发现并理解这方面的变化。”</p>\n<p>以资本主义精神中的时间观念的产生为例，向荣认为“韦伯为了突出宗教思想在资本主义产生过程中的决定作用，极力贬低世俗理性主义思潮的影响，而事实上这种影响是不可忽视的”。法国人勒高夫曾对这种世俗理性主义的影响作出过分析。中世纪只有以教堂钟声为标准的“教会时间”，是13、14世纪“城市运动的成功和由商人和企业主构成的市民阶层的成长”，才推动了一种新的时间划分方式产生——适应于工作的“商人时间”，这种新的时间划分随着不久后机械钟表的问世，极大推动了精确、理性、快节奏生活方式的产生。随后，15、16世纪人文主义者思想的传播也对增进时间观念也起到了重要的作用，如意大利人文主义者阿尔伯蒂提醒人们“注视时间，根据时间安排……工作，然后顺序地去做，一个小时也不要浪费”；北欧基督教人文主义者维韦斯建议，强迫沉迷于赌场和酒馆的纨绔子弟“像面对自己的父亲一样，向地方官员交代他们是怎样打发时光的”。</p>\n<p>除此之外，向荣认为韦伯还忽视了与宗教改革同时代的其他社会经济变化的作用。1500-1650年欧洲进行了一场名为“习俗改革”的运动。“习俗改革”运动提倡简朴，反对挥霍，打击流浪汉，宣扬的是一种“正派、勤劳、严肃、朴实、守纪、有远见、理智、自制、冷静和节俭”的伦理，类似“人世禁欲主义”。而这其实是一场跨越宗教界限的运动，参与者既有人文主义者，也有新教改革家和天主教改革家，它更像是“一些受过教育的人为了改变人口中其他成员的生活态度和价值观念所做的系统努力”。这种努力是有社会根源可以解释的。15、16世纪农奴制瓦解，资本积累和贫富分化的加剧，使得经济个人主义抬头；而同时文艺复兴使得希腊公益思想重新流行，富人和精英在追求个人利益的同时，不得不扶持贫民。历史学家特雷弗·罗伯把16、17世纪称为伟大的”集体主义“和”慈善捐赠“的时代。这一时期英国的慈善捐款达到了历史上前所未有的高峰。政府方面，从16世纪上半叶开始，颁布了一系列济贫法令，并最后以1601年的《贫穷法》确定下来。所以，富人和处于上升状态的工匠或农民“一方面不得不履行传统道德赋予他们的社会责任，另一方面又因履行这些责任所造成的损失而不平。这种矛盾心理使他们产生出对社会依附阶层，尤其是需要帮助的穷人的潜意识或有意识的仇恨情绪。他们将贫穷归因于穷人本身的懒惰、不知节俭和缺乏远见。他们主张改造穷人，去掉他们身上的不良生活习惯。”因此，习俗改革不仅是文化精英对大众文化的改造，也是社会精英对普通民众的改造。这种改革不可能不对社会精神和经济发展产生影响。</p>\n<p>因此，韦伯的《新教伦理和资本主义精神》从精神因素考虑，分析了思想观念的变革对资本主义发展的重要影响，但他的论证似乎有些过于强调教派神学中国先验成分的影响，而忽视了世俗理性主义和社会经济因素的影响。</p>\n<p>对于我们来说，围绕资本主义的经济组织方式、精神文化都可以说是西方的舶来品，是西方的坚船利炮冲击封建主义的同时带来了资本主义的发展模式以及文化。基督教不是东方的宗教文明。对西方而言，宗教精神对资本主义经济的影响是完整连续的，而对东方来说，则是断层的。缺少文化根基的土地上，本土的现代资本主义是如何产生和发展的，这是一个很有意思的话题。因此，韦伯的《新教伦理与资本主义精神》不仅对于弄清楚西方资本主义的起源问题很重要，对于我国进行中的市场经济实践，以及个人理解身边的经济生活，也都有借鉴意义。</p>\n<p><small><em>参考</em></small></p>\n<p><small><em>向荣：《文化变革与西方资本主义的兴起——读韦伯〈新教伦理与资本主义精神〉》</em></small></p>\n<p><small><em>张椿年:《从信仰到理性——意大利人文主义研究》，浙江人民出版社1993年版，第112页</em></small></p>\n<p><small><em>马戈·托德：《基督教人文主义与清教社会秩序》剑桥大学出版社1987年版，第125页</em></small></p>\n<p><small><em>W.K.乔丹：《英国的慈善事业.1480—1660年》伦敦1960年版</em></small></p>\n<p><small><em>伯克：《早期近代欧洲的大众文化》第213页</em></small></p>"},{"title":"命题逻辑","date":"2020-02-24T17:13:00.000Z","mathjax":true,"_content":"\n语法和语义是符号逻辑的基本要素。\n\n语法中推出的结论，可以在语义中找到一致的对应，称该符号逻辑系统是“**可靠（soundness）**”的；\n\n语义中的结论，都可以从语法中推出来，称该符号逻辑系统是“**完备（completeness）**”的。<!--more-->\n\n验证程序的可靠性（程序是正确的）有两种方法：\n\n1. 霍尔逻辑：使用公理描述程序语句对于计算状态的改变，一步步验证程序语句的逻辑正确。\n2. 软件测试：验证输入输出是否满足程序规约要求。\n\n<br/>\n\n## 命题逻辑的语法\n\n---\n\n### 字母表\n\n字母表的组成：\n\n1. 命题符：$P_0,P_1,P_2,...P_n,n\\in \\mathbb{N}$，记命题符集$PS=\\{P_n|n\\in\\mathbb{N}\\}$\n2. 联结词：$\\neg,\\wedge,\\vee,\\to$\n3. 辅助符：( , )\n\n### 命题\n\n1. 命题符为命题；(原子句式)\n2. 若A，B为命题，则$(\\neg A),(A\\wedge B),(A\\vee B) ,(A\\to B)$也为命题；（复合句式）\n3. 命题仅限于此。\n\nBacus-Naur Form命题定义：$\\psi::=P|(\\neg\\psi)|(\\psi_1\\wedge \\psi_2)|(\\psi_1\\vee\\psi_2)|(\\psi_1\\to \\psi_2)$, 其中$P\\in PS$。这种建构句式的方式成为**递归定义（recursive definition）**。\n\n### 命题集\n\n所有命题的集合PROP是满足以下条件最小集合：\n\n1. $PS\\subseteq  PROP$\n2. 若$A\\in PROP$，则$\\neg (A)\\in PROP$\n3. 若$A，B\\in PROP$，则$ (A\\wedge B),(A\\vee B),(A\\to B)\\in PROP$\n\n### 构造序列\n\n一个有穷序列$A_0,A_1,A_2,...A_n$，若对任意$i\\le n$都满足下列条件之一：\n\n1. $A_i\\in PS$\n2. $\\exists k<i$,使$A_i$为$(\\neg A_k)$\n3. $\\exists k,l<i$,使$A_i$为$(A_k\\wedge A_l)或(A_k\\vee A_l)或(A_k\\to A_l)$\n\n记$A$为$A_n$，则序列$A_0,A_1,A_2,...A_n$称为A的构造序列。\n\n<br/>\n\n## 命题逻辑的语义\n\n---\n\n对于所有命题的集合，只要给定最基本的**命题符**集PS中的命题以真假，则**所有命题**皆可推出真假。这样一来，原本只是一些无意义的符号的命题就被赋予了**语义**。这个过程可表示为：\n\n​\t\t\t\t\t\t\t\t\t对于任意的**赋值**$\\nu:PS\\to\\{T,F\\}$，有**解释**$\\hat\\nu:PROP\\to\\{T,F\\}$\n\n从赋值了的原子命题向复合命题推广语义判断的桥梁，是**联结词**和**真值表**：\n\n<img src=\"/images/image-20200225001445007.png\" alt=\"image-20200225001445007\" style=\"zoom:50%;\" />\n\n其中 $H_{\\neg}:B\\to B$为一元布尔函数，$H_{\\flat}:B^2\\to B$为二元布尔函数（$_{\\flat}$为$\\wedge,\\vee ,\\to$），$H_A:B^n\\to B$为n元布尔函数，又称**真值函数**。可以理解为，联结词是由真值表定义的，每个联结词对应真值表的一个二元布尔函数。\n\n<br/>\n\n将上述产生语义的过程用规范语言表达，可以给出**命题语义的归纳定义**：\n\n* 赋值：$\\nu$为赋值，指给命题符一个真伪的判断：$PS\\to\\{T,F\\}$，也即对于命题符$P_i$，使$\\nu(P_i)=T\\ or\\ F$\n* 解释：$\\hat \\nu $为解释，指对任意命题的真伪判断：$PROP\\to\\{T,F\\}$，详细的判断规则如下：\n  1. $\\hat \\nu(P_i)=\\nu(P_i)$，对命题符而言，它的解释就是赋予它的布尔值\n  2. $\\hat \\nu(\\neg P_i)=H\\neg(\\hat v(P_i))$\n  3. $\\hat \\nu({P_i}_{\\flat}P_j)=H_{\\flat}(\\hat \\nu(Pi,P_j))$，其中$_{\\flat}$为$\\wedge,\\vee,\\to$\n\n<br/>\n\n可以证明，对于一个命题A，要判断其真伪，唯一有关系的是最底层的命题符的真伪；只要基本命题符的赋值一致，就不会推出相悖的结论。这些最底层的命题符也叫做**自由变元（FV）**。\n\n<br/>\n\n现在我们赋予了所有命题以语义（真假值），对于一些特定的复合命题及其真假值，可以定义一些**命题的关系**：\n\n1. 蕴含关系（implication）\n\n   记为$\\phi_1,\\phi_2,...\\phi_n \\vDash \\psi$。它表示的是当$（\\phi_1 \\wedge\\phi_2...\\wedge \\phi_n) \\to \\psi$为恒真句时$\\phi_1,\\phi_2,...\\phi_n$和$\\psi$的关系。\n\n2. 等值关系（equivalence）\n\n   记为$\\vDash \\phi \\leftrightarrow\\psi$。它表示的是当为$\\phi \\leftrightarrow\\psi$恒真句时$\\phi$和$\\psi$的关系。\n\n3. 不一致（inconsistency）\n\n   记为$\\Gamma\\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$为矛盾句时它们的关系。从一个不一致的前提可以推出任何结论，从一个说谎的人嘴里什么都可以得到。\n\n4. 一致（consistency）\n\n   记为$\\Gamma \\not \\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$不是矛盾句时它们的关系。\n\n<br/>\n\n**有效论证**：当前提和结论具有蕴含关系时，称论证是有效的。\n\nPS. 描述赋值、联结等动作使用的这一套固定的语言，称为元语言（meta-language）。如$v\\vDash_{\\neg}P\\to Q$\n\nPPS. 归谬法的原理：\n\n>要证$P \\vDash Q$是一个有效论证\n>\n>即证$P \\to Q$为恒真句\n>\n>只要假设结论为False，证$\\neg Q\\wedge P$为False（得到矛盾）即可，因为：\n>\n>$\\neg(\\neg Q\\wedge P)$为True，$\\vDash \\neg(\\neg Q\\wedge P) \\leftrightarrow (Q \\vee \\neg P) \\leftrightarrow (P\\to Q) $\n\n<br/>\n\n## 析合范式和合析范式\n\n---\n\n现在我们知道，给定一个命题，可以由真值函数$H_A:B^n\\to B$推断命题的真伪；那么反之，如果给定的是一个真值函数f，是不是也能找到一个命题A使得$f=H_A$呢？\n\n**析合范式DNF$(\\vee \\wedge-nf)$**：对若干命题先合取再析取：$ \\stackrel{i}\\vee  (\\stackrel{k}\\wedge P_{i,k)}$\n\n**合析范式CNF$(\\wedge \\vee -nf)$**：对若干命题先析取再合取：$ \\stackrel{j}\\wedge  (\\stackrel{k}\\vee P_{j,k)}$\n\n可以证明，**给定真值函数$f:B^n\\to B$，存在为$\\vee \\wedge-nf$的命题A，使$f=H_A$；也存在为$\\wedge \\vee -nf$的命题A'，使$f=H_{A'}$**。\n\n<img src=\"/images/析合范式.png\" alt=\"析合范式\" style=\"zoom:40%;\" />\n\n<img src=\"/images/图1.png\" alt=\"图1\" style=\"zoom:40%;\" />\n\n<img src=\"/images/图2.png\" alt=\"图2\" style=\"zoom:40%;\" />\n\n对于一个由命题P定义的真值函数f，以及f的析合范式命题A、合析范式命题A'，称P、A、A'是**逻辑等价**的，记为**$P\\simeq A \\simeq A'$**。逻辑等价的命题其真值函数相等。另外也可以证明，与P逻辑等价的$(\\vee \\wedge-nf)$$、(\\wedge \\vee -nf)$形式的命题B、B'就是f的析合范式A和合析范式A'。\n\n任意给一个真值函数$f$，因为仅用$\\neg ,\\wedge, \\vee $三个符号就可以构造一个命题A使得$H_A = f$，所以$\\{\\neg,\\wedge,\\vee\\}$被称为具有**函数完备性（functional complete）**的联结词集。另外，$\\{\\neg,\\to\\},\\{\\neg,\\wedge\\},\\{\\neg,\\vee\\}$这些也都具有函数完备性。\n\n<br/>\n\n\n\n## 真值树系统\n\n---\n\n从古典逻辑的发展历史来看，有三种经常被提到的**语法**演算系统：\n\n1. 公理系统（Axiom System）\n2. 自然演绎系统（Natural Deduction System）\n3. 真值树系统（Tableaux System）\n\n我们这里关心的是如何做证明，研究的是**语法的蕴含关系**，与之前的**语义蕴含关系**不同，蕴含符号不是$\\models$而是$\\vdash$。\n\n逻辑刚刚建立时，古希腊人认为推理的基础是一些简单直白到人人都会认同的**公理**（Axiom），依据这些公理及其包含的推理规则，推导出新的理论，称为**定理**（Theory）。这是公理系统的方法。\n\n但公理系统的推导与证明有些过于艰难。使用公理系统工作的人手里拿着的是一把锤子，拿最简单的工具当然可以建造宏大的宫殿，但需要极大的耐心、一些天赋、以及世世代代的努力。对于现代人来说，锤子显得有些原始，他需要的是更轻松省力的工具。在1935年，Gentzen第一次提出了自然演绎法。自然演绎法舍弃了公理，仅保留推理规则，事实上，引入了更多的推理规则。可以用的规则越多，证明就变得越简单。\n\n而真值树系统就像加减法的竖式计算，学会使用它可以不依靠任何天赋和洞见就能完成证明。要证明的一些命题在树顶，要做的是从树顶向下分解，直到分成一个个命题符（原子句式），它们就是树的“根”。\n\n下面看看如何种下你的第一棵“真值树”：\n\n1. 把你想证明的命题公式放到树顶\n\n2. 按照这些规则向下发展树根：\n\n   ![](/images/tableaux.png)\n\n3. 把出现矛盾的分支关闭：\n\n   ![](/images/closebranch.png)\n\n4. 若最后全部分支都封闭，则原命题为假，是**矛盾句**；若存在开放的分支，试着把原命题取反，再走一遍真值树，如果全部分支封闭，则原命题是**恒真句**；如果仍然存在开放的分支，则原命题是**偶真句**。对于第一种情况，称组成原命题的句式是**不相容的**；对后两种情况，称组成原命题的句式是**相容的**。学过线性代数的人会发现，线性方程组的有解、无解就对应着这个由方程组成的连言式相容或不相容的概念，事实上，也确实是这么叫的：相容的方程组至少有一个解，否则就称这个方程组是不相容的。\n\n   对于蕴含证明来说，将结论取反与前提放在一起组成连言式作为树顶，若全部分支封闭，则论证为有效论证；反之为无效论证。（归谬法）\n\n![](/images/a1.png)\n\n![](/images/d2.png)\n\n![](/images/f3.png)\n\n![](/images/g4.png)\n\n![](/images/i5.png)\n\n![](/images/i6.png)\n\n​\t\t\t\t\t\t<center><small>图片来自台湾通识网 傅皓政老师《逻辑》课讲义</small></center>\n\n<br/>\n\n\n\n## 命题逻辑的自然推理系统\n\n---\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[台大开放式课程 傅皓政《逻辑》](http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2)*</small>\n\n","source":"_posts/命题逻辑.md","raw":"---\ntitle: 命题逻辑 \ndate: 2020-02-25 01:13:00\ncategories:\n- 数理逻辑\ntags: \n- 逻辑学\n- 数学\n- 哲学\nmathjax: true\n\n---\n\n语法和语义是符号逻辑的基本要素。\n\n语法中推出的结论，可以在语义中找到一致的对应，称该符号逻辑系统是“**可靠（soundness）**”的；\n\n语义中的结论，都可以从语法中推出来，称该符号逻辑系统是“**完备（completeness）**”的。<!--more-->\n\n验证程序的可靠性（程序是正确的）有两种方法：\n\n1. 霍尔逻辑：使用公理描述程序语句对于计算状态的改变，一步步验证程序语句的逻辑正确。\n2. 软件测试：验证输入输出是否满足程序规约要求。\n\n<br/>\n\n## 命题逻辑的语法\n\n---\n\n### 字母表\n\n字母表的组成：\n\n1. 命题符：$P_0,P_1,P_2,...P_n,n\\in \\mathbb{N}$，记命题符集$PS=\\{P_n|n\\in\\mathbb{N}\\}$\n2. 联结词：$\\neg,\\wedge,\\vee,\\to$\n3. 辅助符：( , )\n\n### 命题\n\n1. 命题符为命题；(原子句式)\n2. 若A，B为命题，则$(\\neg A),(A\\wedge B),(A\\vee B) ,(A\\to B)$也为命题；（复合句式）\n3. 命题仅限于此。\n\nBacus-Naur Form命题定义：$\\psi::=P|(\\neg\\psi)|(\\psi_1\\wedge \\psi_2)|(\\psi_1\\vee\\psi_2)|(\\psi_1\\to \\psi_2)$, 其中$P\\in PS$。这种建构句式的方式成为**递归定义（recursive definition）**。\n\n### 命题集\n\n所有命题的集合PROP是满足以下条件最小集合：\n\n1. $PS\\subseteq  PROP$\n2. 若$A\\in PROP$，则$\\neg (A)\\in PROP$\n3. 若$A，B\\in PROP$，则$ (A\\wedge B),(A\\vee B),(A\\to B)\\in PROP$\n\n### 构造序列\n\n一个有穷序列$A_0,A_1,A_2,...A_n$，若对任意$i\\le n$都满足下列条件之一：\n\n1. $A_i\\in PS$\n2. $\\exists k<i$,使$A_i$为$(\\neg A_k)$\n3. $\\exists k,l<i$,使$A_i$为$(A_k\\wedge A_l)或(A_k\\vee A_l)或(A_k\\to A_l)$\n\n记$A$为$A_n$，则序列$A_0,A_1,A_2,...A_n$称为A的构造序列。\n\n<br/>\n\n## 命题逻辑的语义\n\n---\n\n对于所有命题的集合，只要给定最基本的**命题符**集PS中的命题以真假，则**所有命题**皆可推出真假。这样一来，原本只是一些无意义的符号的命题就被赋予了**语义**。这个过程可表示为：\n\n​\t\t\t\t\t\t\t\t\t对于任意的**赋值**$\\nu:PS\\to\\{T,F\\}$，有**解释**$\\hat\\nu:PROP\\to\\{T,F\\}$\n\n从赋值了的原子命题向复合命题推广语义判断的桥梁，是**联结词**和**真值表**：\n\n<img src=\"/images/image-20200225001445007.png\" alt=\"image-20200225001445007\" style=\"zoom:50%;\" />\n\n其中 $H_{\\neg}:B\\to B$为一元布尔函数，$H_{\\flat}:B^2\\to B$为二元布尔函数（$_{\\flat}$为$\\wedge,\\vee ,\\to$），$H_A:B^n\\to B$为n元布尔函数，又称**真值函数**。可以理解为，联结词是由真值表定义的，每个联结词对应真值表的一个二元布尔函数。\n\n<br/>\n\n将上述产生语义的过程用规范语言表达，可以给出**命题语义的归纳定义**：\n\n* 赋值：$\\nu$为赋值，指给命题符一个真伪的判断：$PS\\to\\{T,F\\}$，也即对于命题符$P_i$，使$\\nu(P_i)=T\\ or\\ F$\n* 解释：$\\hat \\nu $为解释，指对任意命题的真伪判断：$PROP\\to\\{T,F\\}$，详细的判断规则如下：\n  1. $\\hat \\nu(P_i)=\\nu(P_i)$，对命题符而言，它的解释就是赋予它的布尔值\n  2. $\\hat \\nu(\\neg P_i)=H\\neg(\\hat v(P_i))$\n  3. $\\hat \\nu({P_i}_{\\flat}P_j)=H_{\\flat}(\\hat \\nu(Pi,P_j))$，其中$_{\\flat}$为$\\wedge,\\vee,\\to$\n\n<br/>\n\n可以证明，对于一个命题A，要判断其真伪，唯一有关系的是最底层的命题符的真伪；只要基本命题符的赋值一致，就不会推出相悖的结论。这些最底层的命题符也叫做**自由变元（FV）**。\n\n<br/>\n\n现在我们赋予了所有命题以语义（真假值），对于一些特定的复合命题及其真假值，可以定义一些**命题的关系**：\n\n1. 蕴含关系（implication）\n\n   记为$\\phi_1,\\phi_2,...\\phi_n \\vDash \\psi$。它表示的是当$（\\phi_1 \\wedge\\phi_2...\\wedge \\phi_n) \\to \\psi$为恒真句时$\\phi_1,\\phi_2,...\\phi_n$和$\\psi$的关系。\n\n2. 等值关系（equivalence）\n\n   记为$\\vDash \\phi \\leftrightarrow\\psi$。它表示的是当为$\\phi \\leftrightarrow\\psi$恒真句时$\\phi$和$\\psi$的关系。\n\n3. 不一致（inconsistency）\n\n   记为$\\Gamma\\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$为矛盾句时它们的关系。从一个不一致的前提可以推出任何结论，从一个说谎的人嘴里什么都可以得到。\n\n4. 一致（consistency）\n\n   记为$\\Gamma \\not \\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$不是矛盾句时它们的关系。\n\n<br/>\n\n**有效论证**：当前提和结论具有蕴含关系时，称论证是有效的。\n\nPS. 描述赋值、联结等动作使用的这一套固定的语言，称为元语言（meta-language）。如$v\\vDash_{\\neg}P\\to Q$\n\nPPS. 归谬法的原理：\n\n>要证$P \\vDash Q$是一个有效论证\n>\n>即证$P \\to Q$为恒真句\n>\n>只要假设结论为False，证$\\neg Q\\wedge P$为False（得到矛盾）即可，因为：\n>\n>$\\neg(\\neg Q\\wedge P)$为True，$\\vDash \\neg(\\neg Q\\wedge P) \\leftrightarrow (Q \\vee \\neg P) \\leftrightarrow (P\\to Q) $\n\n<br/>\n\n## 析合范式和合析范式\n\n---\n\n现在我们知道，给定一个命题，可以由真值函数$H_A:B^n\\to B$推断命题的真伪；那么反之，如果给定的是一个真值函数f，是不是也能找到一个命题A使得$f=H_A$呢？\n\n**析合范式DNF$(\\vee \\wedge-nf)$**：对若干命题先合取再析取：$ \\stackrel{i}\\vee  (\\stackrel{k}\\wedge P_{i,k)}$\n\n**合析范式CNF$(\\wedge \\vee -nf)$**：对若干命题先析取再合取：$ \\stackrel{j}\\wedge  (\\stackrel{k}\\vee P_{j,k)}$\n\n可以证明，**给定真值函数$f:B^n\\to B$，存在为$\\vee \\wedge-nf$的命题A，使$f=H_A$；也存在为$\\wedge \\vee -nf$的命题A'，使$f=H_{A'}$**。\n\n<img src=\"/images/析合范式.png\" alt=\"析合范式\" style=\"zoom:40%;\" />\n\n<img src=\"/images/图1.png\" alt=\"图1\" style=\"zoom:40%;\" />\n\n<img src=\"/images/图2.png\" alt=\"图2\" style=\"zoom:40%;\" />\n\n对于一个由命题P定义的真值函数f，以及f的析合范式命题A、合析范式命题A'，称P、A、A'是**逻辑等价**的，记为**$P\\simeq A \\simeq A'$**。逻辑等价的命题其真值函数相等。另外也可以证明，与P逻辑等价的$(\\vee \\wedge-nf)$$、(\\wedge \\vee -nf)$形式的命题B、B'就是f的析合范式A和合析范式A'。\n\n任意给一个真值函数$f$，因为仅用$\\neg ,\\wedge, \\vee $三个符号就可以构造一个命题A使得$H_A = f$，所以$\\{\\neg,\\wedge,\\vee\\}$被称为具有**函数完备性（functional complete）**的联结词集。另外，$\\{\\neg,\\to\\},\\{\\neg,\\wedge\\},\\{\\neg,\\vee\\}$这些也都具有函数完备性。\n\n<br/>\n\n\n\n## 真值树系统\n\n---\n\n从古典逻辑的发展历史来看，有三种经常被提到的**语法**演算系统：\n\n1. 公理系统（Axiom System）\n2. 自然演绎系统（Natural Deduction System）\n3. 真值树系统（Tableaux System）\n\n我们这里关心的是如何做证明，研究的是**语法的蕴含关系**，与之前的**语义蕴含关系**不同，蕴含符号不是$\\models$而是$\\vdash$。\n\n逻辑刚刚建立时，古希腊人认为推理的基础是一些简单直白到人人都会认同的**公理**（Axiom），依据这些公理及其包含的推理规则，推导出新的理论，称为**定理**（Theory）。这是公理系统的方法。\n\n但公理系统的推导与证明有些过于艰难。使用公理系统工作的人手里拿着的是一把锤子，拿最简单的工具当然可以建造宏大的宫殿，但需要极大的耐心、一些天赋、以及世世代代的努力。对于现代人来说，锤子显得有些原始，他需要的是更轻松省力的工具。在1935年，Gentzen第一次提出了自然演绎法。自然演绎法舍弃了公理，仅保留推理规则，事实上，引入了更多的推理规则。可以用的规则越多，证明就变得越简单。\n\n而真值树系统就像加减法的竖式计算，学会使用它可以不依靠任何天赋和洞见就能完成证明。要证明的一些命题在树顶，要做的是从树顶向下分解，直到分成一个个命题符（原子句式），它们就是树的“根”。\n\n下面看看如何种下你的第一棵“真值树”：\n\n1. 把你想证明的命题公式放到树顶\n\n2. 按照这些规则向下发展树根：\n\n   ![](/images/tableaux.png)\n\n3. 把出现矛盾的分支关闭：\n\n   ![](/images/closebranch.png)\n\n4. 若最后全部分支都封闭，则原命题为假，是**矛盾句**；若存在开放的分支，试着把原命题取反，再走一遍真值树，如果全部分支封闭，则原命题是**恒真句**；如果仍然存在开放的分支，则原命题是**偶真句**。对于第一种情况，称组成原命题的句式是**不相容的**；对后两种情况，称组成原命题的句式是**相容的**。学过线性代数的人会发现，线性方程组的有解、无解就对应着这个由方程组成的连言式相容或不相容的概念，事实上，也确实是这么叫的：相容的方程组至少有一个解，否则就称这个方程组是不相容的。\n\n   对于蕴含证明来说，将结论取反与前提放在一起组成连言式作为树顶，若全部分支封闭，则论证为有效论证；反之为无效论证。（归谬法）\n\n![](/images/a1.png)\n\n![](/images/d2.png)\n\n![](/images/f3.png)\n\n![](/images/g4.png)\n\n![](/images/i5.png)\n\n![](/images/i6.png)\n\n​\t\t\t\t\t\t<center><small>图片来自台湾通识网 傅皓政老师《逻辑》课讲义</small></center>\n\n<br/>\n\n\n\n## 命题逻辑的自然推理系统\n\n---\n\n\n\n<br/>\n\n<br/>\n\n<br/>\n\n<br/>\n\n<small>*参考*</small>\n\n<small>*[台大开放式课程 傅皓政《逻辑》](http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2)*</small>\n\n","slug":"命题逻辑","published":1,"updated":"2020-10-26T05:56:31.118Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6v002d2sfy8cr39haf","content":"<p>语法和语义是符号逻辑的基本要素。</p>\n<p>语法中推出的结论，可以在语义中找到一致的对应，称该符号逻辑系统是“<strong>可靠（soundness）</strong>”的；</p>\n<p>语义中的结论，都可以从语法中推出来，称该符号逻辑系统是“<strong>完备（completeness）</strong>”的。<a id=\"more\"></a></p>\n<p>验证程序的可靠性（程序是正确的）有两种方法：</p>\n<ol>\n<li>霍尔逻辑：使用公理描述程序语句对于计算状态的改变，一步步验证程序语句的逻辑正确。</li>\n<li>软件测试：验证输入输出是否满足程序规约要求。</li>\n</ol>\n<p><br/></p>\n<h2 id=\"命题逻辑的语法\"><a href=\"#命题逻辑的语法\" class=\"headerlink\" title=\"命题逻辑的语法\"></a>命题逻辑的语法</h2><hr>\n<h3 id=\"字母表\"><a href=\"#字母表\" class=\"headerlink\" title=\"字母表\"></a>字母表</h3><p>字母表的组成：</p>\n<ol>\n<li>命题符：$P_0,P_1,P_2,…P_n,n\\in \\mathbb{N}$，记命题符集$PS=\\{P_n|n\\in\\mathbb{N}\\}$</li>\n<li>联结词：$\\neg,\\wedge,\\vee,\\to$</li>\n<li>辅助符：( , )</li>\n</ol>\n<h3 id=\"命题\"><a href=\"#命题\" class=\"headerlink\" title=\"命题\"></a>命题</h3><ol>\n<li>命题符为命题；(原子句式)</li>\n<li>若A，B为命题，则$(\\neg A),(A\\wedge B),(A\\vee B) ,(A\\to B)$也为命题；（复合句式）</li>\n<li>命题仅限于此。</li>\n</ol>\n<p>Bacus-Naur Form命题定义：$\\psi::=P|(\\neg\\psi)|(\\psi_1\\wedge \\psi_2)|(\\psi_1\\vee\\psi_2)|(\\psi_1\\to \\psi_2)$, 其中$P\\in PS$。这种建构句式的方式成为<strong>递归定义（recursive definition）</strong>。</p>\n<h3 id=\"命题集\"><a href=\"#命题集\" class=\"headerlink\" title=\"命题集\"></a>命题集</h3><p>所有命题的集合PROP是满足以下条件最小集合：</p>\n<ol>\n<li>$PS\\subseteq  PROP$</li>\n<li>若$A\\in PROP$，则$\\neg (A)\\in PROP$</li>\n<li>若$A，B\\in PROP$，则$ (A\\wedge B),(A\\vee B),(A\\to B)\\in PROP$</li>\n</ol>\n<h3 id=\"构造序列\"><a href=\"#构造序列\" class=\"headerlink\" title=\"构造序列\"></a>构造序列</h3><p>一个有穷序列$A_0,A_1,A_2,…A_n$，若对任意$i\\le n$都满足下列条件之一：</p>\n<ol>\n<li>$A_i\\in PS$</li>\n<li>$\\exists k&lt;i$,使$A_i$为$(\\neg A_k)$</li>\n<li>$\\exists k,l&lt;i$,使$A_i$为$(A_k\\wedge A_l)或(A_k\\vee A_l)或(A_k\\to A_l)$</li>\n</ol>\n<p>记$A$为$A_n$，则序列$A_0,A_1,A_2,…A_n$称为A的构造序列。</p>\n<p><br/></p>\n<h2 id=\"命题逻辑的语义\"><a href=\"#命题逻辑的语义\" class=\"headerlink\" title=\"命题逻辑的语义\"></a>命题逻辑的语义</h2><hr>\n<p>对于所有命题的集合，只要给定最基本的<strong>命题符</strong>集PS中的命题以真假，则<strong>所有命题</strong>皆可推出真假。这样一来，原本只是一些无意义的符号的命题就被赋予了<strong>语义</strong>。这个过程可表示为：</p>\n<p>​                                    对于任意的<strong>赋值</strong>$\\nu:PS\\to\\{T,F\\}$，有<strong>解释</strong>$\\hat\\nu:PROP\\to\\{T,F\\}$</p>\n<p>从赋值了的原子命题向复合命题推广语义判断的桥梁，是<strong>联结词</strong>和<strong>真值表</strong>：</p>\n<p><img src=\"/images/image-20200225001445007.png\" alt=\"image-20200225001445007\" style=\"zoom:50%;\" /></p>\n<p>其中 $H_{\\neg}:B\\to B$为一元布尔函数，$H_{\\flat}:B^2\\to B$为二元布尔函数（$_{\\flat}$为$\\wedge,\\vee ,\\to$），$H_A:B^n\\to B$为n元布尔函数，又称<strong>真值函数</strong>。可以理解为，联结词是由真值表定义的，每个联结词对应真值表的一个二元布尔函数。</p>\n<p><br/></p>\n<p>将上述产生语义的过程用规范语言表达，可以给出<strong>命题语义的归纳定义</strong>：</p>\n<ul>\n<li>赋值：$\\nu$为赋值，指给命题符一个真伪的判断：$PS\\to\\{T,F\\}$，也即对于命题符$P_i$，使$\\nu(P_i)=T\\ or\\ F$</li>\n<li>解释：$\\hat \\nu $为解释，指对任意命题的真伪判断：$PROP\\to\\{T,F\\}$，详细的判断规则如下：<ol>\n<li>$\\hat \\nu(P_i)=\\nu(P_i)$，对命题符而言，它的解释就是赋予它的布尔值</li>\n<li>$\\hat \\nu(\\neg P_i)=H\\neg(\\hat v(P_i))$</li>\n<li>$\\hat \\nu({P_i}_{\\flat}P_j)=H_{\\flat}(\\hat \\nu(Pi,P_j))$，其中$_{\\flat}$为$\\wedge,\\vee,\\to$</li>\n</ol>\n</li>\n</ul>\n<p><br/></p>\n<p>可以证明，对于一个命题A，要判断其真伪，唯一有关系的是最底层的命题符的真伪；只要基本命题符的赋值一致，就不会推出相悖的结论。这些最底层的命题符也叫做<strong>自由变元（FV）</strong>。</p>\n<p><br/></p>\n<p>现在我们赋予了所有命题以语义（真假值），对于一些特定的复合命题及其真假值，可以定义一些<strong>命题的关系</strong>：</p>\n<ol>\n<li><p>蕴含关系（implication）</p>\n<p>记为$\\phi_1,\\phi_2,…\\phi_n \\vDash \\psi$。它表示的是当$（\\phi_1 \\wedge\\phi_2…\\wedge \\phi_n) \\to \\psi$为恒真句时$\\phi_1,\\phi_2,…\\phi_n$和$\\psi$的关系。</p>\n</li>\n<li><p>等值关系（equivalence）</p>\n<p>记为$\\vDash \\phi \\leftrightarrow\\psi$。它表示的是当为$\\phi \\leftrightarrow\\psi$恒真句时$\\phi$和$\\psi$的关系。</p>\n</li>\n<li><p>不一致（inconsistency）</p>\n<p>记为$\\Gamma\\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$为矛盾句时它们的关系。从一个不一致的前提可以推出任何结论，从一个说谎的人嘴里什么都可以得到。</p>\n</li>\n<li><p>一致（consistency）</p>\n<p>记为$\\Gamma \\not \\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$不是矛盾句时它们的关系。</p>\n</li>\n</ol>\n<p><br/></p>\n<p><strong>有效论证</strong>：当前提和结论具有蕴含关系时，称论证是有效的。</p>\n<p>PS. 描述赋值、联结等动作使用的这一套固定的语言，称为元语言（meta-language）。如$v\\vDash_{\\neg}P\\to Q$</p>\n<p>PPS. 归谬法的原理：</p>\n<blockquote>\n<p>要证$P \\vDash Q$是一个有效论证</p>\n<p>即证$P \\to Q$为恒真句</p>\n<p>只要假设结论为False，证$\\neg Q\\wedge P$为False（得到矛盾）即可，因为：</p>\n<p>$\\neg(\\neg Q\\wedge P)$为True，$\\vDash \\neg(\\neg Q\\wedge P) \\leftrightarrow (Q \\vee \\neg P) \\leftrightarrow (P\\to Q) $</p>\n</blockquote>\n<p><br/></p>\n<h2 id=\"析合范式和合析范式\"><a href=\"#析合范式和合析范式\" class=\"headerlink\" title=\"析合范式和合析范式\"></a>析合范式和合析范式</h2><hr>\n<p>现在我们知道，给定一个命题，可以由真值函数$H_A:B^n\\to B$推断命题的真伪；那么反之，如果给定的是一个真值函数f，是不是也能找到一个命题A使得$f=H_A$呢？</p>\n<p><strong>析合范式DNF$(\\vee \\wedge-nf)$</strong>：对若干命题先合取再析取：$ \\stackrel{i}\\vee  (\\stackrel{k}\\wedge P_{i,k)}$</p>\n<p><strong>合析范式CNF$(\\wedge \\vee -nf)$</strong>：对若干命题先析取再合取：$ \\stackrel{j}\\wedge  (\\stackrel{k}\\vee P_{j,k)}$</p>\n<p>可以证明，<strong>给定真值函数$f:B^n\\to B$，存在为$\\vee \\wedge-nf$的命题A，使$f=H_A$；也存在为$\\wedge \\vee -nf$的命题A’，使$f=H_{A’}$</strong>。</p>\n<p><img src=\"/images/析合范式.png\" alt=\"析合范式\" style=\"zoom:40%;\" /></p>\n<p><img src=\"/images/图1.png\" alt=\"图1\" style=\"zoom:40%;\" /></p>\n<p><img src=\"/images/图2.png\" alt=\"图2\" style=\"zoom:40%;\" /></p>\n<p>对于一个由命题P定义的真值函数f，以及f的析合范式命题A、合析范式命题A’，称P、A、A’是<strong>逻辑等价</strong>的，记为<strong>$P\\simeq A \\simeq A’$</strong>。逻辑等价的命题其真值函数相等。另外也可以证明，与P逻辑等价的$(\\vee \\wedge-nf)$$、(\\wedge \\vee -nf)$形式的命题B、B’就是f的析合范式A和合析范式A’。</p>\n<p>任意给一个真值函数$f$，因为仅用$\\neg ,\\wedge, \\vee $三个符号就可以构造一个命题A使得$H_A = f$，所以$\\{\\neg,\\wedge,\\vee\\}$被称为具有<strong>函数完备性（functional complete）</strong>的联结词集。另外，$\\{\\neg,\\to\\},\\{\\neg,\\wedge\\},\\{\\neg,\\vee\\}$这些也都具有函数完备性。</p>\n<p><br/></p>\n<h2 id=\"真值树系统\"><a href=\"#真值树系统\" class=\"headerlink\" title=\"真值树系统\"></a>真值树系统</h2><hr>\n<p>从古典逻辑的发展历史来看，有三种经常被提到的<strong>语法</strong>演算系统：</p>\n<ol>\n<li>公理系统（Axiom System）</li>\n<li>自然演绎系统（Natural Deduction System）</li>\n<li>真值树系统（Tableaux System）</li>\n</ol>\n<p>我们这里关心的是如何做证明，研究的是<strong>语法的蕴含关系</strong>，与之前的<strong>语义蕴含关系</strong>不同，蕴含符号不是$\\models$而是$\\vdash$。</p>\n<p>逻辑刚刚建立时，古希腊人认为推理的基础是一些简单直白到人人都会认同的<strong>公理</strong>（Axiom），依据这些公理及其包含的推理规则，推导出新的理论，称为<strong>定理</strong>（Theory）。这是公理系统的方法。</p>\n<p>但公理系统的推导与证明有些过于艰难。使用公理系统工作的人手里拿着的是一把锤子，拿最简单的工具当然可以建造宏大的宫殿，但需要极大的耐心、一些天赋、以及世世代代的努力。对于现代人来说，锤子显得有些原始，他需要的是更轻松省力的工具。在1935年，Gentzen第一次提出了自然演绎法。自然演绎法舍弃了公理，仅保留推理规则，事实上，引入了更多的推理规则。可以用的规则越多，证明就变得越简单。</p>\n<p>而真值树系统就像加减法的竖式计算，学会使用它可以不依靠任何天赋和洞见就能完成证明。要证明的一些命题在树顶，要做的是从树顶向下分解，直到分成一个个命题符（原子句式），它们就是树的“根”。</p>\n<p>下面看看如何种下你的第一棵“真值树”：</p>\n<ol>\n<li><p>把你想证明的命题公式放到树顶</p>\n</li>\n<li><p>按照这些规则向下发展树根：</p>\n<p><img src=\"/images/tableaux.png\" alt=\"\"></p>\n</li>\n<li><p>把出现矛盾的分支关闭：</p>\n<p><img src=\"/images/closebranch.png\" alt=\"\"></p>\n</li>\n<li><p>若最后全部分支都封闭，则原命题为假，是<strong>矛盾句</strong>；若存在开放的分支，试着把原命题取反，再走一遍真值树，如果全部分支封闭，则原命题是<strong>恒真句</strong>；如果仍然存在开放的分支，则原命题是<strong>偶真句</strong>。对于第一种情况，称组成原命题的句式是<strong>不相容的</strong>；对后两种情况，称组成原命题的句式是<strong>相容的</strong>。学过线性代数的人会发现，线性方程组的有解、无解就对应着这个由方程组成的连言式相容或不相容的概念，事实上，也确实是这么叫的：相容的方程组至少有一个解，否则就称这个方程组是不相容的。</p>\n<p>对于蕴含证明来说，将结论取反与前提放在一起组成连言式作为树顶，若全部分支封闭，则论证为有效论证；反之为无效论证。（归谬法）</p>\n</li>\n</ol>\n<p><img src=\"/images/a1.png\" alt=\"\"></p>\n<p><img src=\"/images/d2.png\" alt=\"\"></p>\n<p><img src=\"/images/f3.png\" alt=\"\"></p>\n<p><img src=\"/images/g4.png\" alt=\"\"></p>\n<p><img src=\"/images/i5.png\" alt=\"\"></p>\n<p><img src=\"/images/i6.png\" alt=\"\"></p>\n<p>​                        <center><small>图片来自台湾通识网 傅皓政老师《逻辑》课讲义</small></center></p>\n<p><br/></p>\n<h2 id=\"命题逻辑的自然推理系统\"><a href=\"#命题逻辑的自然推理系统\" class=\"headerlink\" title=\"命题逻辑的自然推理系统\"></a>命题逻辑的自然推理系统</h2><hr>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2\" target=\"_blank\" rel=\"noopener\">台大开放式课程 傅皓政《逻辑》</a></em></small></p>\n","site":{"data":{}},"excerpt":"<p>语法和语义是符号逻辑的基本要素。</p>\n<p>语法中推出的结论，可以在语义中找到一致的对应，称该符号逻辑系统是“<strong>可靠（soundness）</strong>”的；</p>\n<p>语义中的结论，都可以从语法中推出来，称该符号逻辑系统是“<strong>完备（completeness）</strong>”的。","more":"</p>\n<p>验证程序的可靠性（程序是正确的）有两种方法：</p>\n<ol>\n<li>霍尔逻辑：使用公理描述程序语句对于计算状态的改变，一步步验证程序语句的逻辑正确。</li>\n<li>软件测试：验证输入输出是否满足程序规约要求。</li>\n</ol>\n<p><br/></p>\n<h2 id=\"命题逻辑的语法\"><a href=\"#命题逻辑的语法\" class=\"headerlink\" title=\"命题逻辑的语法\"></a>命题逻辑的语法</h2><hr>\n<h3 id=\"字母表\"><a href=\"#字母表\" class=\"headerlink\" title=\"字母表\"></a>字母表</h3><p>字母表的组成：</p>\n<ol>\n<li>命题符：$P_0,P_1,P_2,…P_n,n\\in \\mathbb{N}$，记命题符集$PS=\\{P_n|n\\in\\mathbb{N}\\}$</li>\n<li>联结词：$\\neg,\\wedge,\\vee,\\to$</li>\n<li>辅助符：( , )</li>\n</ol>\n<h3 id=\"命题\"><a href=\"#命题\" class=\"headerlink\" title=\"命题\"></a>命题</h3><ol>\n<li>命题符为命题；(原子句式)</li>\n<li>若A，B为命题，则$(\\neg A),(A\\wedge B),(A\\vee B) ,(A\\to B)$也为命题；（复合句式）</li>\n<li>命题仅限于此。</li>\n</ol>\n<p>Bacus-Naur Form命题定义：$\\psi::=P|(\\neg\\psi)|(\\psi_1\\wedge \\psi_2)|(\\psi_1\\vee\\psi_2)|(\\psi_1\\to \\psi_2)$, 其中$P\\in PS$。这种建构句式的方式成为<strong>递归定义（recursive definition）</strong>。</p>\n<h3 id=\"命题集\"><a href=\"#命题集\" class=\"headerlink\" title=\"命题集\"></a>命题集</h3><p>所有命题的集合PROP是满足以下条件最小集合：</p>\n<ol>\n<li>$PS\\subseteq  PROP$</li>\n<li>若$A\\in PROP$，则$\\neg (A)\\in PROP$</li>\n<li>若$A，B\\in PROP$，则$ (A\\wedge B),(A\\vee B),(A\\to B)\\in PROP$</li>\n</ol>\n<h3 id=\"构造序列\"><a href=\"#构造序列\" class=\"headerlink\" title=\"构造序列\"></a>构造序列</h3><p>一个有穷序列$A_0,A_1,A_2,…A_n$，若对任意$i\\le n$都满足下列条件之一：</p>\n<ol>\n<li>$A_i\\in PS$</li>\n<li>$\\exists k&lt;i$,使$A_i$为$(\\neg A_k)$</li>\n<li>$\\exists k,l&lt;i$,使$A_i$为$(A_k\\wedge A_l)或(A_k\\vee A_l)或(A_k\\to A_l)$</li>\n</ol>\n<p>记$A$为$A_n$，则序列$A_0,A_1,A_2,…A_n$称为A的构造序列。</p>\n<p><br/></p>\n<h2 id=\"命题逻辑的语义\"><a href=\"#命题逻辑的语义\" class=\"headerlink\" title=\"命题逻辑的语义\"></a>命题逻辑的语义</h2><hr>\n<p>对于所有命题的集合，只要给定最基本的<strong>命题符</strong>集PS中的命题以真假，则<strong>所有命题</strong>皆可推出真假。这样一来，原本只是一些无意义的符号的命题就被赋予了<strong>语义</strong>。这个过程可表示为：</p>\n<p>​                                    对于任意的<strong>赋值</strong>$\\nu:PS\\to\\{T,F\\}$，有<strong>解释</strong>$\\hat\\nu:PROP\\to\\{T,F\\}$</p>\n<p>从赋值了的原子命题向复合命题推广语义判断的桥梁，是<strong>联结词</strong>和<strong>真值表</strong>：</p>\n<p><img src=\"/images/image-20200225001445007.png\" alt=\"image-20200225001445007\" style=\"zoom:50%;\" /></p>\n<p>其中 $H_{\\neg}:B\\to B$为一元布尔函数，$H_{\\flat}:B^2\\to B$为二元布尔函数（$_{\\flat}$为$\\wedge,\\vee ,\\to$），$H_A:B^n\\to B$为n元布尔函数，又称<strong>真值函数</strong>。可以理解为，联结词是由真值表定义的，每个联结词对应真值表的一个二元布尔函数。</p>\n<p><br/></p>\n<p>将上述产生语义的过程用规范语言表达，可以给出<strong>命题语义的归纳定义</strong>：</p>\n<ul>\n<li>赋值：$\\nu$为赋值，指给命题符一个真伪的判断：$PS\\to\\{T,F\\}$，也即对于命题符$P_i$，使$\\nu(P_i)=T\\ or\\ F$</li>\n<li>解释：$\\hat \\nu $为解释，指对任意命题的真伪判断：$PROP\\to\\{T,F\\}$，详细的判断规则如下：<ol>\n<li>$\\hat \\nu(P_i)=\\nu(P_i)$，对命题符而言，它的解释就是赋予它的布尔值</li>\n<li>$\\hat \\nu(\\neg P_i)=H\\neg(\\hat v(P_i))$</li>\n<li>$\\hat \\nu({P_i}_{\\flat}P_j)=H_{\\flat}(\\hat \\nu(Pi,P_j))$，其中$_{\\flat}$为$\\wedge,\\vee,\\to$</li>\n</ol>\n</li>\n</ul>\n<p><br/></p>\n<p>可以证明，对于一个命题A，要判断其真伪，唯一有关系的是最底层的命题符的真伪；只要基本命题符的赋值一致，就不会推出相悖的结论。这些最底层的命题符也叫做<strong>自由变元（FV）</strong>。</p>\n<p><br/></p>\n<p>现在我们赋予了所有命题以语义（真假值），对于一些特定的复合命题及其真假值，可以定义一些<strong>命题的关系</strong>：</p>\n<ol>\n<li><p>蕴含关系（implication）</p>\n<p>记为$\\phi_1,\\phi_2,…\\phi_n \\vDash \\psi$。它表示的是当$（\\phi_1 \\wedge\\phi_2…\\wedge \\phi_n) \\to \\psi$为恒真句时$\\phi_1,\\phi_2,…\\phi_n$和$\\psi$的关系。</p>\n</li>\n<li><p>等值关系（equivalence）</p>\n<p>记为$\\vDash \\phi \\leftrightarrow\\psi$。它表示的是当为$\\phi \\leftrightarrow\\psi$恒真句时$\\phi$和$\\psi$的关系。</p>\n</li>\n<li><p>不一致（inconsistency）</p>\n<p>记为$\\Gamma\\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$为矛盾句时它们的关系。从一个不一致的前提可以推出任何结论，从一个说谎的人嘴里什么都可以得到。</p>\n</li>\n<li><p>一致（consistency）</p>\n<p>记为$\\Gamma \\not \\vDash$。它表示的是当为$\\Gamma$中所有命题的连言$\\wedge$不是矛盾句时它们的关系。</p>\n</li>\n</ol>\n<p><br/></p>\n<p><strong>有效论证</strong>：当前提和结论具有蕴含关系时，称论证是有效的。</p>\n<p>PS. 描述赋值、联结等动作使用的这一套固定的语言，称为元语言（meta-language）。如$v\\vDash_{\\neg}P\\to Q$</p>\n<p>PPS. 归谬法的原理：</p>\n<blockquote>\n<p>要证$P \\vDash Q$是一个有效论证</p>\n<p>即证$P \\to Q$为恒真句</p>\n<p>只要假设结论为False，证$\\neg Q\\wedge P$为False（得到矛盾）即可，因为：</p>\n<p>$\\neg(\\neg Q\\wedge P)$为True，$\\vDash \\neg(\\neg Q\\wedge P) \\leftrightarrow (Q \\vee \\neg P) \\leftrightarrow (P\\to Q) $</p>\n</blockquote>\n<p><br/></p>\n<h2 id=\"析合范式和合析范式\"><a href=\"#析合范式和合析范式\" class=\"headerlink\" title=\"析合范式和合析范式\"></a>析合范式和合析范式</h2><hr>\n<p>现在我们知道，给定一个命题，可以由真值函数$H_A:B^n\\to B$推断命题的真伪；那么反之，如果给定的是一个真值函数f，是不是也能找到一个命题A使得$f=H_A$呢？</p>\n<p><strong>析合范式DNF$(\\vee \\wedge-nf)$</strong>：对若干命题先合取再析取：$ \\stackrel{i}\\vee  (\\stackrel{k}\\wedge P_{i,k)}$</p>\n<p><strong>合析范式CNF$(\\wedge \\vee -nf)$</strong>：对若干命题先析取再合取：$ \\stackrel{j}\\wedge  (\\stackrel{k}\\vee P_{j,k)}$</p>\n<p>可以证明，<strong>给定真值函数$f:B^n\\to B$，存在为$\\vee \\wedge-nf$的命题A，使$f=H_A$；也存在为$\\wedge \\vee -nf$的命题A’，使$f=H_{A’}$</strong>。</p>\n<p><img src=\"/images/析合范式.png\" alt=\"析合范式\" style=\"zoom:40%;\" /></p>\n<p><img src=\"/images/图1.png\" alt=\"图1\" style=\"zoom:40%;\" /></p>\n<p><img src=\"/images/图2.png\" alt=\"图2\" style=\"zoom:40%;\" /></p>\n<p>对于一个由命题P定义的真值函数f，以及f的析合范式命题A、合析范式命题A’，称P、A、A’是<strong>逻辑等价</strong>的，记为<strong>$P\\simeq A \\simeq A’$</strong>。逻辑等价的命题其真值函数相等。另外也可以证明，与P逻辑等价的$(\\vee \\wedge-nf)$$、(\\wedge \\vee -nf)$形式的命题B、B’就是f的析合范式A和合析范式A’。</p>\n<p>任意给一个真值函数$f$，因为仅用$\\neg ,\\wedge, \\vee $三个符号就可以构造一个命题A使得$H_A = f$，所以$\\{\\neg,\\wedge,\\vee\\}$被称为具有<strong>函数完备性（functional complete）</strong>的联结词集。另外，$\\{\\neg,\\to\\},\\{\\neg,\\wedge\\},\\{\\neg,\\vee\\}$这些也都具有函数完备性。</p>\n<p><br/></p>\n<h2 id=\"真值树系统\"><a href=\"#真值树系统\" class=\"headerlink\" title=\"真值树系统\"></a>真值树系统</h2><hr>\n<p>从古典逻辑的发展历史来看，有三种经常被提到的<strong>语法</strong>演算系统：</p>\n<ol>\n<li>公理系统（Axiom System）</li>\n<li>自然演绎系统（Natural Deduction System）</li>\n<li>真值树系统（Tableaux System）</li>\n</ol>\n<p>我们这里关心的是如何做证明，研究的是<strong>语法的蕴含关系</strong>，与之前的<strong>语义蕴含关系</strong>不同，蕴含符号不是$\\models$而是$\\vdash$。</p>\n<p>逻辑刚刚建立时，古希腊人认为推理的基础是一些简单直白到人人都会认同的<strong>公理</strong>（Axiom），依据这些公理及其包含的推理规则，推导出新的理论，称为<strong>定理</strong>（Theory）。这是公理系统的方法。</p>\n<p>但公理系统的推导与证明有些过于艰难。使用公理系统工作的人手里拿着的是一把锤子，拿最简单的工具当然可以建造宏大的宫殿，但需要极大的耐心、一些天赋、以及世世代代的努力。对于现代人来说，锤子显得有些原始，他需要的是更轻松省力的工具。在1935年，Gentzen第一次提出了自然演绎法。自然演绎法舍弃了公理，仅保留推理规则，事实上，引入了更多的推理规则。可以用的规则越多，证明就变得越简单。</p>\n<p>而真值树系统就像加减法的竖式计算，学会使用它可以不依靠任何天赋和洞见就能完成证明。要证明的一些命题在树顶，要做的是从树顶向下分解，直到分成一个个命题符（原子句式），它们就是树的“根”。</p>\n<p>下面看看如何种下你的第一棵“真值树”：</p>\n<ol>\n<li><p>把你想证明的命题公式放到树顶</p>\n</li>\n<li><p>按照这些规则向下发展树根：</p>\n<p><img src=\"/images/tableaux.png\" alt=\"\"></p>\n</li>\n<li><p>把出现矛盾的分支关闭：</p>\n<p><img src=\"/images/closebranch.png\" alt=\"\"></p>\n</li>\n<li><p>若最后全部分支都封闭，则原命题为假，是<strong>矛盾句</strong>；若存在开放的分支，试着把原命题取反，再走一遍真值树，如果全部分支封闭，则原命题是<strong>恒真句</strong>；如果仍然存在开放的分支，则原命题是<strong>偶真句</strong>。对于第一种情况，称组成原命题的句式是<strong>不相容的</strong>；对后两种情况，称组成原命题的句式是<strong>相容的</strong>。学过线性代数的人会发现，线性方程组的有解、无解就对应着这个由方程组成的连言式相容或不相容的概念，事实上，也确实是这么叫的：相容的方程组至少有一个解，否则就称这个方程组是不相容的。</p>\n<p>对于蕴含证明来说，将结论取反与前提放在一起组成连言式作为树顶，若全部分支封闭，则论证为有效论证；反之为无效论证。（归谬法）</p>\n</li>\n</ol>\n<p><img src=\"/images/a1.png\" alt=\"\"></p>\n<p><img src=\"/images/d2.png\" alt=\"\"></p>\n<p><img src=\"/images/f3.png\" alt=\"\"></p>\n<p><img src=\"/images/g4.png\" alt=\"\"></p>\n<p><img src=\"/images/i5.png\" alt=\"\"></p>\n<p><img src=\"/images/i6.png\" alt=\"\"></p>\n<p>​                        <center><small>图片来自台湾通识网 傅皓政老师《逻辑》课讲义</small></center></p>\n<p><br/></p>\n<h2 id=\"命题逻辑的自然推理系统\"><a href=\"#命题逻辑的自然推理系统\" class=\"headerlink\" title=\"命题逻辑的自然推理系统\"></a>命题逻辑的自然推理系统</h2><hr>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S105/2\" target=\"_blank\" rel=\"noopener\">台大开放式课程 傅皓政《逻辑》</a></em></small></p>"},{"title":"对环保主义的态度","date":"2020-03-23T05:24:00.000Z","_content":"\n我在大学学的是大气科学。大三的暑假，学院开了一门教授专业英语的课，课上老师问了一个问题：你对气候变化的态度是什么，它是一个科学问题还是一个政治问题？<!--more-->当时我接触过一些推动环境保护主义的声音，既有科学界的，也有社会活动领域的；也接触到了一些持保守主义观点的人的看法，他们认为气候变化在科学领域还争议很大，而且从政治和经济的视角来看，环保主义也有很大问题。比如，限制人们使用一次性塑料袋，改用可循环利用的购物袋，如果去做效益分析的话，会发现限塑反而会导致更多的资源浪费。因为生产一个多次利用的塑料袋要投入更多的生产材料，以及生产过程更多的碳排放，而这需要人们循环使用很多次才能抵消，真正达到保护环境的目的，现实中很少有人能达到这个标准。\n\n假如有人问：你对上帝的态度是什么？这个问题就没那么难回答。一个虔诚的基督教徒会毫不犹疑地回答相信，而对于无神论者，他可能不那么愿意相信自己头顶上有个上帝存在。这是一个单纯的价值领域的问题，它关乎\"should be\"，而无关\"be\"。换句话说，全在于“信”与“不信”，假如一个人相信上帝，上帝就存在；假如不相信，上帝就不存在。你无法用科学的方法驳倒任何一方的观点——因为上帝是超验的。宗教和科学，价值和事实的二分，很早就在历史上完成了，密尔谈自然主义之物时谈论过，爱因斯坦讲宗教与科学的关系时也告诉过我们。这就是环境问题看起来这么让人头疼的原因，它掺杂着科学问题和道德问题，还有裹挟进来的政治博弈、经济战争。像许多对复杂系统的描述一样，科学尚力不能及，这就意味着价值领域的争论有更大的空间。倘若单单把自然保护主义当作价值判断，“自然”要么指的是一切的事物，要么指的是未经人类染指、干涉过的事物。从前者的观点看，人类做什么都是自然的，也就没有回归自然之说；从后者看，人类不管做什么都是不自然的，也就无法遵循自然。但不可否认的是，科学的确告诉我们保护物种多样性对人类自身的生存有益，空气污染会对健康造成威胁，全球气候变暖可能导致一些灾难性后果。这就允许人们基于对科学事实有充分了解的前提下，做出个人的价值判断。这当中有两个重要的假设：科学需要尽可能多的解释事实；个人对科学尽可能多的了解和充分考量。任何意识到这两个假设在实践中的困难的人，都会感到做出价值判断的头疼之处。当然，这些人中并不包括想要把一切推己及人的狂热宗教徒。\n\n","source":"_posts/环境保护主义.md","raw":"---\ntitle: 对环保主义的态度\ndate: 2020-03-23 13:24:00\ncategories:\n- 杂想\ntags: \n- 科学\n- 环保主义\n\n---\n\n我在大学学的是大气科学。大三的暑假，学院开了一门教授专业英语的课，课上老师问了一个问题：你对气候变化的态度是什么，它是一个科学问题还是一个政治问题？<!--more-->当时我接触过一些推动环境保护主义的声音，既有科学界的，也有社会活动领域的；也接触到了一些持保守主义观点的人的看法，他们认为气候变化在科学领域还争议很大，而且从政治和经济的视角来看，环保主义也有很大问题。比如，限制人们使用一次性塑料袋，改用可循环利用的购物袋，如果去做效益分析的话，会发现限塑反而会导致更多的资源浪费。因为生产一个多次利用的塑料袋要投入更多的生产材料，以及生产过程更多的碳排放，而这需要人们循环使用很多次才能抵消，真正达到保护环境的目的，现实中很少有人能达到这个标准。\n\n假如有人问：你对上帝的态度是什么？这个问题就没那么难回答。一个虔诚的基督教徒会毫不犹疑地回答相信，而对于无神论者，他可能不那么愿意相信自己头顶上有个上帝存在。这是一个单纯的价值领域的问题，它关乎\"should be\"，而无关\"be\"。换句话说，全在于“信”与“不信”，假如一个人相信上帝，上帝就存在；假如不相信，上帝就不存在。你无法用科学的方法驳倒任何一方的观点——因为上帝是超验的。宗教和科学，价值和事实的二分，很早就在历史上完成了，密尔谈自然主义之物时谈论过，爱因斯坦讲宗教与科学的关系时也告诉过我们。这就是环境问题看起来这么让人头疼的原因，它掺杂着科学问题和道德问题，还有裹挟进来的政治博弈、经济战争。像许多对复杂系统的描述一样，科学尚力不能及，这就意味着价值领域的争论有更大的空间。倘若单单把自然保护主义当作价值判断，“自然”要么指的是一切的事物，要么指的是未经人类染指、干涉过的事物。从前者的观点看，人类做什么都是自然的，也就没有回归自然之说；从后者看，人类不管做什么都是不自然的，也就无法遵循自然。但不可否认的是，科学的确告诉我们保护物种多样性对人类自身的生存有益，空气污染会对健康造成威胁，全球气候变暖可能导致一些灾难性后果。这就允许人们基于对科学事实有充分了解的前提下，做出个人的价值判断。这当中有两个重要的假设：科学需要尽可能多的解释事实；个人对科学尽可能多的了解和充分考量。任何意识到这两个假设在实践中的困难的人，都会感到做出价值判断的头疼之处。当然，这些人中并不包括想要把一切推己及人的狂热宗教徒。\n\n","slug":"环境保护主义","published":1,"updated":"2020-03-23T05:38:22.034Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd6z002i2sfy681c5xni","content":"<p>我在大学学的是大气科学。大三的暑假，学院开了一门教授专业英语的课，课上老师问了一个问题：你对气候变化的态度是什么，它是一个科学问题还是一个政治问题？<a id=\"more\"></a>当时我接触过一些推动环境保护主义的声音，既有科学界的，也有社会活动领域的；也接触到了一些持保守主义观点的人的看法，他们认为气候变化在科学领域还争议很大，而且从政治和经济的视角来看，环保主义也有很大问题。比如，限制人们使用一次性塑料袋，改用可循环利用的购物袋，如果去做效益分析的话，会发现限塑反而会导致更多的资源浪费。因为生产一个多次利用的塑料袋要投入更多的生产材料，以及生产过程更多的碳排放，而这需要人们循环使用很多次才能抵消，真正达到保护环境的目的，现实中很少有人能达到这个标准。</p>\n<p>假如有人问：你对上帝的态度是什么？这个问题就没那么难回答。一个虔诚的基督教徒会毫不犹疑地回答相信，而对于无神论者，他可能不那么愿意相信自己头顶上有个上帝存在。这是一个单纯的价值领域的问题，它关乎”should be”，而无关”be”。换句话说，全在于“信”与“不信”，假如一个人相信上帝，上帝就存在；假如不相信，上帝就不存在。你无法用科学的方法驳倒任何一方的观点——因为上帝是超验的。宗教和科学，价值和事实的二分，很早就在历史上完成了，密尔谈自然主义之物时谈论过，爱因斯坦讲宗教与科学的关系时也告诉过我们。这就是环境问题看起来这么让人头疼的原因，它掺杂着科学问题和道德问题，还有裹挟进来的政治博弈、经济战争。像许多对复杂系统的描述一样，科学尚力不能及，这就意味着价值领域的争论有更大的空间。倘若单单把自然保护主义当作价值判断，“自然”要么指的是一切的事物，要么指的是未经人类染指、干涉过的事物。从前者的观点看，人类做什么都是自然的，也就没有回归自然之说；从后者看，人类不管做什么都是不自然的，也就无法遵循自然。但不可否认的是，科学的确告诉我们保护物种多样性对人类自身的生存有益，空气污染会对健康造成威胁，全球气候变暖可能导致一些灾难性后果。这就允许人们基于对科学事实有充分了解的前提下，做出个人的价值判断。这当中有两个重要的假设：科学需要尽可能多的解释事实；个人对科学尽可能多的了解和充分考量。任何意识到这两个假设在实践中的困难的人，都会感到做出价值判断的头疼之处。当然，这些人中并不包括想要把一切推己及人的狂热宗教徒。</p>\n","site":{"data":{}},"excerpt":"<p>我在大学学的是大气科学。大三的暑假，学院开了一门教授专业英语的课，课上老师问了一个问题：你对气候变化的态度是什么，它是一个科学问题还是一个政治问题？","more":"当时我接触过一些推动环境保护主义的声音，既有科学界的，也有社会活动领域的；也接触到了一些持保守主义观点的人的看法，他们认为气候变化在科学领域还争议很大，而且从政治和经济的视角来看，环保主义也有很大问题。比如，限制人们使用一次性塑料袋，改用可循环利用的购物袋，如果去做效益分析的话，会发现限塑反而会导致更多的资源浪费。因为生产一个多次利用的塑料袋要投入更多的生产材料，以及生产过程更多的碳排放，而这需要人们循环使用很多次才能抵消，真正达到保护环境的目的，现实中很少有人能达到这个标准。</p>\n<p>假如有人问：你对上帝的态度是什么？这个问题就没那么难回答。一个虔诚的基督教徒会毫不犹疑地回答相信，而对于无神论者，他可能不那么愿意相信自己头顶上有个上帝存在。这是一个单纯的价值领域的问题，它关乎”should be”，而无关”be”。换句话说，全在于“信”与“不信”，假如一个人相信上帝，上帝就存在；假如不相信，上帝就不存在。你无法用科学的方法驳倒任何一方的观点——因为上帝是超验的。宗教和科学，价值和事实的二分，很早就在历史上完成了，密尔谈自然主义之物时谈论过，爱因斯坦讲宗教与科学的关系时也告诉过我们。这就是环境问题看起来这么让人头疼的原因，它掺杂着科学问题和道德问题，还有裹挟进来的政治博弈、经济战争。像许多对复杂系统的描述一样，科学尚力不能及，这就意味着价值领域的争论有更大的空间。倘若单单把自然保护主义当作价值判断，“自然”要么指的是一切的事物，要么指的是未经人类染指、干涉过的事物。从前者的观点看，人类做什么都是自然的，也就没有回归自然之说；从后者看，人类不管做什么都是不自然的，也就无法遵循自然。但不可否认的是，科学的确告诉我们保护物种多样性对人类自身的生存有益，空气污染会对健康造成威胁，全球气候变暖可能导致一些灾难性后果。这就允许人们基于对科学事实有充分了解的前提下，做出个人的价值判断。这当中有两个重要的假设：科学需要尽可能多的解释事实；个人对科学尽可能多的了解和充分考量。任何意识到这两个假设在实践中的困难的人，都会感到做出价值判断的头疼之处。当然，这些人中并不包括想要把一切推己及人的狂热宗教徒。</p>"},{"title":"计算的边界","date":"2020-02-03T18:49:00.000Z","mathjax":true,"_content":"\n集合论为人类认识无穷提供了公理化的理论基础，从集合论出发，我们可以得到很多有意思的结论。\n\n比如，**人类运用各种符号系统可以计算求解的问题，一定小于问题的总和**。 \n\n自计算机问世以来，人类技术进入加速发展的快车道，计算机理论和硬件的革新带来的是人类计算处理复杂问题能力的爆炸式增长。假如真如库兹维尔所说，人类技术是以指数的形式增长，那科学和技术发展的尽头是什么？会是所谓的“技术奇点”吗？人类的认识和计算存在极限吗？<!--more-->\n\n如果以上说法只是基于人们的经验和猜想，那有没有可能从纯粹的逻辑上证明计算的极限存在与否？\n\n集合论可以证明，**不可计算的问题一定存在**。\n\n证明之前，对“可计算的问题”做一个明确的定义：*如果一个问题可计算，则这个问题可用函数(function)表示并且可以用计算机程序算出函数的值。*\n\n下面要证明这个命题，需确认两点：\n\n1. **一切计算机程序，不管用何种语言编写，都是可列的。**\n2. **一个无穷可列集映射到它自身的函数有无穷个，且不可列。**\n\n第一点很容易理解，不管是何种计算机语言写成的程序，都可以看作是有限的字母表组合成的一串字符，且字符的个数一定是有限个（程序首先要能被写出来）。这和从有限整数集合{ 0,1,2,3,4,5,6,7,8,9 }挑选数字组成一个个自然数一样，这些自然数组成的自然数集是可列的，同理，计算机程序组成的集合也是可列的。\n\n要证明第二点，同样可以借助自然数集来说明。自然数集是一个可列集，考虑将自然数集映射到它自身的函数$f$，假设\n$$\nf(1)=d_{1},f(2)=d_{2},f(3)=d_{3},...f(n)=d_{n}...\n$$\n\n用$d_{1},d_{2},d_{3},...d_{n}...$组成一个$[0,1]$之间的无限小数\n\n$$\n0. d_{1}d_{2}d_{3}...d_{n}...\n$$\n\n这样，每个小数就对应一个不同的$f$。我们已经证明，实数集是不可列集，所以$f$组成的集合同样也是不可列集。同理可做推广，任一无穷可列集映射到它自身的函数都构成了一个不可列集。\n\n两个条件都已具备。第一，组成可计算问题的函数集是可列集；第二，证明了还存在不可列的函数集合，因此也就从理论上证明了不可计算问题的存在。\n\n---\n\n尽管借助集合论这样一个公理化的数学理论可以证明不可计算问题的存在，但最早帮助人们认识计算的极限的，却是一台小小的机器——图灵机。\n\n图灵机非常简单，可以把它看作一台最简单的计算机。现在电脑能做的计算，只要给予充分的时间和耐心，图灵机同样能够在有限步骤内实现。可以说，图灵机是所有现代计算机的亚当夏娃，那么，图灵当初为什么要造这样一台机器出来呢？\n\n时间回到1935年，阿兰·图灵刚刚结束他在剑桥国王学院4年的本科学习，进入了研究生院继续追寻他对数学的兴趣。此时，距离大数学家希尔伯特1900年世纪之初提出著名的23个问题，过去了35年。19世纪后期，数学界兴起了一场公理化运动，为的是去除基于直觉或经验的朴素概念所带来的模糊，进一步推动数学走向抽象、形式化，以求达到绝对的严密。到1900年巴黎的数学大会上，希尔伯特宣布：“借助集合论可以建造整个数学大厦……今天我们可以宣称绝对的严密化已经实现了！”但是，他不得不承认，在即将建成的这座大厦里，仍有一些让人不安的疑惑。其中就包括23问中的第二个问题：算术系统的相容性。算术系统的相容性问题后来又引申为三个基本问题：\n\n1. **数学系统是否完备。是否所有命题都可以被证明或证伪？**\n2. **数学系统是否一致。算术系统会否推出相互矛盾的命题？**\n3. **数学系统是否可判定。能否通过机械化的计算，判定命题的对错？**\n\n当一个数学系统同时具备完备性、一致性、可判定性时，之上建立起的数学大厦才是安全的。\n\n不幸的是，1931年，一个叫哥德尔的年轻人提出并证明了两条定理：\n\n1. **任何包含皮亚诺算术公理的数学系统，只要满足一致性，就必然存在不能被证明或证伪的命题。**\n2. **任何包含皮亚诺算术公理的数学系统，如果它是一致的，那么不能从体系内部证明该系统的一致性。**\n\n这就是著名的“哥德尔不完备性定理”，它击碎了希尔伯特雄心勃勃的设想，证明了根本不可能实现算术系统完备性和一致性的相容。这也就相当于告诉人们，在真理和证明之间，还存在着一条不可逾越的鸿沟，数学建立起的逻辑大厦并不能装下所有的真理。数学凭借自己的逻辑的力量，证明了数学的力量是有界限的。\n\n尽管“哥德尔不完备性定理”已经宣告算术系统相容性的失败，但它还留下了一个问题：数学系统可判定性与其它两个问题的关系。完备的数学系统是可判定的吗？\n\n想要解决这个问题，首先要搞清楚什么是“机械计算”。在当时，或许有人可以大致说出“机械计算”的含义，但没有人知道它到底如何实现，这是计算机诞生的史前时代——直到图灵机诞生。\n\n![](/images/120625074322660dd39deed2d7.gif)\n\n图灵说，机器能进行的计算就是“机械运算”，但他创造的图灵机绝对不止这么简单。图灵机强大的地方在于：存在一种**通用图灵机**，它可以模拟任何一台图灵机。这也就是说，通用图灵机可以成为任意一台图灵机执行它的计算过程，是所有机器的机器。那么，依靠图灵机，所有可计算问题均能被解决。那自然而然的，我们就要问，图灵机这么强大，存不存在图灵机无法计算的问题？换句话说，会不会有问题是图灵机永远也无法算出来的？这就是著名的图灵机“停机问题”。\n\n图灵机什么时候会停机？存不存在一个图灵机，可以判断图灵机是否会停机？假如存在，我们可以把数学中许多悬而未决的猜想，如哥德巴赫猜想、黎曼猜想等，扔给图灵机去计算，然后用这台特殊的图灵机判断执行计算的图灵机是否会停机，如此一来这些困难的数学猜想就都能通过停机问题解决。\n\n可惜的是，这样一台能解决停机问题的图灵机并不存在。\n\n图灵机是一个如此强大的系统，强大到存在图灵机可以模拟任意一台图灵机，但这埋藏着它的致命弱点：可模拟的对象包括它自身。\n\n![](/images/DrawingHands.jpg)\n\n​                            <small>危险的自我指涉，悖论往往由此产生。罗素悖论、哥德尔不完备定理、说谎者悖论……本质都是矛盾的自我指涉。</small>\n\n假设存在一台可以判断图灵机停机问题的图灵机 P。\n\n现有一台图灵机R，为其输入的编码为< M >，R需要调用图灵机P，判断图灵机M在输入编码< M >上会不会停机。假如M会停机，R将进入死循环；假如M不会停机，R将会停机。\n\n![](/images/flowchart.png)\n\n若令图灵机M为R自身，输入编码< R >，令R执行计算。假如P判断R在< R >上会停机，则R进入死循环，R不会停机；假如P判断R在< R >上不会停机，R将停机。这样就产生了悖论，说明这样一个图灵机P根本不存在。\n\n如此一来，就证明了存在图灵机不能在有限时间内求解的问题。1936年，图灵将发现写成论文《论可计算数，及其在可判定性问题上的应用》（On Computable Numbers, With an Application to the Entscheidungsproblem），证明了希尔伯特第三个问题可判定性也不可能与完备性相容，希尔伯特关于算术系统相容性的期望完全落空。\n\n哥德尔不完备性定理告诉我们，即使是一个严密公理化的数学系统，仍然可能存在不可证明的命题；\n\n图灵机“停机问题”则说明，即使所有命题可被证明或证伪，人类也无法依靠机械计算对所有命题做出判断。\n\n<br/>\n\n<br/>\n\n---\n\n### 信息论观点\n\n柏拉图主义：认为世界有确定的知识，这种知识是变化的事实背后永恒不变的本质，人要做的是追求、认识这些知识。\n\n> “The only true being is founded upon the forms, the eternal, unchangeable, perfect types, of which particular objects of sense are imperfect copies。”\n\n数学柏拉图主义：数学发现的是独立于思考的人、思想、事物的确定性知识。数学家做的是发现而不是发明。\n\n> Mathematical objects are independent of intelligent agents and their language, thought, and practices.\n\n信息论认为，所有信息中，有用的信息只占极少数（测度为0），虽然数学公理系统可能是不完备的，但数学是对有用知识的逼近。\n\n<br/>\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[方弦《计算的极限》](https://songshuhui.net/archives/75957)*</small>\n\n<small>[LLLBK-如何简单清晰地解释哥德尔不完备定理？](https://www.zhihu.com/question/27528796)</small>\n","source":"_posts/计算的极限.md","raw":"---\ntitle: 计算的边界\ndate: 2020-02-04 02:49:00\ncategories:\n- 数理逻辑\ntags: \n- 数学\n- 集合论\nmathjax: true\n---\n\n集合论为人类认识无穷提供了公理化的理论基础，从集合论出发，我们可以得到很多有意思的结论。\n\n比如，**人类运用各种符号系统可以计算求解的问题，一定小于问题的总和**。 \n\n自计算机问世以来，人类技术进入加速发展的快车道，计算机理论和硬件的革新带来的是人类计算处理复杂问题能力的爆炸式增长。假如真如库兹维尔所说，人类技术是以指数的形式增长，那科学和技术发展的尽头是什么？会是所谓的“技术奇点”吗？人类的认识和计算存在极限吗？<!--more-->\n\n如果以上说法只是基于人们的经验和猜想，那有没有可能从纯粹的逻辑上证明计算的极限存在与否？\n\n集合论可以证明，**不可计算的问题一定存在**。\n\n证明之前，对“可计算的问题”做一个明确的定义：*如果一个问题可计算，则这个问题可用函数(function)表示并且可以用计算机程序算出函数的值。*\n\n下面要证明这个命题，需确认两点：\n\n1. **一切计算机程序，不管用何种语言编写，都是可列的。**\n2. **一个无穷可列集映射到它自身的函数有无穷个，且不可列。**\n\n第一点很容易理解，不管是何种计算机语言写成的程序，都可以看作是有限的字母表组合成的一串字符，且字符的个数一定是有限个（程序首先要能被写出来）。这和从有限整数集合{ 0,1,2,3,4,5,6,7,8,9 }挑选数字组成一个个自然数一样，这些自然数组成的自然数集是可列的，同理，计算机程序组成的集合也是可列的。\n\n要证明第二点，同样可以借助自然数集来说明。自然数集是一个可列集，考虑将自然数集映射到它自身的函数$f$，假设\n$$\nf(1)=d_{1},f(2)=d_{2},f(3)=d_{3},...f(n)=d_{n}...\n$$\n\n用$d_{1},d_{2},d_{3},...d_{n}...$组成一个$[0,1]$之间的无限小数\n\n$$\n0. d_{1}d_{2}d_{3}...d_{n}...\n$$\n\n这样，每个小数就对应一个不同的$f$。我们已经证明，实数集是不可列集，所以$f$组成的集合同样也是不可列集。同理可做推广，任一无穷可列集映射到它自身的函数都构成了一个不可列集。\n\n两个条件都已具备。第一，组成可计算问题的函数集是可列集；第二，证明了还存在不可列的函数集合，因此也就从理论上证明了不可计算问题的存在。\n\n---\n\n尽管借助集合论这样一个公理化的数学理论可以证明不可计算问题的存在，但最早帮助人们认识计算的极限的，却是一台小小的机器——图灵机。\n\n图灵机非常简单，可以把它看作一台最简单的计算机。现在电脑能做的计算，只要给予充分的时间和耐心，图灵机同样能够在有限步骤内实现。可以说，图灵机是所有现代计算机的亚当夏娃，那么，图灵当初为什么要造这样一台机器出来呢？\n\n时间回到1935年，阿兰·图灵刚刚结束他在剑桥国王学院4年的本科学习，进入了研究生院继续追寻他对数学的兴趣。此时，距离大数学家希尔伯特1900年世纪之初提出著名的23个问题，过去了35年。19世纪后期，数学界兴起了一场公理化运动，为的是去除基于直觉或经验的朴素概念所带来的模糊，进一步推动数学走向抽象、形式化，以求达到绝对的严密。到1900年巴黎的数学大会上，希尔伯特宣布：“借助集合论可以建造整个数学大厦……今天我们可以宣称绝对的严密化已经实现了！”但是，他不得不承认，在即将建成的这座大厦里，仍有一些让人不安的疑惑。其中就包括23问中的第二个问题：算术系统的相容性。算术系统的相容性问题后来又引申为三个基本问题：\n\n1. **数学系统是否完备。是否所有命题都可以被证明或证伪？**\n2. **数学系统是否一致。算术系统会否推出相互矛盾的命题？**\n3. **数学系统是否可判定。能否通过机械化的计算，判定命题的对错？**\n\n当一个数学系统同时具备完备性、一致性、可判定性时，之上建立起的数学大厦才是安全的。\n\n不幸的是，1931年，一个叫哥德尔的年轻人提出并证明了两条定理：\n\n1. **任何包含皮亚诺算术公理的数学系统，只要满足一致性，就必然存在不能被证明或证伪的命题。**\n2. **任何包含皮亚诺算术公理的数学系统，如果它是一致的，那么不能从体系内部证明该系统的一致性。**\n\n这就是著名的“哥德尔不完备性定理”，它击碎了希尔伯特雄心勃勃的设想，证明了根本不可能实现算术系统完备性和一致性的相容。这也就相当于告诉人们，在真理和证明之间，还存在着一条不可逾越的鸿沟，数学建立起的逻辑大厦并不能装下所有的真理。数学凭借自己的逻辑的力量，证明了数学的力量是有界限的。\n\n尽管“哥德尔不完备性定理”已经宣告算术系统相容性的失败，但它还留下了一个问题：数学系统可判定性与其它两个问题的关系。完备的数学系统是可判定的吗？\n\n想要解决这个问题，首先要搞清楚什么是“机械计算”。在当时，或许有人可以大致说出“机械计算”的含义，但没有人知道它到底如何实现，这是计算机诞生的史前时代——直到图灵机诞生。\n\n![](/images/120625074322660dd39deed2d7.gif)\n\n图灵说，机器能进行的计算就是“机械运算”，但他创造的图灵机绝对不止这么简单。图灵机强大的地方在于：存在一种**通用图灵机**，它可以模拟任何一台图灵机。这也就是说，通用图灵机可以成为任意一台图灵机执行它的计算过程，是所有机器的机器。那么，依靠图灵机，所有可计算问题均能被解决。那自然而然的，我们就要问，图灵机这么强大，存不存在图灵机无法计算的问题？换句话说，会不会有问题是图灵机永远也无法算出来的？这就是著名的图灵机“停机问题”。\n\n图灵机什么时候会停机？存不存在一个图灵机，可以判断图灵机是否会停机？假如存在，我们可以把数学中许多悬而未决的猜想，如哥德巴赫猜想、黎曼猜想等，扔给图灵机去计算，然后用这台特殊的图灵机判断执行计算的图灵机是否会停机，如此一来这些困难的数学猜想就都能通过停机问题解决。\n\n可惜的是，这样一台能解决停机问题的图灵机并不存在。\n\n图灵机是一个如此强大的系统，强大到存在图灵机可以模拟任意一台图灵机，但这埋藏着它的致命弱点：可模拟的对象包括它自身。\n\n![](/images/DrawingHands.jpg)\n\n​                            <small>危险的自我指涉，悖论往往由此产生。罗素悖论、哥德尔不完备定理、说谎者悖论……本质都是矛盾的自我指涉。</small>\n\n假设存在一台可以判断图灵机停机问题的图灵机 P。\n\n现有一台图灵机R，为其输入的编码为< M >，R需要调用图灵机P，判断图灵机M在输入编码< M >上会不会停机。假如M会停机，R将进入死循环；假如M不会停机，R将会停机。\n\n![](/images/flowchart.png)\n\n若令图灵机M为R自身，输入编码< R >，令R执行计算。假如P判断R在< R >上会停机，则R进入死循环，R不会停机；假如P判断R在< R >上不会停机，R将停机。这样就产生了悖论，说明这样一个图灵机P根本不存在。\n\n如此一来，就证明了存在图灵机不能在有限时间内求解的问题。1936年，图灵将发现写成论文《论可计算数，及其在可判定性问题上的应用》（On Computable Numbers, With an Application to the Entscheidungsproblem），证明了希尔伯特第三个问题可判定性也不可能与完备性相容，希尔伯特关于算术系统相容性的期望完全落空。\n\n哥德尔不完备性定理告诉我们，即使是一个严密公理化的数学系统，仍然可能存在不可证明的命题；\n\n图灵机“停机问题”则说明，即使所有命题可被证明或证伪，人类也无法依靠机械计算对所有命题做出判断。\n\n<br/>\n\n<br/>\n\n---\n\n### 信息论观点\n\n柏拉图主义：认为世界有确定的知识，这种知识是变化的事实背后永恒不变的本质，人要做的是追求、认识这些知识。\n\n> “The only true being is founded upon the forms, the eternal, unchangeable, perfect types, of which particular objects of sense are imperfect copies。”\n\n数学柏拉图主义：数学发现的是独立于思考的人、思想、事物的确定性知识。数学家做的是发现而不是发明。\n\n> Mathematical objects are independent of intelligent agents and their language, thought, and practices.\n\n信息论认为，所有信息中，有用的信息只占极少数（测度为0），虽然数学公理系统可能是不完备的，但数学是对有用知识的逼近。\n\n<br/>\n\n<br/><br/><br/>\n\n<small>*参考*</small>\n\n<small>*[方弦《计算的极限》](https://songshuhui.net/archives/75957)*</small>\n\n<small>[LLLBK-如何简单清晰地解释哥德尔不完备定理？](https://www.zhihu.com/question/27528796)</small>\n","slug":"计算的极限","published":1,"updated":"2022-01-21T12:31:26.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd71002l2sfycrnganxg","content":"<p>集合论为人类认识无穷提供了公理化的理论基础，从集合论出发，我们可以得到很多有意思的结论。</p>\n<p>比如，<strong>人类运用各种符号系统可以计算求解的问题，一定小于问题的总和</strong>。 </p>\n<p>自计算机问世以来，人类技术进入加速发展的快车道，计算机理论和硬件的革新带来的是人类计算处理复杂问题能力的爆炸式增长。假如真如库兹维尔所说，人类技术是以指数的形式增长，那科学和技术发展的尽头是什么？会是所谓的“技术奇点”吗？人类的认识和计算存在极限吗？<a id=\"more\"></a></p>\n<p>如果以上说法只是基于人们的经验和猜想，那有没有可能从纯粹的逻辑上证明计算的极限存在与否？</p>\n<p>集合论可以证明，<strong>不可计算的问题一定存在</strong>。</p>\n<p>证明之前，对“可计算的问题”做一个明确的定义：<em>如果一个问题可计算，则这个问题可用函数(function)表示并且可以用计算机程序算出函数的值。</em></p>\n<p>下面要证明这个命题，需确认两点：</p>\n<ol>\n<li><strong>一切计算机程序，不管用何种语言编写，都是可列的。</strong></li>\n<li><strong>一个无穷可列集映射到它自身的函数有无穷个，且不可列。</strong></li>\n</ol>\n<p>第一点很容易理解，不管是何种计算机语言写成的程序，都可以看作是有限的字母表组合成的一串字符，且字符的个数一定是有限个（程序首先要能被写出来）。这和从有限整数集合{ 0,1,2,3,4,5,6,7,8,9 }挑选数字组成一个个自然数一样，这些自然数组成的自然数集是可列的，同理，计算机程序组成的集合也是可列的。</p>\n<p>要证明第二点，同样可以借助自然数集来说明。自然数集是一个可列集，考虑将自然数集映射到它自身的函数$f$，假设</p>\n<script type=\"math/tex; mode=display\">\nf(1)=d_{1},f(2)=d_{2},f(3)=d_{3},...f(n)=d_{n}...</script><p>用$d_{1},d_{2},d_{3},…d_{n}…$组成一个$[0,1]$之间的无限小数</p>\n<script type=\"math/tex; mode=display\">\n0. d_{1}d_{2}d_{3}...d_{n}...</script><p>这样，每个小数就对应一个不同的$f$。我们已经证明，实数集是不可列集，所以$f$组成的集合同样也是不可列集。同理可做推广，任一无穷可列集映射到它自身的函数都构成了一个不可列集。</p>\n<p>两个条件都已具备。第一，组成可计算问题的函数集是可列集；第二，证明了还存在不可列的函数集合，因此也就从理论上证明了不可计算问题的存在。</p>\n<hr>\n<p>尽管借助集合论这样一个公理化的数学理论可以证明不可计算问题的存在，但最早帮助人们认识计算的极限的，却是一台小小的机器——图灵机。</p>\n<p>图灵机非常简单，可以把它看作一台最简单的计算机。现在电脑能做的计算，只要给予充分的时间和耐心，图灵机同样能够在有限步骤内实现。可以说，图灵机是所有现代计算机的亚当夏娃，那么，图灵当初为什么要造这样一台机器出来呢？</p>\n<p>时间回到1935年，阿兰·图灵刚刚结束他在剑桥国王学院4年的本科学习，进入了研究生院继续追寻他对数学的兴趣。此时，距离大数学家希尔伯特1900年世纪之初提出著名的23个问题，过去了35年。19世纪后期，数学界兴起了一场公理化运动，为的是去除基于直觉或经验的朴素概念所带来的模糊，进一步推动数学走向抽象、形式化，以求达到绝对的严密。到1900年巴黎的数学大会上，希尔伯特宣布：“借助集合论可以建造整个数学大厦……今天我们可以宣称绝对的严密化已经实现了！”但是，他不得不承认，在即将建成的这座大厦里，仍有一些让人不安的疑惑。其中就包括23问中的第二个问题：算术系统的相容性。算术系统的相容性问题后来又引申为三个基本问题：</p>\n<ol>\n<li><strong>数学系统是否完备。是否所有命题都可以被证明或证伪？</strong></li>\n<li><strong>数学系统是否一致。算术系统会否推出相互矛盾的命题？</strong></li>\n<li><strong>数学系统是否可判定。能否通过机械化的计算，判定命题的对错？</strong></li>\n</ol>\n<p>当一个数学系统同时具备完备性、一致性、可判定性时，之上建立起的数学大厦才是安全的。</p>\n<p>不幸的是，1931年，一个叫哥德尔的年轻人提出并证明了两条定理：</p>\n<ol>\n<li><strong>任何包含皮亚诺算术公理的数学系统，只要满足一致性，就必然存在不能被证明或证伪的命题。</strong></li>\n<li><strong>任何包含皮亚诺算术公理的数学系统，如果它是一致的，那么不能从体系内部证明该系统的一致性。</strong></li>\n</ol>\n<p>这就是著名的“哥德尔不完备性定理”，它击碎了希尔伯特雄心勃勃的设想，证明了根本不可能实现算术系统完备性和一致性的相容。这也就相当于告诉人们，在真理和证明之间，还存在着一条不可逾越的鸿沟，数学建立起的逻辑大厦并不能装下所有的真理。数学凭借自己的逻辑的力量，证明了数学的力量是有界限的。</p>\n<p>尽管“哥德尔不完备性定理”已经宣告算术系统相容性的失败，但它还留下了一个问题：数学系统可判定性与其它两个问题的关系。完备的数学系统是可判定的吗？</p>\n<p>想要解决这个问题，首先要搞清楚什么是“机械计算”。在当时，或许有人可以大致说出“机械计算”的含义，但没有人知道它到底如何实现，这是计算机诞生的史前时代——直到图灵机诞生。</p>\n<p><img src=\"/images/120625074322660dd39deed2d7.gif\" alt=\"\"></p>\n<p>图灵说，机器能进行的计算就是“机械运算”，但他创造的图灵机绝对不止这么简单。图灵机强大的地方在于：存在一种<strong>通用图灵机</strong>，它可以模拟任何一台图灵机。这也就是说，通用图灵机可以成为任意一台图灵机执行它的计算过程，是所有机器的机器。那么，依靠图灵机，所有可计算问题均能被解决。那自然而然的，我们就要问，图灵机这么强大，存不存在图灵机无法计算的问题？换句话说，会不会有问题是图灵机永远也无法算出来的？这就是著名的图灵机“停机问题”。</p>\n<p>图灵机什么时候会停机？存不存在一个图灵机，可以判断图灵机是否会停机？假如存在，我们可以把数学中许多悬而未决的猜想，如哥德巴赫猜想、黎曼猜想等，扔给图灵机去计算，然后用这台特殊的图灵机判断执行计算的图灵机是否会停机，如此一来这些困难的数学猜想就都能通过停机问题解决。</p>\n<p>可惜的是，这样一台能解决停机问题的图灵机并不存在。</p>\n<p>图灵机是一个如此强大的系统，强大到存在图灵机可以模拟任意一台图灵机，但这埋藏着它的致命弱点：可模拟的对象包括它自身。</p>\n<p><img src=\"/images/DrawingHands.jpg\" alt=\"\"></p>\n<p>​                            <small>危险的自我指涉，悖论往往由此产生。罗素悖论、哥德尔不完备定理、说谎者悖论……本质都是矛盾的自我指涉。</small></p>\n<p>假设存在一台可以判断图灵机停机问题的图灵机 P。</p>\n<p>现有一台图灵机R，为其输入的编码为&lt; M &gt;，R需要调用图灵机P，判断图灵机M在输入编码&lt; M &gt;上会不会停机。假如M会停机，R将进入死循环；假如M不会停机，R将会停机。</p>\n<p><img src=\"/images/flowchart.png\" alt=\"\"></p>\n<p>若令图灵机M为R自身，输入编码&lt; R &gt;，令R执行计算。假如P判断R在&lt; R &gt;上会停机，则R进入死循环，R不会停机；假如P判断R在&lt; R &gt;上不会停机，R将停机。这样就产生了悖论，说明这样一个图灵机P根本不存在。</p>\n<p>如此一来，就证明了存在图灵机不能在有限时间内求解的问题。1936年，图灵将发现写成论文《论可计算数，及其在可判定性问题上的应用》（On Computable Numbers, With an Application to the Entscheidungsproblem），证明了希尔伯特第三个问题可判定性也不可能与完备性相容，希尔伯特关于算术系统相容性的期望完全落空。</p>\n<p>哥德尔不完备性定理告诉我们，即使是一个严密公理化的数学系统，仍然可能存在不可证明的命题；</p>\n<p>图灵机“停机问题”则说明，即使所有命题可被证明或证伪，人类也无法依靠机械计算对所有命题做出判断。</p>\n<p><br/></p>\n<p><br/></p>\n<hr>\n<h3 id=\"信息论观点\"><a href=\"#信息论观点\" class=\"headerlink\" title=\"信息论观点\"></a>信息论观点</h3><p>柏拉图主义：认为世界有确定的知识，这种知识是变化的事实背后永恒不变的本质，人要做的是追求、认识这些知识。</p>\n<blockquote>\n<p>“The only true being is founded upon the forms, the eternal, unchangeable, perfect types, of which particular objects of sense are imperfect copies。”</p>\n</blockquote>\n<p>数学柏拉图主义：数学发现的是独立于思考的人、思想、事物的确定性知识。数学家做的是发现而不是发明。</p>\n<blockquote>\n<p>Mathematical objects are independent of intelligent agents and their language, thought, and practices.</p>\n</blockquote>\n<p>信息论认为，所有信息中，有用的信息只占极少数（测度为0），虽然数学公理系统可能是不完备的，但数学是对有用知识的逼近。</p>\n<p><br/></p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://songshuhui.net/archives/75957\" target=\"_blank\" rel=\"noopener\">方弦《计算的极限》</a></em></small></p>\n<p><small><a href=\"https://www.zhihu.com/question/27528796\" target=\"_blank\" rel=\"noopener\">LLLBK-如何简单清晰地解释哥德尔不完备定理？</a></small></p>\n","site":{"data":{}},"excerpt":"<p>集合论为人类认识无穷提供了公理化的理论基础，从集合论出发，我们可以得到很多有意思的结论。</p>\n<p>比如，<strong>人类运用各种符号系统可以计算求解的问题，一定小于问题的总和</strong>。 </p>\n<p>自计算机问世以来，人类技术进入加速发展的快车道，计算机理论和硬件的革新带来的是人类计算处理复杂问题能力的爆炸式增长。假如真如库兹维尔所说，人类技术是以指数的形式增长，那科学和技术发展的尽头是什么？会是所谓的“技术奇点”吗？人类的认识和计算存在极限吗？","more":"</p>\n<p>如果以上说法只是基于人们的经验和猜想，那有没有可能从纯粹的逻辑上证明计算的极限存在与否？</p>\n<p>集合论可以证明，<strong>不可计算的问题一定存在</strong>。</p>\n<p>证明之前，对“可计算的问题”做一个明确的定义：<em>如果一个问题可计算，则这个问题可用函数(function)表示并且可以用计算机程序算出函数的值。</em></p>\n<p>下面要证明这个命题，需确认两点：</p>\n<ol>\n<li><strong>一切计算机程序，不管用何种语言编写，都是可列的。</strong></li>\n<li><strong>一个无穷可列集映射到它自身的函数有无穷个，且不可列。</strong></li>\n</ol>\n<p>第一点很容易理解，不管是何种计算机语言写成的程序，都可以看作是有限的字母表组合成的一串字符，且字符的个数一定是有限个（程序首先要能被写出来）。这和从有限整数集合{ 0,1,2,3,4,5,6,7,8,9 }挑选数字组成一个个自然数一样，这些自然数组成的自然数集是可列的，同理，计算机程序组成的集合也是可列的。</p>\n<p>要证明第二点，同样可以借助自然数集来说明。自然数集是一个可列集，考虑将自然数集映射到它自身的函数$f$，假设</p>\n<script type=\"math/tex; mode=display\">\nf(1)=d_{1},f(2)=d_{2},f(3)=d_{3},...f(n)=d_{n}...</script><p>用$d_{1},d_{2},d_{3},…d_{n}…$组成一个$[0,1]$之间的无限小数</p>\n<script type=\"math/tex; mode=display\">\n0. d_{1}d_{2}d_{3}...d_{n}...</script><p>这样，每个小数就对应一个不同的$f$。我们已经证明，实数集是不可列集，所以$f$组成的集合同样也是不可列集。同理可做推广，任一无穷可列集映射到它自身的函数都构成了一个不可列集。</p>\n<p>两个条件都已具备。第一，组成可计算问题的函数集是可列集；第二，证明了还存在不可列的函数集合，因此也就从理论上证明了不可计算问题的存在。</p>\n<hr>\n<p>尽管借助集合论这样一个公理化的数学理论可以证明不可计算问题的存在，但最早帮助人们认识计算的极限的，却是一台小小的机器——图灵机。</p>\n<p>图灵机非常简单，可以把它看作一台最简单的计算机。现在电脑能做的计算，只要给予充分的时间和耐心，图灵机同样能够在有限步骤内实现。可以说，图灵机是所有现代计算机的亚当夏娃，那么，图灵当初为什么要造这样一台机器出来呢？</p>\n<p>时间回到1935年，阿兰·图灵刚刚结束他在剑桥国王学院4年的本科学习，进入了研究生院继续追寻他对数学的兴趣。此时，距离大数学家希尔伯特1900年世纪之初提出著名的23个问题，过去了35年。19世纪后期，数学界兴起了一场公理化运动，为的是去除基于直觉或经验的朴素概念所带来的模糊，进一步推动数学走向抽象、形式化，以求达到绝对的严密。到1900年巴黎的数学大会上，希尔伯特宣布：“借助集合论可以建造整个数学大厦……今天我们可以宣称绝对的严密化已经实现了！”但是，他不得不承认，在即将建成的这座大厦里，仍有一些让人不安的疑惑。其中就包括23问中的第二个问题：算术系统的相容性。算术系统的相容性问题后来又引申为三个基本问题：</p>\n<ol>\n<li><strong>数学系统是否完备。是否所有命题都可以被证明或证伪？</strong></li>\n<li><strong>数学系统是否一致。算术系统会否推出相互矛盾的命题？</strong></li>\n<li><strong>数学系统是否可判定。能否通过机械化的计算，判定命题的对错？</strong></li>\n</ol>\n<p>当一个数学系统同时具备完备性、一致性、可判定性时，之上建立起的数学大厦才是安全的。</p>\n<p>不幸的是，1931年，一个叫哥德尔的年轻人提出并证明了两条定理：</p>\n<ol>\n<li><strong>任何包含皮亚诺算术公理的数学系统，只要满足一致性，就必然存在不能被证明或证伪的命题。</strong></li>\n<li><strong>任何包含皮亚诺算术公理的数学系统，如果它是一致的，那么不能从体系内部证明该系统的一致性。</strong></li>\n</ol>\n<p>这就是著名的“哥德尔不完备性定理”，它击碎了希尔伯特雄心勃勃的设想，证明了根本不可能实现算术系统完备性和一致性的相容。这也就相当于告诉人们，在真理和证明之间，还存在着一条不可逾越的鸿沟，数学建立起的逻辑大厦并不能装下所有的真理。数学凭借自己的逻辑的力量，证明了数学的力量是有界限的。</p>\n<p>尽管“哥德尔不完备性定理”已经宣告算术系统相容性的失败，但它还留下了一个问题：数学系统可判定性与其它两个问题的关系。完备的数学系统是可判定的吗？</p>\n<p>想要解决这个问题，首先要搞清楚什么是“机械计算”。在当时，或许有人可以大致说出“机械计算”的含义，但没有人知道它到底如何实现，这是计算机诞生的史前时代——直到图灵机诞生。</p>\n<p><img src=\"/images/120625074322660dd39deed2d7.gif\" alt=\"\"></p>\n<p>图灵说，机器能进行的计算就是“机械运算”，但他创造的图灵机绝对不止这么简单。图灵机强大的地方在于：存在一种<strong>通用图灵机</strong>，它可以模拟任何一台图灵机。这也就是说，通用图灵机可以成为任意一台图灵机执行它的计算过程，是所有机器的机器。那么，依靠图灵机，所有可计算问题均能被解决。那自然而然的，我们就要问，图灵机这么强大，存不存在图灵机无法计算的问题？换句话说，会不会有问题是图灵机永远也无法算出来的？这就是著名的图灵机“停机问题”。</p>\n<p>图灵机什么时候会停机？存不存在一个图灵机，可以判断图灵机是否会停机？假如存在，我们可以把数学中许多悬而未决的猜想，如哥德巴赫猜想、黎曼猜想等，扔给图灵机去计算，然后用这台特殊的图灵机判断执行计算的图灵机是否会停机，如此一来这些困难的数学猜想就都能通过停机问题解决。</p>\n<p>可惜的是，这样一台能解决停机问题的图灵机并不存在。</p>\n<p>图灵机是一个如此强大的系统，强大到存在图灵机可以模拟任意一台图灵机，但这埋藏着它的致命弱点：可模拟的对象包括它自身。</p>\n<p><img src=\"/images/DrawingHands.jpg\" alt=\"\"></p>\n<p>​                            <small>危险的自我指涉，悖论往往由此产生。罗素悖论、哥德尔不完备定理、说谎者悖论……本质都是矛盾的自我指涉。</small></p>\n<p>假设存在一台可以判断图灵机停机问题的图灵机 P。</p>\n<p>现有一台图灵机R，为其输入的编码为&lt; M &gt;，R需要调用图灵机P，判断图灵机M在输入编码&lt; M &gt;上会不会停机。假如M会停机，R将进入死循环；假如M不会停机，R将会停机。</p>\n<p><img src=\"/images/flowchart.png\" alt=\"\"></p>\n<p>若令图灵机M为R自身，输入编码&lt; R &gt;，令R执行计算。假如P判断R在&lt; R &gt;上会停机，则R进入死循环，R不会停机；假如P判断R在&lt; R &gt;上不会停机，R将停机。这样就产生了悖论，说明这样一个图灵机P根本不存在。</p>\n<p>如此一来，就证明了存在图灵机不能在有限时间内求解的问题。1936年，图灵将发现写成论文《论可计算数，及其在可判定性问题上的应用》（On Computable Numbers, With an Application to the Entscheidungsproblem），证明了希尔伯特第三个问题可判定性也不可能与完备性相容，希尔伯特关于算术系统相容性的期望完全落空。</p>\n<p>哥德尔不完备性定理告诉我们，即使是一个严密公理化的数学系统，仍然可能存在不可证明的命题；</p>\n<p>图灵机“停机问题”则说明，即使所有命题可被证明或证伪，人类也无法依靠机械计算对所有命题做出判断。</p>\n<p><br/></p>\n<p><br/></p>\n<hr>\n<h3 id=\"信息论观点\"><a href=\"#信息论观点\" class=\"headerlink\" title=\"信息论观点\"></a>信息论观点</h3><p>柏拉图主义：认为世界有确定的知识，这种知识是变化的事实背后永恒不变的本质，人要做的是追求、认识这些知识。</p>\n<blockquote>\n<p>“The only true being is founded upon the forms, the eternal, unchangeable, perfect types, of which particular objects of sense are imperfect copies。”</p>\n</blockquote>\n<p>数学柏拉图主义：数学发现的是独立于思考的人、思想、事物的确定性知识。数学家做的是发现而不是发明。</p>\n<blockquote>\n<p>Mathematical objects are independent of intelligent agents and their language, thought, and practices.</p>\n</blockquote>\n<p>信息论认为，所有信息中，有用的信息只占极少数（测度为0），虽然数学公理系统可能是不完备的，但数学是对有用知识的逼近。</p>\n<p><br/></p>\n<p><br/><br/><br/></p>\n<p><small><em>参考</em></small></p>\n<p><small><em><a href=\"https://songshuhui.net/archives/75957\" target=\"_blank\" rel=\"noopener\">方弦《计算的极限》</a></em></small></p>\n<p><small><a href=\"https://www.zhihu.com/question/27528796\" target=\"_blank\" rel=\"noopener\">LLLBK-如何简单清晰地解释哥德尔不完备定理？</a></small></p>"},{"title":"集合论与无限","date":"2020-02-02T15:04:00.000Z","mathjax":true,"_content":"\n> \"The infinite, like no other problem, has always deeply moved the soul of men. The infinite, like no other idea, has had a stimulating and fertile influence upon the mind. But the infinite is also more than any other concept, in need of clarification.\"\n>\n> ​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——David Hilbert\n\n<!--more-->\n\n###  一、自然数的定义\n\n一位德国数学家克罗内克说过：“上帝创造了整数；其余皆出凡人之手”。克罗内克是直觉派的坚定支持者，他认为只有整数符合人类的经验和直觉，除此之外都是人类的想象。\n\n自然数是如何产生的？如今，12个刻度的时钟、银行账户的存款数目、电话号码、身份证号，这些数字根深蒂固地存在于每一个现代人脑中。但这些构成我们现代生活的数字并非与生俱来。从智人出现算起漫长的20多万年时间里，我们这个物种很长一段时间并没有发展出精确表示数量的方法。事实上，直到现在，非洲亚马逊河流域还存在一些“无数字”的原始部落，他们以狩猎-采集为生，只用“少量”、“一些”这些词语表示数量。当要求他们准确辨别一些数量，比如“罐子里有五个坚果，拿走三个之后，罐子里还有几个”，“树上总共有多少个椰子”这些问题，对他们来说仍然异常艰难。即使在现代社会，“无数字”的生活也离我们并不遥远。仅仅回到50年前，中国农村仍然可以见到不会做十以上加减法的老人，在封建历史时期，基础的算术教育并没有像如今一样普及。刚出生的婴儿在接触数学前，只能大致分辨三以上的数量。这也就是说，最初的计数和运算仅仅在人们需要时才出现，与经验和生存的需求密不可分，当没有数字也可以生活得不错时，也就无需扩展对数字的认知——除非纯粹的想象和思辨出现。（关于直观和形式逻辑的纷争水很深，涉及到三大数学哲学流派：逻辑主义、形式主义、直觉主义，代表人物分别是罗素、希尔伯特、布劳威尔。《什么是数学》的作者柯朗写书时正值公理演绎盛行时期，他反对过分强调公理演绎。爱因斯坦曾经说过，“西方科学的发展是以两个伟大的成就为基础的：希腊哲学家发明的形式逻辑体系（在欧几里得几何学中），以及（在文艺复兴时期）发现通过系统的实验可能找出因果关系。”对于科学来讲，古希腊的逻辑演绎精神和文艺复兴时期产生的实验精神对科学同等重要。）\n\n假如非要说上帝创造了整数，那也只能从这个角度理解：上帝赋予人类十根手指，并从地面解放了它们。人类从认识自己的手指开始，头脑中闪过的关于数字的想法被语言记录下来，一代代流传下去。有一天，当人们意识到可以不依赖手指、石头、绳结去分辨羊群里有几头羊——事实上，可以不依赖任何实物——数字才被抽象了出来。\n\n克罗内克反对康托尔的集合论，他拒绝无理数的概念，认为整数具有“自然”的优越地位，也就把自己挡在了更加广阔的数学世界门外。\n\n#### Cantor :从集合构造自然数\n\n设x为集合，定义x的后继(successor)$x^+为x\\cup \\{x\\}$,则自然数可构造为：\n$$\n0=\\emptyset,1=0^+,2=0^{++}...n=0^{+...+(n个+)}\n$$\n定义归纳集（inductive set）A：\n$$\n\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\n$$\n有了归纳集的概念，引入一条**无穷公理**（Axiom of Infinity, ZFC.7）说明自然数集$\\mathbb{N}$的存在：\n$$\n\\exists A(\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A))\n$$\n\n于是，自然数和自然数集就在集合论中构造出来了。这里要注意，无穷并不是构造出来的，而是通过公理引入的。\n\n#### Peano自然数算术公理系统\n\n1. $0\\in\\mathbb{N}$\n\n2. $n\\in\\mathbb{N}\\to n^+\\in\\mathbb{N}$\n\n3. $n^+=m^+\\to n= m$\n\n4. $0\\not=n^+$\n\n5. $\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\\to (\\forall x\\in \\mathbb{N})(x\\in A)$\n\n\n\n\n### 二、集合的基数\n\n#### 集合的基数与等势关系\n\n- 集合的基数：集合中元素的个数称为集合A的**基数（cardinals）**，又称集合A的势（cardinality），记为cardA或$\\mid A \\mid$。\n\n- 集合等势：A**等势（equipotence)**于B指存在A与B的双射函数（1-1,onto），记为$A\\approx B$。\n\n#### 有限集和无穷集\n\n- 有限集：若$\\exists n\\in \\mathbb{N}$，使得$A\\approx n$，则A是有限集。\n\n- 无穷集：非有限集为无穷集。\n\n#### “无穷的大小”：可列集、不可列集、集合优势关系\n\n- 可列集：与自然数集N等势的集合，$A \\approx \\mathbb{N}$，A的势$|A|=\\aleph _0$。\n\n<small>（可以按某种确定的规则与自然数集一一对应（1-1，onto）。意味着可列集可以按确定的顺序线性排列，并用自然数数出。所谓“确定的”顺序是指对序列中任一元素，可以说出它“前一个”、“后一个”元素是什么。）</small>\n\n- 不可列集：不是可列集的集合。\n\n> **一个经典的例子——「证明实数集是不可列集(Cantor's-diagonalization-argument)」**\n>\n> 由于$R\\approx[0,1]$，故只需证明$[0,1]$之间的实数点集不可列\n>\n> 假设$[0,1]$之间的实数点集可列，则$[0,1]$上的值可列举为：\n> $$\n> 0.b_{11}b_{12}b_{13}b_{14}...\\\\\n> 0.b_{21}b_{22}b_{23}b_{24}...\\\\\n> 0.b_{31}b_{32}b_{33}b_{34}...\\\\\n> 0.b_{41}b_{42}b_{43}b_{44}...\\\\\n> ...\n> $$\n> 取实数$y\\in[0,1]$，表示为$0.b_{1}b_{2}b_{3}...$，并令$b_{i}\\not=b_{ii}(i=1,2,3,...)$。易见，$y$与上表中任一值均不相等，假设错误，实数集不可列。\n\n可能有人会问，y不在列出的数里，把y加进去不就可以了？这样还是可以列举出来。\n\n但是，加进y之后，我们还是可以用相同的方法，找到一个新的y，令它每一位与列表里对应的对角线数字均不相等。这个过程可以**无穷地进行下去**，而且永远也无法确定下一个y是什么。显然，永远不可能把$[0,1]$间的实数列举完。\n\n可能有人还会问，把“$[0,1]$间实数”换成“自然数”，按照Cantor的方法，不是也可以找到无穷无尽符合条件的y吗？这样岂不证明自然数集也是不可列的，也即自然数集的势不等于它自身吗？这显然是矛盾的。\n\n错，这样的y并不存在。\n\n问题的关键在于，**实数小数点后的位数可以是无穷的**，而虽然自然数集是无穷集，但**一个自然数必然有确定的位数**，不存在一个无穷位数的自然数。当试图找到一个$y=b_{1}b_{2}b_{3}...b_{n}$，令$b_{i}\\not=b_{ii}(i=1,2,3,...n)$时，因为n是一个确定的自然数，n位数的自然数列表是可以穷举列出的，所以不在列表里的y根本不存在。\n\n至此我们意识到，**无穷集与无穷集之间似乎也有差别，有些无穷集比另一些无穷集更“无穷”，无穷集也有“大小”。**\n\n\n\n命题：$\\mathcal{P} (A)\\approx \\{0,1\\}^A=\\{f|f:A\\to \\{0,1\\}\\}$\n\n例如，$A=\\{a,b,c\\}，X\\subseteq A$，判定函数\n$$\nf_X(x)=\\left\\{\n             \\begin{array}{lr}\n             1, \\ x \\in X  \\\\\n            \n             0,\\ x \\not \\in X\n             \\end{array}\n\\right.\n$$\n是A中子集X的特征判定函数，每个子集对应一个判定函数，比如子集$X=\\{a,c\\}$，则$f_X(a)=1,f_X(b)=0,f_X(c)=1$。\n\n**Cantor定理（1891）**\n$$\n\\begin{aligned}\n&(1)\\ \\mathbb{N} \\not \\approx \\mathbb{R}\\\\\n&(2)\\ \\forall A,\\ A\\not \\approx \\mathcal{P}(A)\n\\end{aligned}\n$$\n集合A一定和它的幂集不等势。\n\n可以证明，A的幂集的势大于A。\n\n为了比较无穷集的大小，引入集合的**“优势关系”**：\n\n*集合的优势关系*：设A，B为集合，若存在从A到B的单射(1-1)函数，则称集合B优势于集合A，记做：$A\\leq·B$。若$A\\leq·B\\land A\\not\\approx B$，称为真优势，记做：$A<·B$。\n\n\n\n**Cantor-Bernstein-Schroder定理（三明治定理）**\n\n\n\n**连续统假设（C.H）**：在自然数无穷和实数无穷之间不存在其他等级的无穷\n\n连续统假设和ZFC是独立的，ZFC+C.H，$ZFC+\\neg C.H$，$\\neg ZFC+C.H$，$\\neg ZFC+\\neg  C.H$均是相容的。\n\n<br/><br/><br/>\n\n<small>参考</small>：\n\n<small>[超理论坛 如何理解克罗内克的话「上帝创造了整数，其余都是人做的工作」]( https://chaoli.club/index.php/3650/0)</small>\n\n<small>[海德沙龙 当认知世界中没有数字](https://card.weibo.com/article/m/show/id/2309404466491167736057?_wb_client_=1)</small>\n\n<small>[数学的三个纲领：逻辑、形式语言、以及直觉](https://zhuanlan.zhihu.com/p/46168413)</small>\n\n<small>[wikipedia 大卫·希尔伯特](https://en.wikipedia.org/wiki/David_Hilbert)</small>\n\n","source":"_posts/集合论.md","raw":"---\ntitle: 集合论与无限\ndate: 2020-02-02 23:04:00\ncategories:\n- 数理逻辑\ntags: \n- 集合论\n- 逻辑学\n- 数学\nmathjax: true\n---\n\n> \"The infinite, like no other problem, has always deeply moved the soul of men. The infinite, like no other idea, has had a stimulating and fertile influence upon the mind. But the infinite is also more than any other concept, in need of clarification.\"\n>\n> ​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——David Hilbert\n\n<!--more-->\n\n###  一、自然数的定义\n\n一位德国数学家克罗内克说过：“上帝创造了整数；其余皆出凡人之手”。克罗内克是直觉派的坚定支持者，他认为只有整数符合人类的经验和直觉，除此之外都是人类的想象。\n\n自然数是如何产生的？如今，12个刻度的时钟、银行账户的存款数目、电话号码、身份证号，这些数字根深蒂固地存在于每一个现代人脑中。但这些构成我们现代生活的数字并非与生俱来。从智人出现算起漫长的20多万年时间里，我们这个物种很长一段时间并没有发展出精确表示数量的方法。事实上，直到现在，非洲亚马逊河流域还存在一些“无数字”的原始部落，他们以狩猎-采集为生，只用“少量”、“一些”这些词语表示数量。当要求他们准确辨别一些数量，比如“罐子里有五个坚果，拿走三个之后，罐子里还有几个”，“树上总共有多少个椰子”这些问题，对他们来说仍然异常艰难。即使在现代社会，“无数字”的生活也离我们并不遥远。仅仅回到50年前，中国农村仍然可以见到不会做十以上加减法的老人，在封建历史时期，基础的算术教育并没有像如今一样普及。刚出生的婴儿在接触数学前，只能大致分辨三以上的数量。这也就是说，最初的计数和运算仅仅在人们需要时才出现，与经验和生存的需求密不可分，当没有数字也可以生活得不错时，也就无需扩展对数字的认知——除非纯粹的想象和思辨出现。（关于直观和形式逻辑的纷争水很深，涉及到三大数学哲学流派：逻辑主义、形式主义、直觉主义，代表人物分别是罗素、希尔伯特、布劳威尔。《什么是数学》的作者柯朗写书时正值公理演绎盛行时期，他反对过分强调公理演绎。爱因斯坦曾经说过，“西方科学的发展是以两个伟大的成就为基础的：希腊哲学家发明的形式逻辑体系（在欧几里得几何学中），以及（在文艺复兴时期）发现通过系统的实验可能找出因果关系。”对于科学来讲，古希腊的逻辑演绎精神和文艺复兴时期产生的实验精神对科学同等重要。）\n\n假如非要说上帝创造了整数，那也只能从这个角度理解：上帝赋予人类十根手指，并从地面解放了它们。人类从认识自己的手指开始，头脑中闪过的关于数字的想法被语言记录下来，一代代流传下去。有一天，当人们意识到可以不依赖手指、石头、绳结去分辨羊群里有几头羊——事实上，可以不依赖任何实物——数字才被抽象了出来。\n\n克罗内克反对康托尔的集合论，他拒绝无理数的概念，认为整数具有“自然”的优越地位，也就把自己挡在了更加广阔的数学世界门外。\n\n#### Cantor :从集合构造自然数\n\n设x为集合，定义x的后继(successor)$x^+为x\\cup \\{x\\}$,则自然数可构造为：\n$$\n0=\\emptyset,1=0^+,2=0^{++}...n=0^{+...+(n个+)}\n$$\n定义归纳集（inductive set）A：\n$$\n\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\n$$\n有了归纳集的概念，引入一条**无穷公理**（Axiom of Infinity, ZFC.7）说明自然数集$\\mathbb{N}$的存在：\n$$\n\\exists A(\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A))\n$$\n\n于是，自然数和自然数集就在集合论中构造出来了。这里要注意，无穷并不是构造出来的，而是通过公理引入的。\n\n#### Peano自然数算术公理系统\n\n1. $0\\in\\mathbb{N}$\n\n2. $n\\in\\mathbb{N}\\to n^+\\in\\mathbb{N}$\n\n3. $n^+=m^+\\to n= m$\n\n4. $0\\not=n^+$\n\n5. $\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\\to (\\forall x\\in \\mathbb{N})(x\\in A)$\n\n\n\n\n### 二、集合的基数\n\n#### 集合的基数与等势关系\n\n- 集合的基数：集合中元素的个数称为集合A的**基数（cardinals）**，又称集合A的势（cardinality），记为cardA或$\\mid A \\mid$。\n\n- 集合等势：A**等势（equipotence)**于B指存在A与B的双射函数（1-1,onto），记为$A\\approx B$。\n\n#### 有限集和无穷集\n\n- 有限集：若$\\exists n\\in \\mathbb{N}$，使得$A\\approx n$，则A是有限集。\n\n- 无穷集：非有限集为无穷集。\n\n#### “无穷的大小”：可列集、不可列集、集合优势关系\n\n- 可列集：与自然数集N等势的集合，$A \\approx \\mathbb{N}$，A的势$|A|=\\aleph _0$。\n\n<small>（可以按某种确定的规则与自然数集一一对应（1-1，onto）。意味着可列集可以按确定的顺序线性排列，并用自然数数出。所谓“确定的”顺序是指对序列中任一元素，可以说出它“前一个”、“后一个”元素是什么。）</small>\n\n- 不可列集：不是可列集的集合。\n\n> **一个经典的例子——「证明实数集是不可列集(Cantor's-diagonalization-argument)」**\n>\n> 由于$R\\approx[0,1]$，故只需证明$[0,1]$之间的实数点集不可列\n>\n> 假设$[0,1]$之间的实数点集可列，则$[0,1]$上的值可列举为：\n> $$\n> 0.b_{11}b_{12}b_{13}b_{14}...\\\\\n> 0.b_{21}b_{22}b_{23}b_{24}...\\\\\n> 0.b_{31}b_{32}b_{33}b_{34}...\\\\\n> 0.b_{41}b_{42}b_{43}b_{44}...\\\\\n> ...\n> $$\n> 取实数$y\\in[0,1]$，表示为$0.b_{1}b_{2}b_{3}...$，并令$b_{i}\\not=b_{ii}(i=1,2,3,...)$。易见，$y$与上表中任一值均不相等，假设错误，实数集不可列。\n\n可能有人会问，y不在列出的数里，把y加进去不就可以了？这样还是可以列举出来。\n\n但是，加进y之后，我们还是可以用相同的方法，找到一个新的y，令它每一位与列表里对应的对角线数字均不相等。这个过程可以**无穷地进行下去**，而且永远也无法确定下一个y是什么。显然，永远不可能把$[0,1]$间的实数列举完。\n\n可能有人还会问，把“$[0,1]$间实数”换成“自然数”，按照Cantor的方法，不是也可以找到无穷无尽符合条件的y吗？这样岂不证明自然数集也是不可列的，也即自然数集的势不等于它自身吗？这显然是矛盾的。\n\n错，这样的y并不存在。\n\n问题的关键在于，**实数小数点后的位数可以是无穷的**，而虽然自然数集是无穷集，但**一个自然数必然有确定的位数**，不存在一个无穷位数的自然数。当试图找到一个$y=b_{1}b_{2}b_{3}...b_{n}$，令$b_{i}\\not=b_{ii}(i=1,2,3,...n)$时，因为n是一个确定的自然数，n位数的自然数列表是可以穷举列出的，所以不在列表里的y根本不存在。\n\n至此我们意识到，**无穷集与无穷集之间似乎也有差别，有些无穷集比另一些无穷集更“无穷”，无穷集也有“大小”。**\n\n\n\n命题：$\\mathcal{P} (A)\\approx \\{0,1\\}^A=\\{f|f:A\\to \\{0,1\\}\\}$\n\n例如，$A=\\{a,b,c\\}，X\\subseteq A$，判定函数\n$$\nf_X(x)=\\left\\{\n             \\begin{array}{lr}\n             1, \\ x \\in X  \\\\\n            \n             0,\\ x \\not \\in X\n             \\end{array}\n\\right.\n$$\n是A中子集X的特征判定函数，每个子集对应一个判定函数，比如子集$X=\\{a,c\\}$，则$f_X(a)=1,f_X(b)=0,f_X(c)=1$。\n\n**Cantor定理（1891）**\n$$\n\\begin{aligned}\n&(1)\\ \\mathbb{N} \\not \\approx \\mathbb{R}\\\\\n&(2)\\ \\forall A,\\ A\\not \\approx \\mathcal{P}(A)\n\\end{aligned}\n$$\n集合A一定和它的幂集不等势。\n\n可以证明，A的幂集的势大于A。\n\n为了比较无穷集的大小，引入集合的**“优势关系”**：\n\n*集合的优势关系*：设A，B为集合，若存在从A到B的单射(1-1)函数，则称集合B优势于集合A，记做：$A\\leq·B$。若$A\\leq·B\\land A\\not\\approx B$，称为真优势，记做：$A<·B$。\n\n\n\n**Cantor-Bernstein-Schroder定理（三明治定理）**\n\n\n\n**连续统假设（C.H）**：在自然数无穷和实数无穷之间不存在其他等级的无穷\n\n连续统假设和ZFC是独立的，ZFC+C.H，$ZFC+\\neg C.H$，$\\neg ZFC+C.H$，$\\neg ZFC+\\neg  C.H$均是相容的。\n\n<br/><br/><br/>\n\n<small>参考</small>：\n\n<small>[超理论坛 如何理解克罗内克的话「上帝创造了整数，其余都是人做的工作」]( https://chaoli.club/index.php/3650/0)</small>\n\n<small>[海德沙龙 当认知世界中没有数字](https://card.weibo.com/article/m/show/id/2309404466491167736057?_wb_client_=1)</small>\n\n<small>[数学的三个纲领：逻辑、形式语言、以及直觉](https://zhuanlan.zhihu.com/p/46168413)</small>\n\n<small>[wikipedia 大卫·希尔伯特](https://en.wikipedia.org/wiki/David_Hilbert)</small>\n\n","slug":"集合论","published":1,"updated":"2020-11-12T03:57:44.322Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckyoogd73002q2sfyb7kf9086","content":"<blockquote>\n<p>“The infinite, like no other problem, has always deeply moved the soul of men. The infinite, like no other idea, has had a stimulating and fertile influence upon the mind. But the infinite is also more than any other concept, in need of clarification.”</p>\n<p>​                                                                                                                                         ——David Hilbert</p>\n</blockquote>\n<a id=\"more\"></a>\n<h3 id=\"一、自然数的定义\"><a href=\"#一、自然数的定义\" class=\"headerlink\" title=\"一、自然数的定义\"></a>一、自然数的定义</h3><p>一位德国数学家克罗内克说过：“上帝创造了整数；其余皆出凡人之手”。克罗内克是直觉派的坚定支持者，他认为只有整数符合人类的经验和直觉，除此之外都是人类的想象。</p>\n<p>自然数是如何产生的？如今，12个刻度的时钟、银行账户的存款数目、电话号码、身份证号，这些数字根深蒂固地存在于每一个现代人脑中。但这些构成我们现代生活的数字并非与生俱来。从智人出现算起漫长的20多万年时间里，我们这个物种很长一段时间并没有发展出精确表示数量的方法。事实上，直到现在，非洲亚马逊河流域还存在一些“无数字”的原始部落，他们以狩猎-采集为生，只用“少量”、“一些”这些词语表示数量。当要求他们准确辨别一些数量，比如“罐子里有五个坚果，拿走三个之后，罐子里还有几个”，“树上总共有多少个椰子”这些问题，对他们来说仍然异常艰难。即使在现代社会，“无数字”的生活也离我们并不遥远。仅仅回到50年前，中国农村仍然可以见到不会做十以上加减法的老人，在封建历史时期，基础的算术教育并没有像如今一样普及。刚出生的婴儿在接触数学前，只能大致分辨三以上的数量。这也就是说，最初的计数和运算仅仅在人们需要时才出现，与经验和生存的需求密不可分，当没有数字也可以生活得不错时，也就无需扩展对数字的认知——除非纯粹的想象和思辨出现。（关于直观和形式逻辑的纷争水很深，涉及到三大数学哲学流派：逻辑主义、形式主义、直觉主义，代表人物分别是罗素、希尔伯特、布劳威尔。《什么是数学》的作者柯朗写书时正值公理演绎盛行时期，他反对过分强调公理演绎。爱因斯坦曾经说过，“西方科学的发展是以两个伟大的成就为基础的：希腊哲学家发明的形式逻辑体系（在欧几里得几何学中），以及（在文艺复兴时期）发现通过系统的实验可能找出因果关系。”对于科学来讲，古希腊的逻辑演绎精神和文艺复兴时期产生的实验精神对科学同等重要。）</p>\n<p>假如非要说上帝创造了整数，那也只能从这个角度理解：上帝赋予人类十根手指，并从地面解放了它们。人类从认识自己的手指开始，头脑中闪过的关于数字的想法被语言记录下来，一代代流传下去。有一天，当人们意识到可以不依赖手指、石头、绳结去分辨羊群里有几头羊——事实上，可以不依赖任何实物——数字才被抽象了出来。</p>\n<p>克罗内克反对康托尔的集合论，他拒绝无理数的概念，认为整数具有“自然”的优越地位，也就把自己挡在了更加广阔的数学世界门外。</p>\n<h4 id=\"Cantor-从集合构造自然数\"><a href=\"#Cantor-从集合构造自然数\" class=\"headerlink\" title=\"Cantor :从集合构造自然数\"></a>Cantor :从集合构造自然数</h4><p>设x为集合，定义x的后继(successor)$x^+为x\\cup \\{x\\}$,则自然数可构造为：</p>\n<script type=\"math/tex; mode=display\">\n0=\\emptyset,1=0^+,2=0^{++}...n=0^{+...+(n个+)}</script><p>定义归纳集（inductive set）A：</p>\n<script type=\"math/tex; mode=display\">\n\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)</script><p>有了归纳集的概念，引入一条<strong>无穷公理</strong>（Axiom of Infinity, ZFC.7）说明自然数集$\\mathbb{N}$的存在：</p>\n<script type=\"math/tex; mode=display\">\n\\exists A(\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A))</script><p>于是，自然数和自然数集就在集合论中构造出来了。这里要注意，无穷并不是构造出来的，而是通过公理引入的。</p>\n<h4 id=\"Peano自然数算术公理系统\"><a href=\"#Peano自然数算术公理系统\" class=\"headerlink\" title=\"Peano自然数算术公理系统\"></a>Peano自然数算术公理系统</h4><ol>\n<li><p>$0\\in\\mathbb{N}$</p>\n</li>\n<li><p>$n\\in\\mathbb{N}\\to n^+\\in\\mathbb{N}$</p>\n</li>\n<li><p>$n^+=m^+\\to n= m$</p>\n</li>\n<li><p>$0\\not=n^+$</p>\n</li>\n<li><p>$\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\\to (\\forall x\\in \\mathbb{N})(x\\in A)$</p>\n</li>\n</ol>\n<h3 id=\"二、集合的基数\"><a href=\"#二、集合的基数\" class=\"headerlink\" title=\"二、集合的基数\"></a>二、集合的基数</h3><h4 id=\"集合的基数与等势关系\"><a href=\"#集合的基数与等势关系\" class=\"headerlink\" title=\"集合的基数与等势关系\"></a>集合的基数与等势关系</h4><ul>\n<li><p>集合的基数：集合中元素的个数称为集合A的<strong>基数（cardinals）</strong>，又称集合A的势（cardinality），记为cardA或$\\mid A \\mid$。</p>\n</li>\n<li><p>集合等势：A<strong>等势（equipotence)</strong>于B指存在A与B的双射函数（1-1,onto），记为$A\\approx B$。</p>\n</li>\n</ul>\n<h4 id=\"有限集和无穷集\"><a href=\"#有限集和无穷集\" class=\"headerlink\" title=\"有限集和无穷集\"></a>有限集和无穷集</h4><ul>\n<li><p>有限集：若$\\exists n\\in \\mathbb{N}$，使得$A\\approx n$，则A是有限集。</p>\n</li>\n<li><p>无穷集：非有限集为无穷集。</p>\n</li>\n</ul>\n<h4 id=\"“无穷的大小”：可列集、不可列集、集合优势关系\"><a href=\"#“无穷的大小”：可列集、不可列集、集合优势关系\" class=\"headerlink\" title=\"“无穷的大小”：可列集、不可列集、集合优势关系\"></a>“无穷的大小”：可列集、不可列集、集合优势关系</h4><ul>\n<li>可列集：与自然数集N等势的集合，$A \\approx \\mathbb{N}$，A的势$|A|=\\aleph _0$。</li>\n</ul>\n<p><small>（可以按某种确定的规则与自然数集一一对应（1-1，onto）。意味着可列集可以按确定的顺序线性排列，并用自然数数出。所谓“确定的”顺序是指对序列中任一元素，可以说出它“前一个”、“后一个”元素是什么。）</small></p>\n<ul>\n<li>不可列集：不是可列集的集合。</li>\n</ul>\n<blockquote>\n<p><strong>一个经典的例子——「证明实数集是不可列集(Cantor’s-diagonalization-argument)」</strong></p>\n<p>由于$R\\approx[0,1]$，故只需证明$[0,1]$之间的实数点集不可列</p>\n<p>假设$[0,1]$之间的实数点集可列，则$[0,1]$上的值可列举为：</p>\n<script type=\"math/tex; mode=display\">\n0.b_{11}b_{12}b_{13}b_{14}...\\\\\n0.b_{21}b_{22}b_{23}b_{24}...\\\\\n0.b_{31}b_{32}b_{33}b_{34}...\\\\\n0.b_{41}b_{42}b_{43}b_{44}...\\\\\n...</script><p>取实数$y\\in[0,1]$，表示为$0.b_{1}b_{2}b_{3}…$，并令$b_{i}\\not=b_{ii}(i=1,2,3,…)$。易见，$y$与上表中任一值均不相等，假设错误，实数集不可列。</p>\n</blockquote>\n<p>可能有人会问，y不在列出的数里，把y加进去不就可以了？这样还是可以列举出来。</p>\n<p>但是，加进y之后，我们还是可以用相同的方法，找到一个新的y，令它每一位与列表里对应的对角线数字均不相等。这个过程可以<strong>无穷地进行下去</strong>，而且永远也无法确定下一个y是什么。显然，永远不可能把$[0,1]$间的实数列举完。</p>\n<p>可能有人还会问，把“$[0,1]$间实数”换成“自然数”，按照Cantor的方法，不是也可以找到无穷无尽符合条件的y吗？这样岂不证明自然数集也是不可列的，也即自然数集的势不等于它自身吗？这显然是矛盾的。</p>\n<p>错，这样的y并不存在。</p>\n<p>问题的关键在于，<strong>实数小数点后的位数可以是无穷的</strong>，而虽然自然数集是无穷集，但<strong>一个自然数必然有确定的位数</strong>，不存在一个无穷位数的自然数。当试图找到一个$y=b_{1}b_{2}b_{3}…b_{n}$，令$b_{i}\\not=b_{ii}(i=1,2,3,…n)$时，因为n是一个确定的自然数，n位数的自然数列表是可以穷举列出的，所以不在列表里的y根本不存在。</p>\n<p>至此我们意识到，<strong>无穷集与无穷集之间似乎也有差别，有些无穷集比另一些无穷集更“无穷”，无穷集也有“大小”。</strong></p>\n<p>命题：$\\mathcal{P} (A)\\approx \\{0,1\\}^A=\\{f|f:A\\to \\{0,1\\}\\}$</p>\n<p>例如，$A=\\{a,b,c\\}，X\\subseteq A$，判定函数</p>\n<script type=\"math/tex; mode=display\">\nf_X(x)=\\left\\{\n             \\begin{array}{lr}\n             1, \\ x \\in X  \\\\\n\n             0,\\ x \\not \\in X\n             \\end{array}\n\\right.</script><p>是A中子集X的特征判定函数，每个子集对应一个判定函数，比如子集$X=\\{a,c\\}$，则$f_X(a)=1,f_X(b)=0,f_X(c)=1$。</p>\n<p><strong>Cantor定理（1891）</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&(1)\\ \\mathbb{N} \\not \\approx \\mathbb{R}\\\\\n&(2)\\ \\forall A,\\ A\\not \\approx \\mathcal{P}(A)\n\\end{aligned}</script><p>集合A一定和它的幂集不等势。</p>\n<p>可以证明，A的幂集的势大于A。</p>\n<p>为了比较无穷集的大小，引入集合的<strong>“优势关系”</strong>：</p>\n<p><em>集合的优势关系</em>：设A，B为集合，若存在从A到B的单射(1-1)函数，则称集合B优势于集合A，记做：$A\\leq·B$。若$A\\leq·B\\land A\\not\\approx B$，称为真优势，记做：$A&lt;·B$。</p>\n<p><strong>Cantor-Bernstein-Schroder定理（三明治定理）</strong></p>\n<p><strong>连续统假设（C.H）</strong>：在自然数无穷和实数无穷之间不存在其他等级的无穷</p>\n<p>连续统假设和ZFC是独立的，ZFC+C.H，$ZFC+\\neg C.H$，$\\neg ZFC+C.H$，$\\neg ZFC+\\neg  C.H$均是相容的。</p>\n<p><br/><br/><br/></p>\n<p><small>参考</small>：</p>\n<p><small><a href=\"https://chaoli.club/index.php/3650/0\" target=\"_blank\" rel=\"noopener\">超理论坛 如何理解克罗内克的话「上帝创造了整数，其余都是人做的工作」</a></small></p>\n<p><small><a href=\"https://card.weibo.com/article/m/show/id/2309404466491167736057?_wb_client_=1\" target=\"_blank\" rel=\"noopener\">海德沙龙 当认知世界中没有数字</a></small></p>\n<p><small><a href=\"https://zhuanlan.zhihu.com/p/46168413\" target=\"_blank\" rel=\"noopener\">数学的三个纲领：逻辑、形式语言、以及直觉</a></small></p>\n<p><small><a href=\"https://en.wikipedia.org/wiki/David_Hilbert\" target=\"_blank\" rel=\"noopener\">wikipedia 大卫·希尔伯特</a></small></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>“The infinite, like no other problem, has always deeply moved the soul of men. The infinite, like no other idea, has had a stimulating and fertile influence upon the mind. But the infinite is also more than any other concept, in need of clarification.”</p>\n<p>​                                                                                                                                         ——David Hilbert</p>\n</blockquote>","more":"<h3 id=\"一、自然数的定义\"><a href=\"#一、自然数的定义\" class=\"headerlink\" title=\"一、自然数的定义\"></a>一、自然数的定义</h3><p>一位德国数学家克罗内克说过：“上帝创造了整数；其余皆出凡人之手”。克罗内克是直觉派的坚定支持者，他认为只有整数符合人类的经验和直觉，除此之外都是人类的想象。</p>\n<p>自然数是如何产生的？如今，12个刻度的时钟、银行账户的存款数目、电话号码、身份证号，这些数字根深蒂固地存在于每一个现代人脑中。但这些构成我们现代生活的数字并非与生俱来。从智人出现算起漫长的20多万年时间里，我们这个物种很长一段时间并没有发展出精确表示数量的方法。事实上，直到现在，非洲亚马逊河流域还存在一些“无数字”的原始部落，他们以狩猎-采集为生，只用“少量”、“一些”这些词语表示数量。当要求他们准确辨别一些数量，比如“罐子里有五个坚果，拿走三个之后，罐子里还有几个”，“树上总共有多少个椰子”这些问题，对他们来说仍然异常艰难。即使在现代社会，“无数字”的生活也离我们并不遥远。仅仅回到50年前，中国农村仍然可以见到不会做十以上加减法的老人，在封建历史时期，基础的算术教育并没有像如今一样普及。刚出生的婴儿在接触数学前，只能大致分辨三以上的数量。这也就是说，最初的计数和运算仅仅在人们需要时才出现，与经验和生存的需求密不可分，当没有数字也可以生活得不错时，也就无需扩展对数字的认知——除非纯粹的想象和思辨出现。（关于直观和形式逻辑的纷争水很深，涉及到三大数学哲学流派：逻辑主义、形式主义、直觉主义，代表人物分别是罗素、希尔伯特、布劳威尔。《什么是数学》的作者柯朗写书时正值公理演绎盛行时期，他反对过分强调公理演绎。爱因斯坦曾经说过，“西方科学的发展是以两个伟大的成就为基础的：希腊哲学家发明的形式逻辑体系（在欧几里得几何学中），以及（在文艺复兴时期）发现通过系统的实验可能找出因果关系。”对于科学来讲，古希腊的逻辑演绎精神和文艺复兴时期产生的实验精神对科学同等重要。）</p>\n<p>假如非要说上帝创造了整数，那也只能从这个角度理解：上帝赋予人类十根手指，并从地面解放了它们。人类从认识自己的手指开始，头脑中闪过的关于数字的想法被语言记录下来，一代代流传下去。有一天，当人们意识到可以不依赖手指、石头、绳结去分辨羊群里有几头羊——事实上，可以不依赖任何实物——数字才被抽象了出来。</p>\n<p>克罗内克反对康托尔的集合论，他拒绝无理数的概念，认为整数具有“自然”的优越地位，也就把自己挡在了更加广阔的数学世界门外。</p>\n<h4 id=\"Cantor-从集合构造自然数\"><a href=\"#Cantor-从集合构造自然数\" class=\"headerlink\" title=\"Cantor :从集合构造自然数\"></a>Cantor :从集合构造自然数</h4><p>设x为集合，定义x的后继(successor)$x^+为x\\cup \\{x\\}$,则自然数可构造为：</p>\n<script type=\"math/tex; mode=display\">\n0=\\emptyset,1=0^+,2=0^{++}...n=0^{+...+(n个+)}</script><p>定义归纳集（inductive set）A：</p>\n<script type=\"math/tex; mode=display\">\n\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)</script><p>有了归纳集的概念，引入一条<strong>无穷公理</strong>（Axiom of Infinity, ZFC.7）说明自然数集$\\mathbb{N}$的存在：</p>\n<script type=\"math/tex; mode=display\">\n\\exists A(\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A))</script><p>于是，自然数和自然数集就在集合论中构造出来了。这里要注意，无穷并不是构造出来的，而是通过公理引入的。</p>\n<h4 id=\"Peano自然数算术公理系统\"><a href=\"#Peano自然数算术公理系统\" class=\"headerlink\" title=\"Peano自然数算术公理系统\"></a>Peano自然数算术公理系统</h4><ol>\n<li><p>$0\\in\\mathbb{N}$</p>\n</li>\n<li><p>$n\\in\\mathbb{N}\\to n^+\\in\\mathbb{N}$</p>\n</li>\n<li><p>$n^+=m^+\\to n= m$</p>\n</li>\n<li><p>$0\\not=n^+$</p>\n</li>\n<li><p>$\\emptyset\\in A\\land (\\forall x\\in A )(x^+\\in A)\\to (\\forall x\\in \\mathbb{N})(x\\in A)$</p>\n</li>\n</ol>\n<h3 id=\"二、集合的基数\"><a href=\"#二、集合的基数\" class=\"headerlink\" title=\"二、集合的基数\"></a>二、集合的基数</h3><h4 id=\"集合的基数与等势关系\"><a href=\"#集合的基数与等势关系\" class=\"headerlink\" title=\"集合的基数与等势关系\"></a>集合的基数与等势关系</h4><ul>\n<li><p>集合的基数：集合中元素的个数称为集合A的<strong>基数（cardinals）</strong>，又称集合A的势（cardinality），记为cardA或$\\mid A \\mid$。</p>\n</li>\n<li><p>集合等势：A<strong>等势（equipotence)</strong>于B指存在A与B的双射函数（1-1,onto），记为$A\\approx B$。</p>\n</li>\n</ul>\n<h4 id=\"有限集和无穷集\"><a href=\"#有限集和无穷集\" class=\"headerlink\" title=\"有限集和无穷集\"></a>有限集和无穷集</h4><ul>\n<li><p>有限集：若$\\exists n\\in \\mathbb{N}$，使得$A\\approx n$，则A是有限集。</p>\n</li>\n<li><p>无穷集：非有限集为无穷集。</p>\n</li>\n</ul>\n<h4 id=\"“无穷的大小”：可列集、不可列集、集合优势关系\"><a href=\"#“无穷的大小”：可列集、不可列集、集合优势关系\" class=\"headerlink\" title=\"“无穷的大小”：可列集、不可列集、集合优势关系\"></a>“无穷的大小”：可列集、不可列集、集合优势关系</h4><ul>\n<li>可列集：与自然数集N等势的集合，$A \\approx \\mathbb{N}$，A的势$|A|=\\aleph _0$。</li>\n</ul>\n<p><small>（可以按某种确定的规则与自然数集一一对应（1-1，onto）。意味着可列集可以按确定的顺序线性排列，并用自然数数出。所谓“确定的”顺序是指对序列中任一元素，可以说出它“前一个”、“后一个”元素是什么。）</small></p>\n<ul>\n<li>不可列集：不是可列集的集合。</li>\n</ul>\n<blockquote>\n<p><strong>一个经典的例子——「证明实数集是不可列集(Cantor’s-diagonalization-argument)」</strong></p>\n<p>由于$R\\approx[0,1]$，故只需证明$[0,1]$之间的实数点集不可列</p>\n<p>假设$[0,1]$之间的实数点集可列，则$[0,1]$上的值可列举为：</p>\n<script type=\"math/tex; mode=display\">\n0.b_{11}b_{12}b_{13}b_{14}...\\\\\n0.b_{21}b_{22}b_{23}b_{24}...\\\\\n0.b_{31}b_{32}b_{33}b_{34}...\\\\\n0.b_{41}b_{42}b_{43}b_{44}...\\\\\n...</script><p>取实数$y\\in[0,1]$，表示为$0.b_{1}b_{2}b_{3}…$，并令$b_{i}\\not=b_{ii}(i=1,2,3,…)$。易见，$y$与上表中任一值均不相等，假设错误，实数集不可列。</p>\n</blockquote>\n<p>可能有人会问，y不在列出的数里，把y加进去不就可以了？这样还是可以列举出来。</p>\n<p>但是，加进y之后，我们还是可以用相同的方法，找到一个新的y，令它每一位与列表里对应的对角线数字均不相等。这个过程可以<strong>无穷地进行下去</strong>，而且永远也无法确定下一个y是什么。显然，永远不可能把$[0,1]$间的实数列举完。</p>\n<p>可能有人还会问，把“$[0,1]$间实数”换成“自然数”，按照Cantor的方法，不是也可以找到无穷无尽符合条件的y吗？这样岂不证明自然数集也是不可列的，也即自然数集的势不等于它自身吗？这显然是矛盾的。</p>\n<p>错，这样的y并不存在。</p>\n<p>问题的关键在于，<strong>实数小数点后的位数可以是无穷的</strong>，而虽然自然数集是无穷集，但<strong>一个自然数必然有确定的位数</strong>，不存在一个无穷位数的自然数。当试图找到一个$y=b_{1}b_{2}b_{3}…b_{n}$，令$b_{i}\\not=b_{ii}(i=1,2,3,…n)$时，因为n是一个确定的自然数，n位数的自然数列表是可以穷举列出的，所以不在列表里的y根本不存在。</p>\n<p>至此我们意识到，<strong>无穷集与无穷集之间似乎也有差别，有些无穷集比另一些无穷集更“无穷”，无穷集也有“大小”。</strong></p>\n<p>命题：$\\mathcal{P} (A)\\approx \\{0,1\\}^A=\\{f|f:A\\to \\{0,1\\}\\}$</p>\n<p>例如，$A=\\{a,b,c\\}，X\\subseteq A$，判定函数</p>\n<script type=\"math/tex; mode=display\">\nf_X(x)=\\left\\{\n             \\begin{array}{lr}\n             1, \\ x \\in X  \\\\\n\n             0,\\ x \\not \\in X\n             \\end{array}\n\\right.</script><p>是A中子集X的特征判定函数，每个子集对应一个判定函数，比如子集$X=\\{a,c\\}$，则$f_X(a)=1,f_X(b)=0,f_X(c)=1$。</p>\n<p><strong>Cantor定理（1891）</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&(1)\\ \\mathbb{N} \\not \\approx \\mathbb{R}\\\\\n&(2)\\ \\forall A,\\ A\\not \\approx \\mathcal{P}(A)\n\\end{aligned}</script><p>集合A一定和它的幂集不等势。</p>\n<p>可以证明，A的幂集的势大于A。</p>\n<p>为了比较无穷集的大小，引入集合的<strong>“优势关系”</strong>：</p>\n<p><em>集合的优势关系</em>：设A，B为集合，若存在从A到B的单射(1-1)函数，则称集合B优势于集合A，记做：$A\\leq·B$。若$A\\leq·B\\land A\\not\\approx B$，称为真优势，记做：$A&lt;·B$。</p>\n<p><strong>Cantor-Bernstein-Schroder定理（三明治定理）</strong></p>\n<p><strong>连续统假设（C.H）</strong>：在自然数无穷和实数无穷之间不存在其他等级的无穷</p>\n<p>连续统假设和ZFC是独立的，ZFC+C.H，$ZFC+\\neg C.H$，$\\neg ZFC+C.H$，$\\neg ZFC+\\neg  C.H$均是相容的。</p>\n<p><br/><br/><br/></p>\n<p><small>参考</small>：</p>\n<p><small><a href=\"https://chaoli.club/index.php/3650/0\" target=\"_blank\" rel=\"noopener\">超理论坛 如何理解克罗内克的话「上帝创造了整数，其余都是人做的工作」</a></small></p>\n<p><small><a href=\"https://card.weibo.com/article/m/show/id/2309404466491167736057?_wb_client_=1\" target=\"_blank\" rel=\"noopener\">海德沙龙 当认知世界中没有数字</a></small></p>\n<p><small><a href=\"https://zhuanlan.zhihu.com/p/46168413\" target=\"_blank\" rel=\"noopener\">数学的三个纲领：逻辑、形式语言、以及直觉</a></small></p>\n<p><small><a href=\"https://en.wikipedia.org/wiki/David_Hilbert\" target=\"_blank\" rel=\"noopener\">wikipedia 大卫·希尔伯特</a></small></p>"},{"title":"乘法表和计算器","date":"2022-03-02T07:43:00.000Z","mathjax":true,"_content":"\n\"Having heard without comprehension they are like the deaf; this saying bears witness to them: present they are absent.''    -- Heraclitus\n\n学而不思则罔。 --孔子 <!--more-->\n\n二辩聊过，中国学生心算能力普遍比美国学生高很多，因为背乘法表是中国小学生的必修技能。但单论计算效率而言的话，计算器也已经不是什么稀奇的高科技产品了，假如一个熟练掌握乘法表的中国小学生和一个熟练使用计算器的美国小学生比赛算术，恐怕是美国小学生胜出，而且题目越难算越是如此。那么背乘法表的意义何在？\n\n一部记录高中教育改革的记录片（《真实生长》），北京十一中的一名历史教师和教理科的教师辩论，该不该让学生熟背历史事件的时间节点。历史老师认为这应是学生基本素养的一部分。理科老师认为学生只需花几分钟查表就能知道的东西，无需死记硬背。\n\n费曼去上生物学系的课，老师要他读一篇论文然后做报告。论文需要他了解猫全身肌肉的分布，于是他花十来分钟去图书馆查到了“猫地图”。当他作报告画“猫地图”的时候，生物系的学生说，“这些我们都知道”。费曼说“哦，你们知道，怪不得我能这么快就赶上你们这些学了四年生物的。”15分钟能找的到的东西，他们却把时间都浪费在死记硬背这些东西上。\n\n在巴西的一家饭馆，费曼和一个日本人比赛算术。日本人精通算盘。开始是简单的加法，然后是乘法，除法，费曼都比不过日本人。最后一题求1729.03的立方根，费曼用微积分的近似法很快得到结果约是12.003，打败了日本人。费曼认为，这是因为他真的“懂数字”。\n\n第四个例子似乎和第一个例子有矛盾。在较量计算速度的这个比赛里，似乎掌握更先进的工具就具有优势。日本人用算盘，算盘已经是比乘法表心算更强大的工具了，但最后为什么还是败给了看似简单的微分近似法则？乘法表，计算器，算盘，近似法则其实都是一些规则，或者工具，工具是被用来解决问题，从而有了低效和高效之分。但这些辩论到底是围绕什么？\n\n费曼还举过一个例子。他的爸爸指着树上的一只鸟问他：“你知道这是什么鸟吗？这是brown-throated thrush，它在葡萄牙语里叫Bom da Peida，意大利语里，它叫Chutto Lapittida，中文读作Chung-long-tah，日语叫作Katano Teketa，不一而足。你可以用你想了解的所有语种，去称呼这只鸟，但当你叫出它的名字之后，你其实对这只鸟还是一无所知。现在，让我们来好好看看这只鸟，观察它在做什么。”\n\n<br/><br/>\n\n2022年3月17日更新\n\n今天看到一个2017年的talk，Naftali Tishby教授讲机器学习中的信息论[Information Theory of Deep Learning](https://www.youtube.com/watch?v=bLqJHjXihK8)，很mind-blowing（看得我小脸通红）。\n\n<img src=\"/images/IMG_1190.jpg\" alt=\"IMG_1190\" style=\"zoom:40%;\" />\n\n<center><small>The encoder(x axis) vs decoder(y axis) mutual information（from Tishby's talk）</small></center>\n\n这张图是Tishby talk的精髓，用一句话概括就是“先读厚，再读薄。”\n\n想起来几年前问过我很喜欢的日本dancer Yu-mah一个问题，想要跳得很棒的话，那些日复一日对基本功的重复训练，是不是一个必经的阶段？现在想想，当年naïve的我真的问了一个哲学问题。我大概是想知道存不存在这样一条路线\n\n<img src=\"/images/Numofepochs3999.jpg\" alt=\"Numofepochs3999\" style=\"zoom:55%;\" />\n<center><small>我想象中的《如来神掌》</small></center>\n\n<br/><br/><br/>\n","source":"_posts/multi_table_n_calclator.md","raw":"---\ntitle: 乘法表和计算器\ndate: 2022-03-02 15:43:00\ncategories:\n- 杂想\ntags: \n\nmathjax: true\n\n\n---\n\n\"Having heard without comprehension they are like the deaf; this saying bears witness to them: present they are absent.''    -- Heraclitus\n\n学而不思则罔。 --孔子 <!--more-->\n\n二辩聊过，中国学生心算能力普遍比美国学生高很多，因为背乘法表是中国小学生的必修技能。但单论计算效率而言的话，计算器也已经不是什么稀奇的高科技产品了，假如一个熟练掌握乘法表的中国小学生和一个熟练使用计算器的美国小学生比赛算术，恐怕是美国小学生胜出，而且题目越难算越是如此。那么背乘法表的意义何在？\n\n一部记录高中教育改革的记录片（《真实生长》），北京十一中的一名历史教师和教理科的教师辩论，该不该让学生熟背历史事件的时间节点。历史老师认为这应是学生基本素养的一部分。理科老师认为学生只需花几分钟查表就能知道的东西，无需死记硬背。\n\n费曼去上生物学系的课，老师要他读一篇论文然后做报告。论文需要他了解猫全身肌肉的分布，于是他花十来分钟去图书馆查到了“猫地图”。当他作报告画“猫地图”的时候，生物系的学生说，“这些我们都知道”。费曼说“哦，你们知道，怪不得我能这么快就赶上你们这些学了四年生物的。”15分钟能找的到的东西，他们却把时间都浪费在死记硬背这些东西上。\n\n在巴西的一家饭馆，费曼和一个日本人比赛算术。日本人精通算盘。开始是简单的加法，然后是乘法，除法，费曼都比不过日本人。最后一题求1729.03的立方根，费曼用微积分的近似法很快得到结果约是12.003，打败了日本人。费曼认为，这是因为他真的“懂数字”。\n\n第四个例子似乎和第一个例子有矛盾。在较量计算速度的这个比赛里，似乎掌握更先进的工具就具有优势。日本人用算盘，算盘已经是比乘法表心算更强大的工具了，但最后为什么还是败给了看似简单的微分近似法则？乘法表，计算器，算盘，近似法则其实都是一些规则，或者工具，工具是被用来解决问题，从而有了低效和高效之分。但这些辩论到底是围绕什么？\n\n费曼还举过一个例子。他的爸爸指着树上的一只鸟问他：“你知道这是什么鸟吗？这是brown-throated thrush，它在葡萄牙语里叫Bom da Peida，意大利语里，它叫Chutto Lapittida，中文读作Chung-long-tah，日语叫作Katano Teketa，不一而足。你可以用你想了解的所有语种，去称呼这只鸟，但当你叫出它的名字之后，你其实对这只鸟还是一无所知。现在，让我们来好好看看这只鸟，观察它在做什么。”\n\n<br/><br/>\n\n2022年3月17日更新\n\n今天看到一个2017年的talk，Naftali Tishby教授讲机器学习中的信息论[Information Theory of Deep Learning](https://www.youtube.com/watch?v=bLqJHjXihK8)，很mind-blowing（看得我小脸通红）。\n\n<img src=\"/images/IMG_1190.jpg\" alt=\"IMG_1190\" style=\"zoom:40%;\" />\n\n<center><small>The encoder(x axis) vs decoder(y axis) mutual information（from Tishby's talk）</small></center>\n\n这张图是Tishby talk的精髓，用一句话概括就是“先读厚，再读薄。”\n\n想起来几年前问过我很喜欢的日本dancer Yu-mah一个问题，想要跳得很棒的话，那些日复一日对基本功的重复训练，是不是一个必经的阶段？现在想想，当年naïve的我真的问了一个哲学问题。我大概是想知道存不存在这样一条路线\n\n<img src=\"/images/Numofepochs3999.jpg\" alt=\"Numofepochs3999\" style=\"zoom:55%;\" />\n<center><small>我想象中的《如来神掌》</small></center>\n\n<br/><br/><br/>\n","slug":"multi_table_n_calclator","published":1,"updated":"2022-03-17T19:11:33.948Z","_id":"cl09aye9k0000bsqea2oag18k","comments":1,"layout":"post","photos":[],"link":"","content":"<p>“Having heard without comprehension they are like the deaf; this saying bears witness to them: present they are absent.’’    — Heraclitus</p>\n<p>学而不思则罔。 —孔子 <a id=\"more\"></a></p>\n<p>二辩聊过，中国学生心算能力普遍比美国学生高很多，因为背乘法表是中国小学生的必修技能。但单论计算效率而言的话，计算器也已经不是什么稀奇的高科技产品了，假如一个熟练掌握乘法表的中国小学生和一个熟练使用计算器的美国小学生比赛算术，恐怕是美国小学生胜出，而且题目越难算越是如此。那么背乘法表的意义何在？</p>\n<p>一部记录高中教育改革的记录片（《真实生长》），北京十一中的一名历史教师和教理科的教师辩论，该不该让学生熟背历史事件的时间节点。历史老师认为这应是学生基本素养的一部分。理科老师认为学生只需花几分钟查表就能知道的东西，无需死记硬背。</p>\n<p>费曼去上生物学系的课，老师要他读一篇论文然后做报告。论文需要他了解猫全身肌肉的分布，于是他花十来分钟去图书馆查到了“猫地图”。当他作报告画“猫地图”的时候，生物系的学生说，“这些我们都知道”。费曼说“哦，你们知道，怪不得我能这么快就赶上你们这些学了四年生物的。”15分钟能找的到的东西，他们却把时间都浪费在死记硬背这些东西上。</p>\n<p>在巴西的一家饭馆，费曼和一个日本人比赛算术。日本人精通算盘。开始是简单的加法，然后是乘法，除法，费曼都比不过日本人。最后一题求1729.03的立方根，费曼用微积分的近似法很快得到结果约是12.003，打败了日本人。费曼认为，这是因为他真的“懂数字”。</p>\n<p>第四个例子似乎和第一个例子有矛盾。在较量计算速度的这个比赛里，似乎掌握更先进的工具就具有优势。日本人用算盘，算盘已经是比乘法表心算更强大的工具了，但最后为什么还是败给了看似简单的微分近似法则？乘法表，计算器，算盘，近似法则其实都是一些规则，或者工具，工具是被用来解决问题，从而有了低效和高效之分。但这些辩论到底是围绕什么？</p>\n<p>费曼还举过一个例子。他的爸爸指着树上的一只鸟问他：“你知道这是什么鸟吗？这是brown-throated thrush，它在葡萄牙语里叫Bom da Peida，意大利语里，它叫Chutto Lapittida，中文读作Chung-long-tah，日语叫作Katano Teketa，不一而足。你可以用你想了解的所有语种，去称呼这只鸟，但当你叫出它的名字之后，你其实对这只鸟还是一无所知。现在，让我们来好好看看这只鸟，观察它在做什么。”</p>\n<p><br/><br/></p>\n<p>2022年3月17日更新</p>\n<p>今天看到一个2017年的talk，Naftali Tishby教授讲机器学习中的信息论<a href=\"https://www.youtube.com/watch?v=bLqJHjXihK8\" target=\"_blank\" rel=\"noopener\">Information Theory of Deep Learning</a>，很mind-blowing（看得我小脸通红）。</p>\n<p><img src=\"/images/IMG_1190.jpg\" alt=\"IMG_1190\" style=\"zoom:40%;\" /></p>\n<center><small>The encoder(x axis) vs decoder(y axis) mutual information（from Tishby's talk）</small></center>\n\n<p>这张图是Tishby talk的精髓，用一句话概括就是“先读厚，再读薄。”</p>\n<p>想起来几年前问过我很喜欢的日本dancer Yu-mah一个问题，想要跳得很棒的话，那些日复一日对基本功的重复训练，是不是一个必经的阶段？现在想想，当年naïve的我真的问了一个哲学问题。我大概是想知道存不存在这样一条路线</p>\n<p><img src=\"/images/Numofepochs3999.jpg\" alt=\"Numofepochs3999\" style=\"zoom:55%;\" /></p>\n<center><small>我想象中的《如来神掌》</small></center>\n\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>“Having heard without comprehension they are like the deaf; this saying bears witness to them: present they are absent.’’    — Heraclitus</p>\n<p>学而不思则罔。 —孔子","more":"</p>\n<p>二辩聊过，中国学生心算能力普遍比美国学生高很多，因为背乘法表是中国小学生的必修技能。但单论计算效率而言的话，计算器也已经不是什么稀奇的高科技产品了，假如一个熟练掌握乘法表的中国小学生和一个熟练使用计算器的美国小学生比赛算术，恐怕是美国小学生胜出，而且题目越难算越是如此。那么背乘法表的意义何在？</p>\n<p>一部记录高中教育改革的记录片（《真实生长》），北京十一中的一名历史教师和教理科的教师辩论，该不该让学生熟背历史事件的时间节点。历史老师认为这应是学生基本素养的一部分。理科老师认为学生只需花几分钟查表就能知道的东西，无需死记硬背。</p>\n<p>费曼去上生物学系的课，老师要他读一篇论文然后做报告。论文需要他了解猫全身肌肉的分布，于是他花十来分钟去图书馆查到了“猫地图”。当他作报告画“猫地图”的时候，生物系的学生说，“这些我们都知道”。费曼说“哦，你们知道，怪不得我能这么快就赶上你们这些学了四年生物的。”15分钟能找的到的东西，他们却把时间都浪费在死记硬背这些东西上。</p>\n<p>在巴西的一家饭馆，费曼和一个日本人比赛算术。日本人精通算盘。开始是简单的加法，然后是乘法，除法，费曼都比不过日本人。最后一题求1729.03的立方根，费曼用微积分的近似法很快得到结果约是12.003，打败了日本人。费曼认为，这是因为他真的“懂数字”。</p>\n<p>第四个例子似乎和第一个例子有矛盾。在较量计算速度的这个比赛里，似乎掌握更先进的工具就具有优势。日本人用算盘，算盘已经是比乘法表心算更强大的工具了，但最后为什么还是败给了看似简单的微分近似法则？乘法表，计算器，算盘，近似法则其实都是一些规则，或者工具，工具是被用来解决问题，从而有了低效和高效之分。但这些辩论到底是围绕什么？</p>\n<p>费曼还举过一个例子。他的爸爸指着树上的一只鸟问他：“你知道这是什么鸟吗？这是brown-throated thrush，它在葡萄牙语里叫Bom da Peida，意大利语里，它叫Chutto Lapittida，中文读作Chung-long-tah，日语叫作Katano Teketa，不一而足。你可以用你想了解的所有语种，去称呼这只鸟，但当你叫出它的名字之后，你其实对这只鸟还是一无所知。现在，让我们来好好看看这只鸟，观察它在做什么。”</p>\n<p><br/><br/></p>\n<p>2022年3月17日更新</p>\n<p>今天看到一个2017年的talk，Naftali Tishby教授讲机器学习中的信息论<a href=\"https://www.youtube.com/watch?v=bLqJHjXihK8\" target=\"_blank\" rel=\"noopener\">Information Theory of Deep Learning</a>，很mind-blowing（看得我小脸通红）。</p>\n<p><img src=\"/images/IMG_1190.jpg\" alt=\"IMG_1190\" style=\"zoom:40%;\" /></p>\n<center><small>The encoder(x axis) vs decoder(y axis) mutual information（from Tishby's talk）</small></center>\n\n<p>这张图是Tishby talk的精髓，用一句话概括就是“先读厚，再读薄。”</p>\n<p>想起来几年前问过我很喜欢的日本dancer Yu-mah一个问题，想要跳得很棒的话，那些日复一日对基本功的重复训练，是不是一个必经的阶段？现在想想，当年naïve的我真的问了一个哲学问题。我大概是想知道存不存在这样一条路线</p>\n<p><img src=\"/images/Numofepochs3999.jpg\" alt=\"Numofepochs3999\" style=\"zoom:55%;\" /></p>\n<center><small>我想象中的《如来神掌》</small></center>\n\n<p><br/><br/><br/></p>"},{"title":"生命·梦·?","date":"2022-02-07T17:21:00.000Z","_content":"\n有些人好像是真的有一种宿命感，早早地碰到一件事情就知道，我就是要做这个的。<!--more-->比如李安刚上大学，第一次登上舞台就强烈地感觉到，这辈子就是舞台；爱因斯坦在他上中学时就明确写出了未来的计划，将来会学习数学和物理，从事科学的事业。\n\n大四的北野武走在人行道上，好像“万里无云的天空里突然划过一道闪电”，突然想到要退学，但他当时并不清晰地知道自己能做什么，只是怀着对文学、戏剧之类的向往，去了浅草。这样想来，他的行为的确是与“自杀”无异，需要极大的勇气。在“自杀”的绝境里，北野武存活下来了。\n\n我自觉对事情有不错的sense，过去投入精力的事情，也多少能做得有点样子。但我总觉得差了那么一点热情，还没有一件事我全情地，长时间地投入过。更多时候我在一边观察那些大师和他们的作品，靠着不错的感受力也能体会其中况味，但都不是我自己的。作品是他们的，为理想燃烧的过程也是他们的，我还是平庸着，浑浑噩噩地过着。\n\n我嘲笑自己是“雨露均沾，大饼均摊”，总是有很多爱好。但费曼说过：“我的智慧是有限的，我只把它们用于特定的方向。”这本身就是某种智慧。知道了这个道理，就明白“大饼均摊”只是黑色幽默。去年春天我对打鼓很感兴趣，便跟着视频自学了一学期。初初入门，由于边际效益，体会到诸多妙处。可等到低垂的果实摘完，需要下苦工夫，花大把汗水磨练基本功才能更进一步的时候，我就停下了。想来跳舞也是如此，好不容易到了茅塞顿开的阶段，再下点功夫就能大有长进的时候，我变成了精神舞者，想的多做得少。学数学也大致如此，接触离散数学，逻辑，重学概率论和线性代数，凭着对趣味的追求没头没脑地野蛮生长，虽然也有收获，但始终没有系统扎实地训练，没法再走远一步。\n\n油管上教我打鼓的Jemi说过一句话：“每个人其实最难的就是自律，但是学乐器的话，如果没有自律的心，至少你要有热忱。” Jemi真的很温柔，她知道真正的热忱是稀有的东西，平凡人能做到自律已经很厉害了。快20岁生日的时候我对自己说过，19年来都走着别人划的印儿，20岁还迷茫合情合理。又一个五年过去了，还是一样的话送给自己：但我绝不向虚无投降。\n\n<img src=\"/images/IMG_1113.jpg\" alt=\"IMG_1113\" style=\"zoom:40%;\" />\n\n<br/><br/><br/>","source":"_posts/life_dream_x.md","raw":"---\ntitle: 生命·梦·?\ndate: 2022-02-08 01:21:00\ncategories:\n- 杂想\ntags: \n\n---\n\n有些人好像是真的有一种宿命感，早早地碰到一件事情就知道，我就是要做这个的。<!--more-->比如李安刚上大学，第一次登上舞台就强烈地感觉到，这辈子就是舞台；爱因斯坦在他上中学时就明确写出了未来的计划，将来会学习数学和物理，从事科学的事业。\n\n大四的北野武走在人行道上，好像“万里无云的天空里突然划过一道闪电”，突然想到要退学，但他当时并不清晰地知道自己能做什么，只是怀着对文学、戏剧之类的向往，去了浅草。这样想来，他的行为的确是与“自杀”无异，需要极大的勇气。在“自杀”的绝境里，北野武存活下来了。\n\n我自觉对事情有不错的sense，过去投入精力的事情，也多少能做得有点样子。但我总觉得差了那么一点热情，还没有一件事我全情地，长时间地投入过。更多时候我在一边观察那些大师和他们的作品，靠着不错的感受力也能体会其中况味，但都不是我自己的。作品是他们的，为理想燃烧的过程也是他们的，我还是平庸着，浑浑噩噩地过着。\n\n我嘲笑自己是“雨露均沾，大饼均摊”，总是有很多爱好。但费曼说过：“我的智慧是有限的，我只把它们用于特定的方向。”这本身就是某种智慧。知道了这个道理，就明白“大饼均摊”只是黑色幽默。去年春天我对打鼓很感兴趣，便跟着视频自学了一学期。初初入门，由于边际效益，体会到诸多妙处。可等到低垂的果实摘完，需要下苦工夫，花大把汗水磨练基本功才能更进一步的时候，我就停下了。想来跳舞也是如此，好不容易到了茅塞顿开的阶段，再下点功夫就能大有长进的时候，我变成了精神舞者，想的多做得少。学数学也大致如此，接触离散数学，逻辑，重学概率论和线性代数，凭着对趣味的追求没头没脑地野蛮生长，虽然也有收获，但始终没有系统扎实地训练，没法再走远一步。\n\n油管上教我打鼓的Jemi说过一句话：“每个人其实最难的就是自律，但是学乐器的话，如果没有自律的心，至少你要有热忱。” Jemi真的很温柔，她知道真正的热忱是稀有的东西，平凡人能做到自律已经很厉害了。快20岁生日的时候我对自己说过，19年来都走着别人划的印儿，20岁还迷茫合情合理。又一个五年过去了，还是一样的话送给自己：但我绝不向虚无投降。\n\n<img src=\"/images/IMG_1113.jpg\" alt=\"IMG_1113\" style=\"zoom:40%;\" />\n\n<br/><br/><br/>","slug":"life_dream_x","published":1,"updated":"2022-04-22T02:48:46.404Z","_id":"cl29twdca00026kqe3a0lhf0v","comments":1,"layout":"post","photos":[],"link":"","content":"<p>有些人好像是真的有一种宿命感，早早地碰到一件事情就知道，我就是要做这个的。<a id=\"more\"></a>比如李安刚上大学，第一次登上舞台就强烈地感觉到，这辈子就是舞台；爱因斯坦在他上中学时就明确写出了未来的计划，将来会学习数学和物理，从事科学的事业。</p>\n<p>大四的北野武走在人行道上，好像“万里无云的天空里突然划过一道闪电”，突然想到要退学，但他当时并不清晰地知道自己能做什么，只是怀着对文学、戏剧之类的向往，去了浅草。这样想来，他的行为的确是与“自杀”无异，需要极大的勇气。在“自杀”的绝境里，北野武存活下来了。</p>\n<p>我自觉对事情有不错的sense，过去投入精力的事情，也多少能做得有点样子。但我总觉得差了那么一点热情，还没有一件事我全情地，长时间地投入过。更多时候我在一边观察那些大师和他们的作品，靠着不错的感受力也能体会其中况味，但都不是我自己的。作品是他们的，为理想燃烧的过程也是他们的，我还是平庸着，浑浑噩噩地过着。</p>\n<p>我嘲笑自己是“雨露均沾，大饼均摊”，总是有很多爱好。但费曼说过：“我的智慧是有限的，我只把它们用于特定的方向。”这本身就是某种智慧。知道了这个道理，就明白“大饼均摊”只是黑色幽默。去年春天我对打鼓很感兴趣，便跟着视频自学了一学期。初初入门，由于边际效益，体会到诸多妙处。可等到低垂的果实摘完，需要下苦工夫，花大把汗水磨练基本功才能更进一步的时候，我就停下了。想来跳舞也是如此，好不容易到了茅塞顿开的阶段，再下点功夫就能大有长进的时候，我变成了精神舞者，想的多做得少。学数学也大致如此，接触离散数学，逻辑，重学概率论和线性代数，凭着对趣味的追求没头没脑地野蛮生长，虽然也有收获，但始终没有系统扎实地训练，没法再走远一步。</p>\n<p>油管上教我打鼓的Jemi说过一句话：“每个人其实最难的就是自律，但是学乐器的话，如果没有自律的心，至少你要有热忱。” Jemi真的很温柔，她知道真正的热忱是稀有的东西，平凡人能做到自律已经很厉害了。快20岁生日的时候我对自己说过，19年来都走着别人划的印儿，20岁还迷茫合情合理。又一个五年过去了，还是一样的话送给自己：但我绝不向虚无投降。</p>\n<p><img src=\"/images/IMG_1113.jpg\" alt=\"IMG_1113\" style=\"zoom:40%;\" /></p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>有些人好像是真的有一种宿命感，早早地碰到一件事情就知道，我就是要做这个的。","more":"比如李安刚上大学，第一次登上舞台就强烈地感觉到，这辈子就是舞台；爱因斯坦在他上中学时就明确写出了未来的计划，将来会学习数学和物理，从事科学的事业。</p>\n<p>大四的北野武走在人行道上，好像“万里无云的天空里突然划过一道闪电”，突然想到要退学，但他当时并不清晰地知道自己能做什么，只是怀着对文学、戏剧之类的向往，去了浅草。这样想来，他的行为的确是与“自杀”无异，需要极大的勇气。在“自杀”的绝境里，北野武存活下来了。</p>\n<p>我自觉对事情有不错的sense，过去投入精力的事情，也多少能做得有点样子。但我总觉得差了那么一点热情，还没有一件事我全情地，长时间地投入过。更多时候我在一边观察那些大师和他们的作品，靠着不错的感受力也能体会其中况味，但都不是我自己的。作品是他们的，为理想燃烧的过程也是他们的，我还是平庸着，浑浑噩噩地过着。</p>\n<p>我嘲笑自己是“雨露均沾，大饼均摊”，总是有很多爱好。但费曼说过：“我的智慧是有限的，我只把它们用于特定的方向。”这本身就是某种智慧。知道了这个道理，就明白“大饼均摊”只是黑色幽默。去年春天我对打鼓很感兴趣，便跟着视频自学了一学期。初初入门，由于边际效益，体会到诸多妙处。可等到低垂的果实摘完，需要下苦工夫，花大把汗水磨练基本功才能更进一步的时候，我就停下了。想来跳舞也是如此，好不容易到了茅塞顿开的阶段，再下点功夫就能大有长进的时候，我变成了精神舞者，想的多做得少。学数学也大致如此，接触离散数学，逻辑，重学概率论和线性代数，凭着对趣味的追求没头没脑地野蛮生长，虽然也有收获，但始终没有系统扎实地训练，没法再走远一步。</p>\n<p>油管上教我打鼓的Jemi说过一句话：“每个人其实最难的就是自律，但是学乐器的话，如果没有自律的心，至少你要有热忱。” Jemi真的很温柔，她知道真正的热忱是稀有的东西，平凡人能做到自律已经很厉害了。快20岁生日的时候我对自己说过，19年来都走着别人划的印儿，20岁还迷茫合情合理。又一个五年过去了，还是一样的话送给自己：但我绝不向虚无投降。</p>\n<p><img src=\"/images/IMG_1113.jpg\" alt=\"IMG_1113\" style=\"zoom:40%;\" /></p>\n<p><br/><br/><br/></p>"},{"title":"Touch of untouchable","date":"2022-03-29T12:21:00.000Z","_content":"\n现实是飘渺的，\n\n混沌的，\n\n不确定的。<!--more-->\n\n一旦付梓，显像，观察，记录，\n\n就变实了。\n\n不确定性坍缩，\n\n只剩一种可能性。\n\n可以做出种种尝试。\n\n用数学建模，\n\n可以拍电影，写小说，画画，作曲，\n\n但现实是总结不了的。\n\n不可描述和描述之间，\n\n有美。\n\n\n<br/><br/><br/>","source":"_posts/touch.md","raw":"---\ntitle: Touch of untouchable\ndate: 2022-03-29 20:21:00\ncategories:\n- 杂想\ntags: \n\n---\n\n现实是飘渺的，\n\n混沌的，\n\n不确定的。<!--more-->\n\n一旦付梓，显像，观察，记录，\n\n就变实了。\n\n不确定性坍缩，\n\n只剩一种可能性。\n\n可以做出种种尝试。\n\n用数学建模，\n\n可以拍电影，写小说，画画，作曲，\n\n但现实是总结不了的。\n\n不可描述和描述之间，\n\n有美。\n\n\n<br/><br/><br/>","slug":"touch","published":1,"updated":"2022-07-07T08:51:13.057Z","_id":"cl29tycgg00006vqe18tqdqmk","comments":1,"layout":"post","photos":[],"link":"","content":"<p>现实是飘渺的，</p>\n<p>混沌的，</p>\n<p>不确定的。<a id=\"more\"></a></p>\n<p>一旦付梓，显像，观察，记录，</p>\n<p>就变实了。</p>\n<p>不确定性坍缩，</p>\n<p>只剩一种可能性。</p>\n<p>可以做出种种尝试。</p>\n<p>用数学建模，</p>\n<p>可以拍电影，写小说，画画，作曲，</p>\n<p>但现实是总结不了的。</p>\n<p>不可描述和描述之间，</p>\n<p>有美。</p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p>现实是飘渺的，</p>\n<p>混沌的，</p>\n<p>不确定的。","more":"</p>\n<p>一旦付梓，显像，观察，记录，</p>\n<p>就变实了。</p>\n<p>不确定性坍缩，</p>\n<p>只剩一种可能性。</p>\n<p>可以做出种种尝试。</p>\n<p>用数学建模，</p>\n<p>可以拍电影，写小说，画画，作曲，</p>\n<p>但现实是总结不了的。</p>\n<p>不可描述和描述之间，</p>\n<p>有美。</p>\n<p><br/><br/><br/></p>"},{"title":"My Learning Bible","date":"2022-07-28T07:00:00.000Z","_content":"\n**Find something you love.** <!--more-->\n\n- *Find your unique passion*\n- *Quote others passion (Fashion is the inferior choice)*\n\n**Learning the right way**\n\n- Have a grasp of artistry\n- Find a good teacher\n\n**Get your hands dirty & try your best.** \n\n- Practice! Practice! Practice! \n- Learn and relearn\n\n**Forgetting.**\n\n- The “post-rigorous” stage\n- Free up your mind for intuition\n\n**Congratulations! Enjoy yourself.**\n\n<br/><br/><br/>","source":"_posts/LearningBible.md","raw":"---\ntitle: My Learning Bible\ndate: 2022-07-28 15:00:00\ncategories:\n- 杂想\ntags: \n\n---\n\n**Find something you love.** <!--more-->\n\n- *Find your unique passion*\n- *Quote others passion (Fashion is the inferior choice)*\n\n**Learning the right way**\n\n- Have a grasp of artistry\n- Find a good teacher\n\n**Get your hands dirty & try your best.** \n\n- Practice! Practice! Practice! \n- Learn and relearn\n\n**Forgetting.**\n\n- The “post-rigorous” stage\n- Free up your mind for intuition\n\n**Congratulations! Enjoy yourself.**\n\n<br/><br/><br/>","slug":"LearningBible","published":1,"updated":"2022-07-28T06:57:26.844Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl64opa1b00002uqeblj83fzs","content":"<p><strong>Find something you love.</strong> <a id=\"more\"></a></p>\n<ul>\n<li><em>Find your unique passion</em></li>\n<li><em>Quote others passion (Fashion is the inferior choice)</em></li>\n</ul>\n<p><strong>Learning the right way</strong></p>\n<ul>\n<li>Have a grasp of artistry</li>\n<li>Find a good teacher</li>\n</ul>\n<p><strong>Get your hands dirty &amp; try your best.</strong> </p>\n<ul>\n<li>Practice! Practice! Practice! </li>\n<li>Learn and relearn</li>\n</ul>\n<p><strong>Forgetting.</strong></p>\n<ul>\n<li>The “post-rigorous” stage</li>\n<li>Free up your mind for intuition</li>\n</ul>\n<p><strong>Congratulations! Enjoy yourself.</strong></p>\n<p><br/><br/><br/></p>\n","site":{"data":{}},"excerpt":"<p><strong>Find something you love.</strong>","more":"</p>\n<ul>\n<li><em>Find your unique passion</em></li>\n<li><em>Quote others passion (Fashion is the inferior choice)</em></li>\n</ul>\n<p><strong>Learning the right way</strong></p>\n<ul>\n<li>Have a grasp of artistry</li>\n<li>Find a good teacher</li>\n</ul>\n<p><strong>Get your hands dirty &amp; try your best.</strong> </p>\n<ul>\n<li>Practice! Practice! Practice! </li>\n<li>Learn and relearn</li>\n</ul>\n<p><strong>Forgetting.</strong></p>\n<ul>\n<li>The “post-rigorous” stage</li>\n<li>Free up your mind for intuition</li>\n</ul>\n<p><strong>Congratulations! Enjoy yourself.</strong></p>\n<p><br/><br/><br/></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckyoogd4k00002sfy8szt1ked","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd5j000e2sfy2yod5n60"},{"post_id":"ckyoogd5h000c2sfy3dt1bo5l","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd5t000j2sfyc2j73r4h"},{"post_id":"ckyoogd5500022sfycxz4cmiw","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd5w000n2sfyc4fb27o4"},{"post_id":"ckyoogd5i000d2sfy3qul9dlp","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd5x000q2sfy0tu42r36"},{"post_id":"ckyoogd5o000h2sfyh02rh1og","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd61000v2sfy1jxlhyko"},{"post_id":"ckyoogd5b00062sfyg3lg3epi","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd64000y2sfyfcnoag8k"},{"post_id":"ckyoogd5r000i2sfyazuaafml","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd6500122sfy8a86gfzf"},{"post_id":"ckyoogd5e00082sfy4nj7h0cx","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd6700152sfy9jy59t1y"},{"post_id":"ckyoogd5x000p2sfy4j4n543i","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd6800182sfy7jr60wm7"},{"post_id":"ckyoogd5f00092sfyf1bnde42","category_id":"ckyoogd5y000r2sfyhlva7uf8","_id":"ckyoogd6a001b2sfy03hn5o9u"},{"post_id":"ckyoogd5v000m2sfy9iwb1wrv","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6b001f2sfy0mpi27c3"},{"post_id":"ckyoogd6700172sfy0f1t8hry","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd6e001j2sfy38rr6dry"},{"post_id":"ckyoogd5z000u2sfycdwngfek","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6j001n2sfy1h7j95s1"},{"post_id":"ckyoogd69001a2sfy9i4da3wy","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6l001r2sfy4n3d761r"},{"post_id":"ckyoogd6b001e2sfy0wtqc7pv","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6m001t2sfy0zh28nb6"},{"post_id":"ckyoogd63000x2sfy2cue9d1p","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6o001x2sfy6gqc2jgy"},{"post_id":"ckyoogd6d001i2sfy2egu11bo","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd6q001z2sfy5akn2g57"},{"post_id":"ckyoogd6500112sfy7yaxbh60","category_id":"ckyoogd6h001l2sfyfx121cfc","_id":"ckyoogd6r00232sfy066o0dv0"},{"post_id":"ckyoogd6600142sfy5any6s9y","category_id":"ckyoogd6h001l2sfyfx121cfc","_id":"ckyoogd6t00272sfyf8sn1wz2"},{"post_id":"ckyoogd6p001y2sfyfsufh2zi","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd6v002b2sfye9r55jj6"},{"post_id":"ckyoogd6r00222sfy042724an","category_id":"ckyoogd5g000a2sfy8l48hd7k","_id":"ckyoogd6w002e2sfybezsgljh"},{"post_id":"ckyoogd6i001m2sfydei93sg4","category_id":"ckyoogd6h001l2sfyfx121cfc","_id":"ckyoogd70002j2sfy5ec20cso"},{"post_id":"ckyoogd6s00252sfy1xv7eq92","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd72002m2sfyeffyg5od"},{"post_id":"ckyoogd6k001q2sfyav1n8x8y","category_id":"ckyoogd6t00262sfy85pm8aqg","_id":"ckyoogd74002r2sfy5mo3huh3"},{"post_id":"ckyoogd6v002d2sfy8cr39haf","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd75002t2sfy5azm1t5x"},{"post_id":"ckyoogd6z002i2sfy681c5xni","category_id":"ckyoogd6400102sfydlvubysr","_id":"ckyoogd76002x2sfy4y5geoz3"},{"post_id":"ckyoogd6m001s2sfy1pk082hx","category_id":"ckyoogd6t00262sfy85pm8aqg","_id":"ckyoogd77002z2sfy7qlz6xz9"},{"post_id":"ckyoogd71002l2sfycrnganxg","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd7700322sfydkweduok"},{"post_id":"ckyoogd73002q2sfyb7kf9086","category_id":"ckyoogd5800042sfyckmy6ka0","_id":"ckyoogd7800342sfyb7zmhqm8"},{"post_id":"ckyoogd6o001w2sfy6w2a1dqm","category_id":"ckyoogd6t00262sfy85pm8aqg","_id":"ckyoogd7900372sfyhbjmbrtm"},{"post_id":"ckyoogd6u002a2sfy7aty26ur","category_id":"ckyoogd75002v2sfy8r9kd4fn","_id":"ckyoogd7900392sfygjjg3872"},{"post_id":"cl09aye9k0000bsqea2oag18k","category_id":"ckyoogd6400102sfydlvubysr","_id":"cl09aye9v0001bsqe2d6xbikm"},{"post_id":"cl29twdca00026kqe3a0lhf0v","category_id":"ckyoogd6400102sfydlvubysr","_id":"cl29twdcc00036kqefzd77uf9"},{"post_id":"cl29tycgg00006vqe18tqdqmk","category_id":"ckyoogd6400102sfydlvubysr","_id":"cl29tycgj00016vqe39i5ajwb"},{"post_id":"cl64opa1b00002uqeblj83fzs","category_id":"ckyoogd6400102sfydlvubysr","_id":"cl64opa1u00012uqe9n6196b3"}],"PostTag":[{"post_id":"ckyoogd4k00002sfy8szt1ked","tag_id":"ckyoogd5a00052sfya5px284s","_id":"ckyoogd5w000o2sfy80sob4n9"},{"post_id":"ckyoogd4k00002sfy8szt1ked","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd5y000s2sfy2b5g5kz5"},{"post_id":"ckyoogd4k00002sfy8szt1ked","tag_id":"ckyoogd5k000g2sfy8y6peiw6","_id":"ckyoogd62000w2sfyhosq2rwy"},{"post_id":"ckyoogd5v000m2sfy9iwb1wrv","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd64000z2sfy4pzd4dmm"},{"post_id":"ckyoogd5500022sfycxz4cmiw","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd6a001c2sfy5flxhlbo"},{"post_id":"ckyoogd5500022sfycxz4cmiw","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd6c001g2sfy7ywacs2o"},{"post_id":"ckyoogd5500022sfycxz4cmiw","tag_id":"ckyoogd6500132sfyhl628t9b","_id":"ckyoogd6e001k2sfygor2ag8c"},{"post_id":"ckyoogd5b00062sfyg3lg3epi","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd6j001o2sfy6htx9pvi"},{"post_id":"ckyoogd5e00082sfy4nj7h0cx","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd6s00242sfy14d4b70j"},{"post_id":"ckyoogd5e00082sfy4nj7h0cx","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd6t00282sfy2vo19ti7"},{"post_id":"ckyoogd5e00082sfy4nj7h0cx","tag_id":"ckyoogd6500132sfyhl628t9b","_id":"ckyoogd6v002c2sfy650t2ldn"},{"post_id":"ckyoogd6r00222sfy042724an","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd6w002f2sfya77a3mq4"},{"post_id":"ckyoogd6s00252sfy1xv7eq92","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd70002k2sfy007p2kc1"},{"post_id":"ckyoogd5f00092sfyf1bnde42","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd72002n2sfy1znn4wu7"},{"post_id":"ckyoogd5f00092sfyf1bnde42","tag_id":"ckyoogd6t00292sfyfeci5vrs","_id":"ckyoogd74002s2sfy7wibbj17"},{"post_id":"ckyoogd6v002d2sfy8cr39haf","tag_id":"ckyoogd5a00052sfya5px284s","_id":"ckyoogd75002u2sfydnh41xzv"},{"post_id":"ckyoogd6v002d2sfy8cr39haf","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd76002y2sfy6ybn98hn"},{"post_id":"ckyoogd6v002d2sfy8cr39haf","tag_id":"ckyoogd5k000g2sfy8y6peiw6","_id":"ckyoogd7700302sfygvil3uuy"},{"post_id":"ckyoogd5h000c2sfy3dt1bo5l","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd7800332sfydzpi0b4n"},{"post_id":"ckyoogd5h000c2sfy3dt1bo5l","tag_id":"ckyoogd73002p2sfy0d9l5x6d","_id":"ckyoogd7800352sfybc2gf91e"},{"post_id":"ckyoogd5h000c2sfy3dt1bo5l","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd7900382sfyfyla37an"},{"post_id":"ckyoogd5i000d2sfy3qul9dlp","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd7e003d2sfy5pkc1hoc"},{"post_id":"ckyoogd5i000d2sfy3qul9dlp","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd7e003e2sfy0sv3cm89"},{"post_id":"ckyoogd5i000d2sfy3qul9dlp","tag_id":"ckyoogd79003a2sfy52ou0cgh","_id":"ckyoogd7f003g2sfy51hx6b6y"},{"post_id":"ckyoogd5i000d2sfy3qul9dlp","tag_id":"ckyoogd7a003b2sfy9ohbf698","_id":"ckyoogd7g003h2sfy9gft9jyy"},{"post_id":"ckyoogd5o000h2sfyh02rh1og","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd7k003l2sfyadmf7tnq"},{"post_id":"ckyoogd5o000h2sfyh02rh1og","tag_id":"ckyoogd79003a2sfy52ou0cgh","_id":"ckyoogd7k003m2sfy9a4r17nd"},{"post_id":"ckyoogd5o000h2sfyh02rh1og","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd7l003o2sfy45ns2xuw"},{"post_id":"ckyoogd5o000h2sfyh02rh1og","tag_id":"ckyoogd7i003j2sfy2hpt2074","_id":"ckyoogd7l003p2sfygmgm88sv"},{"post_id":"ckyoogd5r000i2sfyazuaafml","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd7o003t2sfy180hh2px"},{"post_id":"ckyoogd5r000i2sfyazuaafml","tag_id":"ckyoogd5u000l2sfyart99i16","_id":"ckyoogd7p003u2sfy5f1o7lun"},{"post_id":"ckyoogd5r000i2sfyazuaafml","tag_id":"ckyoogd79003a2sfy52ou0cgh","_id":"ckyoogd7q003w2sfy6jcch3f5"},{"post_id":"ckyoogd5r000i2sfyazuaafml","tag_id":"ckyoogd7i003j2sfy2hpt2074","_id":"ckyoogd7q003x2sfy5xm5d59c"},{"post_id":"ckyoogd5x000p2sfy4j4n543i","tag_id":"ckyoogd5y000t2sfyczho0hz0","_id":"ckyoogd7r003z2sfyff1tb1s4"},{"post_id":"ckyoogd5z000u2sfycdwngfek","tag_id":"ckyoogd7p003v2sfyeem2c9xu","_id":"ckyoogd7t00412sfyas2be5wu"},{"post_id":"ckyoogd5z000u2sfycdwngfek","tag_id":"ckyoogd7r003y2sfy56t4h16d","_id":"ckyoogd7u00422sfy594g7s18"},{"post_id":"ckyoogd5z000u2sfycdwngfek","tag_id":"ckyoogd5k000g2sfy8y6peiw6","_id":"ckyoogd7v00442sfy8fljga2i"},{"post_id":"ckyoogd63000x2sfy2cue9d1p","tag_id":"ckyoogd7s00402sfy4nq3dd17","_id":"ckyoogd7w00452sfy4085f8wf"},{"post_id":"ckyoogd6500112sfy7yaxbh60","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd7y00482sfy369rbsdx"},{"post_id":"ckyoogd6500112sfy7yaxbh60","tag_id":"ckyoogd7u00432sfycrbrctzd","_id":"ckyoogd7y00492sfy0jsm62oe"},{"post_id":"ckyoogd6500112sfy7yaxbh60","tag_id":"ckyoogd7w00462sfy6n1p4kto","_id":"ckyoogd7z004b2sfygck42jz8"},{"post_id":"ckyoogd6600142sfy5any6s9y","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd7z004c2sfy3hau295s"},{"post_id":"ckyoogd6600142sfy5any6s9y","tag_id":"ckyoogd7u00432sfycrbrctzd","_id":"ckyoogd80004e2sfy8f6m3hid"},{"post_id":"ckyoogd6700172sfy0f1t8hry","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd81004f2sfy7s277xwu"},{"post_id":"ckyoogd6700172sfy0f1t8hry","tag_id":"ckyoogd7y004a2sfycdxh2y2z","_id":"ckyoogd82004h2sfyh8p42iig"},{"post_id":"ckyoogd69001a2sfy9i4da3wy","tag_id":"ckyoogd80004d2sfyfun39inw","_id":"ckyoogd83004j2sfy3d121ayu"},{"post_id":"ckyoogd69001a2sfy9i4da3wy","tag_id":"ckyoogd81004g2sfy5rlncf2z","_id":"ckyoogd83004k2sfy8u1a3j98"},{"post_id":"ckyoogd69001a2sfy9i4da3wy","tag_id":"ckyoogd5k000g2sfy8y6peiw6","_id":"ckyoogd84004m2sfyhhtqhjcm"},{"post_id":"ckyoogd6b001e2sfy0wtqc7pv","tag_id":"ckyoogd82004i2sfy1u8y8hzk","_id":"ckyoogd84004n2sfy6fgcfs14"},{"post_id":"ckyoogd6b001e2sfy0wtqc7pv","tag_id":"ckyoogd5k000g2sfy8y6peiw6","_id":"ckyoogd85004p2sfy2pvza61k"},{"post_id":"ckyoogd6d001i2sfy2egu11bo","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd85004q2sfygzmj559g"},{"post_id":"ckyoogd6d001i2sfy2egu11bo","tag_id":"ckyoogd7y004a2sfycdxh2y2z","_id":"ckyoogd85004s2sfy1ckh75ig"},{"post_id":"ckyoogd6i001m2sfydei93sg4","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd85004t2sfy3qv57agl"},{"post_id":"ckyoogd6i001m2sfydei93sg4","tag_id":"ckyoogd7u00432sfycrbrctzd","_id":"ckyoogd87004v2sfycqy95i6l"},{"post_id":"ckyoogd6k001q2sfyav1n8x8y","tag_id":"ckyoogd85004r2sfy14vy4ltk","_id":"ckyoogd87004w2sfyha8yhm1h"},{"post_id":"ckyoogd6m001s2sfy1pk082hx","tag_id":"ckyoogd85004r2sfy14vy4ltk","_id":"ckyoogd89004z2sfybv6z96gh"},{"post_id":"ckyoogd6m001s2sfy1pk082hx","tag_id":"ckyoogd87004x2sfy6hlqc0fy","_id":"ckyoogd8900502sfya3yrd2w5"},{"post_id":"ckyoogd6o001w2sfy6w2a1dqm","tag_id":"ckyoogd85004r2sfy14vy4ltk","_id":"ckyoogd8c00532sfy20u329ih"},{"post_id":"ckyoogd6o001w2sfy6w2a1dqm","tag_id":"ckyoogd87004x2sfy6hlqc0fy","_id":"ckyoogd8e00542sfy170p31xg"},{"post_id":"ckyoogd6p001y2sfyfsufh2zi","tag_id":"ckyoogd7s00402sfy4nq3dd17","_id":"ckyoogd8f00562sfyb48i8s5p"},{"post_id":"ckyoogd6u002a2sfy7aty26ur","tag_id":"ckyoogd82004i2sfy1u8y8hzk","_id":"ckyoogd8g00592sfy6sak9l3d"},{"post_id":"ckyoogd6u002a2sfy7aty26ur","tag_id":"ckyoogd8f00572sfy5tvcepkp","_id":"ckyoogd8g005a2sfy9wt8dsor"},{"post_id":"ckyoogd6z002i2sfy681c5xni","tag_id":"ckyoogd8g00582sfy1npucbwp","_id":"ckyoogd8i005d2sfyhfmdat9w"},{"post_id":"ckyoogd6z002i2sfy681c5xni","tag_id":"ckyoogd8g005b2sfy690g6ozk","_id":"ckyoogd8i005e2sfy9rjb80r7"},{"post_id":"ckyoogd71002l2sfycrnganxg","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd8j005g2sfy4e7u24sm"},{"post_id":"ckyoogd71002l2sfycrnganxg","tag_id":"ckyoogd7y004a2sfycdxh2y2z","_id":"ckyoogd8j005h2sfy2qsrheb9"},{"post_id":"ckyoogd73002q2sfyb7kf9086","tag_id":"ckyoogd7y004a2sfycdxh2y2z","_id":"ckyoogd8j005i2sfy93rn0utw"},{"post_id":"ckyoogd73002q2sfyb7kf9086","tag_id":"ckyoogd5a00052sfya5px284s","_id":"ckyoogd8j005j2sfycvx1ebwg"},{"post_id":"ckyoogd73002q2sfyb7kf9086","tag_id":"ckyoogd5g000b2sfy51775fv6","_id":"ckyoogd8k005k2sfygtecdemr"}],"Tag":[{"name":"逻辑学","_id":"ckyoogd5a00052sfya5px284s"},{"name":"数学","_id":"ckyoogd5g000b2sfy51775fv6"},{"name":"哲学","_id":"ckyoogd5k000g2sfy8y6peiw6"},{"name":"人工智能","_id":"ckyoogd5u000l2sfyart99i16"},{"name":"python","_id":"ckyoogd5y000t2sfyczho0hz0"},{"name":"算法","_id":"ckyoogd6500132sfyhl628t9b"},{"name":"最优化","_id":"ckyoogd6t00292sfyfeci5vrs"},{"name":"tensorflow","_id":"ckyoogd73002p2sfy0d9l5x6d"},{"name":"机器学习","_id":"ckyoogd79003a2sfy52ou0cgh"},{"name":"GPU","_id":"ckyoogd7a003b2sfy9ohbf698"},{"name":"深度学习","_id":"ckyoogd7i003j2sfy2hpt2074"},{"name":"社会科学","_id":"ckyoogd7p003v2sfyeem2c9xu"},{"name":"艺术","_id":"ckyoogd7r003y2sfy56t4h16d"},{"name":"近代史","_id":"ckyoogd7s00402sfy4nq3dd17"},{"name":"统计","_id":"ckyoogd7u00432sfycrbrctzd"},{"name":"回归分析","_id":"ckyoogd7w00462sfy6n1p4kto"},{"name":"集合论","_id":"ckyoogd7y004a2sfycdxh2y2z"},{"name":"相对论","_id":"ckyoogd80004d2sfyfun39inw"},{"name":"物理","_id":"ckyoogd81004g2sfy5rlncf2z"},{"name":"宗教","_id":"ckyoogd82004i2sfy1u8y8hzk"},{"name":"像","_id":"ckyoogd85004r2sfy14vy4ltk"},{"name":"画","_id":"ckyoogd87004x2sfy6hlqc0fy"},{"name":"资本主义","_id":"ckyoogd8f00572sfy5tvcepkp"},{"name":"科学","_id":"ckyoogd8g00582sfy1npucbwp"},{"name":"环保主义","_id":"ckyoogd8g005b2sfy690g6ozk"}]}}